{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from typing import Type, List, Dict, Tuple, Set\n",
    "import argparse\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.externals.joblib import parallel_backend, Parallel, delayed\n",
    "except ImportError:\n",
    "    import joblib\n",
    "    from joblib import parallel_backend, Parallel, delayed\n",
    "    \n",
    "import pandas as pd\n",
    "import json, ijson\n",
    "import os, sys, uuid\n",
    "from pykalman import KalmanFilter\n",
    "from PIL import Image\n",
    "import math\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter, FFMpegWriter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "CHANNELS = [37,38,39]\n",
    "N_ESTIMATORS = 100\n",
    "MISSING_VALUE = -100\n",
    "DEBUG_LOGGING = False\n",
    "S3_CACHING_BUCKET = 'cognosos-ml-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_data_woc(scan: List[Dict]) -> Dict:\n",
    "    # Parse each scan to get maximum reading for each MAC address in specified channels\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            readings_by_mac_addr_and_channel[mac_addr] += readings\n",
    "    return {mac_addr: int(max(readings)) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if readings}\n",
    "\n",
    "\n",
    "def process_training(data_filepath: str) -> List[Dict]:\n",
    "    X = []\n",
    "\n",
    "    # parse it incrementally\n",
    "    with open(data_filepath, 'r') as f:\n",
    "        # reads the JSON incrementally\n",
    "        objects = ijson.items(f, 'item') \n",
    "\n",
    "        print('Done loading JSON incrementally')\n",
    "\n",
    "        for scan in objects:\n",
    "            \n",
    "            Zone_id = str(scan['zoneId'])\n",
    "            Room_name = str(scan['zoneName'])\n",
    "            parent_zone_id = str(scan['parentZoneId'])\n",
    "            tagId = scan['tagId']\n",
    "            timestamp = scan['rxAt']\n",
    "            scan_readings: List[Dict] = scan['scandata']\n",
    "            \n",
    "            row = parse_scan_data_woc(scan_readings) \n",
    "\n",
    "            row.update({\n",
    "                'Zone_id': Zone_id,\n",
    "                'Room_name': Room_name,\n",
    "                'parent_zone_id': parent_zone_id,\n",
    "                'tagId': tagId,\n",
    "                'timestamp': timestamp,\n",
    "            })\n",
    "\n",
    "            if row:\n",
    "                X.append(row)\n",
    "\n",
    "    print('Done processing data')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13926dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variable(X_train, y_train_floor, y_train, save_models=False):\n",
    "    \n",
    "    floor_pipeline = Pipeline([\n",
    "        ('rf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    floor_pipeline.fit(X_train, y_train_floor)\n",
    "\n",
    "    clf_floor = floor_pipeline.named_steps['rf']\n",
    "\n",
    "    clf_rooms = {}\n",
    "\n",
    "    selected_features = {}\n",
    "\n",
    "    for floor_num, samples in X_train.groupby(y_train_floor):\n",
    "        \n",
    "        floor_labels = y_train[samples.index]\n",
    "\n",
    "        non_all_neg_120_columns = samples.columns[~np.all(samples == -120, axis=0)]\n",
    "\n",
    "        selected_samples = samples[non_all_neg_120_columns]\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=200, random_state=100)\n",
    "\n",
    "        classifier.fit(selected_samples, floor_labels)\n",
    "\n",
    "        clf_rooms[str(floor_num)] = classifier\n",
    "\n",
    "        selected_features[str(floor_num)] = selected_samples.columns.tolist()\n",
    "\n",
    "    if save_models:\n",
    "        model = {\n",
    "        'selected_features': selected_features,\n",
    "        'clf_rooms': clf_rooms,\n",
    "        'clf_floor': clf_floor\n",
    "        }\n",
    "        joblib.dump(model, 'Hier_Features.joblib')\n",
    "        \n",
    "    return selected_features, clf_rooms, clf_floor\n",
    "\n",
    "def predict_variable(X_test, clf_floor, clf_rooms, selected_features):\n",
    "    \n",
    "    predicted_floors = clf_floor.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "    for floor_num, sample in zip(predicted_floors, X_test.values):\n",
    "        classifier = clf_rooms[str(floor_num)]\n",
    "\n",
    "        selected_names = selected_features[floor_num]\n",
    "\n",
    "        selected_sample = sample[X_test.columns.isin(selected_names)].reshape(1, -1)\n",
    "\n",
    "        predicted_room = classifier.predict(selected_sample)[0]\n",
    "#         predicted_room = predicted_room.astype(str)\n",
    "        predictions.append(predicted_room)\n",
    "\n",
    "    return predictions, predicted_floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(scan_data):\n",
    "\n",
    "    if scan_data is None:\n",
    "        return []\n",
    "    return [\n",
    "        {'macHex': entry['macHex'], 'channel': entry['channel'], 'readings': [entry['rssi'][0]]}\n",
    "        for entry in scan_data if 'macHex' in entry and 'rssi' in entry\n",
    "    ]\n",
    "\n",
    "def parse_scan_data(scan: List[Dict]) -> Dict:\n",
    "\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            channel = beacon_reading['channel']\n",
    "            readings_by_mac_addr_and_channel[f'{mac_addr}'] += readings#-{channel}\n",
    "    return {mac_addr: max(readings) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if len(readings) > 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digital_twin(Anchor_point_location_file, ground_truth_file_location, map_file_location):\n",
    "\n",
    "    anchor_df = pd.read_csv(Anchor_point_location_file)\n",
    "    anchor_df[\"x\"] = anchor_df[\"x\"].astype(int)\n",
    "    anchor_df[\"y\"] = anchor_df[\"y\"].astype(int)\n",
    "    \n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_df['Mac'] = anchor_df['Mac'].astype(str).str.zfill(12)\n",
    "    \n",
    "    macLists = anchor_df['Mac'].to_list()\n",
    "\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file_location)\n",
    "    ground_truth_df[\"Zone_id\"] = ground_truth_df[\"Zone_id\"].astype(str)\n",
    "    \n",
    "    #create a empty map with 0s for future calculation\n",
    "    map_ = np.zeros((65,28))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    image = Image.open(map_file_location)\n",
    "    \n",
    "    plt.scatter(anchor_df.x,anchor_df.y, color='blue', s=50, edgecolors='black', label='Beacons', marker='o', alpha=0.6)\n",
    "\n",
    "#     plt.scatter(ground_truth_df[\"x\"], ground_truth_df[\"y\"], color='red', s=20, label='Ground Truth', marker='^')\n",
    "#     for i, label in enumerate(ground_truth_df['Room_name']):  \n",
    "#         plt.text(ground_truth_df['x'][i], ground_truth_df['y'][i], label, fontsize=9, color='w', ha='right', va='bottom')\n",
    "    plt.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    plt.xlim(0, 65)\n",
    "    plt.ylim(0, 28)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xticks([i for i in range(0, 65, 5)])\n",
    "    plt.yticks([i for i in range(0, 28, 4)])\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.title(\"Beacon distribution in meters\")\n",
    "    plt.legend()\n",
    "    plt.savefig('beacon_map_cognosos.png')\n",
    "\n",
    "    return anchor_df, ground_truth_df, map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823e749-f1be-43ef-8323-29d6aa5ec414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beacon_file = 'ground_truth/Beacon_map_cognosos_flr3.csv'\n",
    "ground_truth_file = \"ground_truth/Ground_truth_Mar25.csv\"\n",
    "map_file = 'ground_truth/Cognosos_view.png'\n",
    "\n",
    "anchor_point_df, ground_truth_df, map_ = create_digital_twin(beacon_file, ground_truth_file, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_features(row, df1):\n",
    "\n",
    "    valid_features = {}\n",
    "    \n",
    "    for mac in df1['Mac']:\n",
    "       \n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] != -100:\n",
    "            valid_features[mac] = row[mac]\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "def convert_coordinates(coord_str):\n",
    "    if isinstance(coord_str, str):\n",
    "       \n",
    "        try:\n",
    "            coord_str = coord_str.strip(\"[]\")\n",
    "            elements = coord_str.split()\n",
    "            return [float(elem) for elem in elements] \n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "        try:\n",
    "            coord_str = coord_str.replace(\" \", \",\")\n",
    "            coord_str = coord_str.replace(\",,\", \",\")\n",
    "            coord_str = coord_str.strip(',')\n",
    "            return ast.literal_eval(coord_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error processing coordinate string: {coord_str}\")\n",
    "            return None\n",
    "    else:\n",
    "        return coord_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0506413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_all(result, ground_truth_df, map_file_location, output_file=\"compare_plot_MLE_NLOS.png\"):\n",
    "    results = []\n",
    "    total_points_mle = 0\n",
    "    total_points_Optimisation = 0\n",
    "    total_points_fuse = 0\n",
    "\n",
    "    total_inside_mle = 0\n",
    "    total_inside_Optimisation = 0\n",
    "    total_inside_fuse = 0\n",
    "\n",
    "    merged_df = pd.merge(result, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        has_fused = 'Predicted_NLOS' in room_data.columns\n",
    "\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "\n",
    "        if not polygon.is_valid:\n",
    "            print(f\"Invalid polygon for '{room_name}', attempting to fix with buffer(0).\")\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Parse MLE predictions\n",
    "        x_pred_mle, y_pred_mle = [], []\n",
    "        for coord in room_data[\"Predicted_MLE\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_mle.append(float(coord[0]))\n",
    "                y_pred_mle.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid MLE coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Optimisation predictions\n",
    "        x_pred_Optimisation, y_pred_Optimisation = [], []\n",
    "        for coord in room_data[\"Predicted_Optimisation\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_Optimisation.append(float(coord[0]))\n",
    "                y_pred_Optimisation.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid Optimisation coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Fused predictions only if available\n",
    "        x_pred_fuse, y_pred_fuse = [], []\n",
    "        if has_fused:\n",
    "            for coord in room_data[\"Predicted_NLOS\"]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_pred_fuse.append(float(coord[0]))\n",
    "                    y_pred_fuse.append(float(coord[1]))\n",
    "                except:\n",
    "                    print(f\"Invalid NLOS coord in '{room_name}': {coord}\")\n",
    "\n",
    "        inside_count_mle = sum(1 for x, y in zip(x_pred_mle, y_pred_mle) if Point(x, y).within(polygon))\n",
    "        inside_count_Optimisation = sum(1 for x, y in zip(x_pred_Optimisation, y_pred_Optimisation) if Point(x, y).within(polygon))\n",
    "        inside_count_fuse = sum(1 for x, y in zip(x_pred_fuse, y_pred_fuse) if Point(x, y).within(polygon)) if has_fused else 0\n",
    "\n",
    "        total_points_mle += len(x_pred_mle)\n",
    "        total_inside_mle += inside_count_mle\n",
    "\n",
    "        total_points_Optimisation += len(x_pred_Optimisation)\n",
    "        total_inside_Optimisation += inside_count_Optimisation\n",
    "\n",
    "        if has_fused:\n",
    "            total_points_fuse += len(x_pred_fuse)\n",
    "            total_inside_fuse += inside_count_fuse\n",
    "\n",
    "        percentage_inside_mle = (inside_count_mle / len(x_pred_mle)) * 100 if x_pred_mle else 0\n",
    "        percentage_inside_Optimisation = (inside_count_Optimisation / len(x_pred_Optimisation)) * 100 if x_pred_Optimisation else 0\n",
    "        percentage_inside_fuse = (inside_count_fuse / len(x_pred_fuse)) * 100 if has_fused and x_pred_fuse else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            'Room_name': room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            'MLE_Accuracy': percentage_inside_mle,\n",
    "            'Optimisation_Accuracy': percentage_inside_Optimisation,\n",
    "            'NLOS_Accuracy': percentage_inside_fuse if has_fused else None,\n",
    "            'MLE_Inside_Points': inside_count_mle,\n",
    "            'Optimisation_Inside_Points': inside_count_Optimisation,\n",
    "            'NLOS_Inside_Points': inside_count_fuse if has_fused else None,\n",
    "            'Total_Points': len(x_pred_mle),\n",
    "        })\n",
    "\n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_pred_mle, y_pred_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_pred_Optimisation, y_pred_Optimisation, color='green', s=8, label='Optimisation')\n",
    "        if has_fused:\n",
    "            ax.scatter(x_pred_fuse, y_pred_fuse, color='red', s=8, label='NLOS')\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        title_str = f\"{room_name} - MLE: {percentage_inside_mle:.1f}%, Optimisation: {percentage_inside_Optimisation:.1f}%\"\n",
    "        if has_fused:\n",
    "            title_str += f\", NLOS: {percentage_inside_fuse:.1f}%\"\n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    overall_mle_accuracy = (total_inside_mle / total_points_mle) * 100 if total_points_mle > 0 else 0\n",
    "    overall_Optimisation_accuracy = (total_inside_Optimisation / total_points_Optimisation) * 100 if total_points_Optimisation > 0 else 0\n",
    "    overall_fuse_accuracy = (total_inside_fuse / total_points_fuse) * 100 if total_points_fuse > 0 else 0\n",
    "\n",
    "    print(f\"\\nOverall MLE Accuracy: {overall_mle_accuracy:.2f}%\")\n",
    "    print(f\"Overall Optimisation Accuracy: {overall_Optimisation_accuracy:.2f}%\")\n",
    "    if total_points_fuse > 0:\n",
    "        print(f\"Overall NLOS Accuracy: {overall_fuse_accuracy:.2f}%\")\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89394c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MLE_data_survey_portal(df, ground_truth_df, anchor_point_df, export_unheard=False, export_path=\"unheard_anchor_points.csv\"):\n",
    "\n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_point_df['Mac'] = anchor_point_df['Mac'].astype(str).str.zfill(12)\n",
    "\n",
    "    data_set_df = pd.DataFrame()\n",
    "    merged_df = pd.merge(df, ground_truth_df, on=[\"Zone_id\", 'Room_name'], how='inner').drop(['parent_zone_id'], axis=1)\n",
    "    zones = df['Zone_id']\n",
    "    heard_anchor_points = []\n",
    "\n",
    "    for mac_addr in anchor_point_df['Mac']:\n",
    "        if mac_addr in merged_df.columns:\n",
    "            data_set_df[mac_addr] = merged_df[mac_addr]\n",
    "            heard_anchor_points.append(mac_addr)\n",
    "\n",
    "    heard_anchor_point_df = anchor_point_df[anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "    unheard_anchor_point_df = anchor_point_df[~anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "\n",
    "    heard_anchor_points_coord = heard_anchor_point_df[['x', 'y']].values\n",
    "\n",
    "    data_set_df[\"Zone_id\"] = merged_df[\"Zone_id\"]\n",
    "    data_set_df[\"Room_name\"] = merged_df[\"Room_name\"]\n",
    "    data_set_df[\"tagId\"] = merged_df[\"tagId\"]\n",
    "    data_set_df[\"timestamp\"] = merged_df[\"timestamp\"]\n",
    "    if \"channel\" in merged_df.columns:\n",
    "        data_set_df[\"channel\"] = merged_df[\"channel\"]\n",
    "    \n",
    "    data_set_df[\"x\"] = merged_df[\"x\"]\n",
    "    data_set_df[\"y\"] = merged_df[\"y\"]\n",
    "\n",
    "    return data_set_df, heard_anchor_points_coord, unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data_walking_Stats.json\"\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "X1 = process_training(filepath)\n",
    "df1 = pd.DataFrame(X1)\n",
    "df1 = df1.fillna(MISSING_VALUE)  \n",
    "\n",
    "float_cols = df1.select_dtypes(include=['float']).columns\n",
    "df1[float_cols] = df1[float_cols].astype(np.int8)\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], utc=True, errors='coerce')\n",
    "df1 = df1.sort_values(by='timestamp')\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# Fix specific zone_id\n",
    "df1.loc[df1['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# # Conditional exclusion based on filename\n",
    "# if filename == \"data_duress_access_Jun23.json\":\n",
    "#     rooms_to_exclude = [\"Tech Office 2\"]  # this room has issue with data as very low beacon signal, beacon count\n",
    "#     df1 = df1[~df1['Room_name'].isin(rooms_to_exclude)]\n",
    "\n",
    "# Beacon processing\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "# df1= df1[df1.Zone_id.isin(zones)]\n",
    "print(df1.shape)\n",
    "\n",
    "df1 = df1[df1['beacon_count'] >= 5]\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6160d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['beacon_count'].max(), df1['beacon_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df1, df2], axis=0, ignore_index=True, sort=False)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce10978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv(\"data/Duress/data_duress_edgecase_combine.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc9d5",
   "metadata": {},
   "source": [
    "## REMOVE ALL ROWS < -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "rows_all_below_90 = df1[beacon_cols].lt(-99).all(axis=1)\n",
    "df1 = df1[~rows_all_below_90]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f079dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tagId.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee041",
   "metadata": {},
   "source": [
    "ankle: '7002352', '7002100', '7002065, '7002074'\n",
    "\n",
    "pocket: '7000588', '7000245'\n",
    "\n",
    "front: '7003640', '7003999', '7004370', '7004333'\n",
    "\n",
    "back: '7004162', '7004545', '7004268', '7004050'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd17c3",
   "metadata": {},
   "source": [
    "### Check if dataset have enoguh beacon heard >=-90, SELECT ONLY the number of strong features >=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1.drop(columns=[\"parent_zone_id\", \"beacon_count\"]), \\\n",
    "                                ground_truth_df[['Zone_id','x', 'y']], on=[\"Zone_id\"], how='left')\n",
    "rssi_cols = [col for col in df2.columns if col.startswith('0')]\n",
    "\n",
    "# Create a new column counting RSSIs >= -90\n",
    "df2['num_strong_features'] = (df2[rssi_cols] >= -95).sum(axis=1)\n",
    "\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bd06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df = df2[df2['num_strong_features'] >= 5].copy()\n",
    "\n",
    "data_set_df=data_set_df.drop(columns='num_strong_features').reset_index(drop=True)\n",
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.iloc[[2885]].describe().T.sort_values(by=\"max\", ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa23c3b",
   "metadata": {},
   "source": [
    "# Take 10% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_df = data_set_df_all.groupby('Zone_id', group_keys=False).\\\n",
    "#         apply(lambda x: x.sample(frac=1, random_state=42))\n",
    "# data_set_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8071023",
   "metadata": {},
   "source": [
    "# *** MAKE SURE THAT ALL BEACONS ARE BEING HEARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if any ionstalled beacon not heard by the tags\n",
    "# unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab65e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set_df.columns.intersection(anchor_point_df.Mac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76b8dd",
   "metadata": {},
   "source": [
    "# NLOS: Fused of Opt and MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55701418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(point, points, height_diff=1.5):\n",
    "    return np.sqrt(np.sum((points - point) ** 2, axis=1) + height_diff ** 2)\n",
    "\n",
    "def generate_grid(center, resolution=5, radius=4):\n",
    "    step = 1 / resolution\n",
    "    x_vals = np.arange(center[0] - radius, center[0] + radius + step, step)\n",
    "    y_vals = np.arange(center[1] - radius, center[1] + radius + step, step)\n",
    "    xv, yv = np.meshgrid(x_vals, y_vals)\n",
    "    return np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "\n",
    "def trilateration(coords, distances):\n",
    "    def fun(x, coords, distances):\n",
    "        weights = 1 / (distances + 1e-5)\n",
    "        return weights * (np.linalg.norm(coords - x, axis=1) - distances)\n",
    "    x0 = np.mean(coords, axis=0)\n",
    "    result = least_squares(fun, x0, args=(coords, distances))\n",
    "    return result.x if result.success else x0\n",
    "\n",
    "def rssi_to_distance(rssi, A=-65, n=3.5, scale=0.8):\n",
    "    return scale * np.exp((A - rssi) / (10 * n))\n",
    "\n",
    "def filter_rssi(row, beacon_positions, rssi_threshold=-90):\n",
    "    return {\n",
    "        mac: row[mac]\n",
    "        for mac in beacon_positions.keys()\n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] > rssi_threshold\n",
    "    }\n",
    "\n",
    "def localization_error(tag_position, beacons, distances):\n",
    "    estimated_distances = np.linalg.norm(beacons - tag_position, axis=1)\n",
    "    sigma = np.std(distances) + 1e-3\n",
    "    weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "    return np.sum(weights * (estimated_distances - distances) ** 2)\n",
    "\n",
    "def generate_expansion_area(initial_guess, std_dev=0.5, radius=0.8, num_points=500, range_box=2):\n",
    "    num_gauss = int(num_points * 0.4)\n",
    "    num_box = int(num_points * 0.3)\n",
    "    num_circle = num_points - num_gauss - num_box\n",
    "    box_points = np.random.uniform(-range_box, range_box, size=(num_box, 2)) + initial_guess\n",
    "    gauss_points = np.random.normal(0, std_dev, size=(num_gauss, 2)) + initial_guess\n",
    "    r = radius * np.sqrt(np.random.uniform(0, 1, num_circle))\n",
    "    theta = np.random.uniform(0, 2 * np.pi, num_circle)\n",
    "    circ_points = np.column_stack((initial_guess[0] + r * np.cos(theta), initial_guess[1] + r * np.sin(theta)))\n",
    "    return np.vstack((gauss_points, box_points, circ_points))\n",
    "\n",
    "def calculate_total_error_to_all_beacons(best_position, beacon_coords):\n",
    "    distances = np.linalg.norm(beacon_coords - best_position, axis=1)\n",
    "    return np.sum(distances)\n",
    "\n",
    "def compute_likelihood_weighted(grid_coords, anchor_coords, rssi_values, T, n, sigma_noise=4, anchor_weights=None):\n",
    "    if anchor_weights is None:\n",
    "        anchor_weights = np.ones_like(rssi_values)\n",
    "    diff = grid_coords[:, None, :] - anchor_coords[None, :, :]\n",
    "    dists = np.sqrt(np.sum(diff ** 2, axis=2) + 1.5**2)\n",
    "    pred_rssi = T - 10 * n * np.log10(dists + 1e-5)\n",
    "    residuals = pred_rssi - rssi_values\n",
    "    weighted_residuals = (residuals / sigma_noise)**2 * anchor_weights\n",
    "    likelihood = np.exp(-0.5 * weighted_residuals)\n",
    "    return np.prod(likelihood, axis=1)\n",
    "\n",
    "def find_mle_params(P_j, d_ij, init_guess=[-45, 3]):  # Chneg from -65\n",
    "    def squared_error(params, dists, rssi):\n",
    "        T_i, n_p = params\n",
    "        valid_mask = rssi != -100\n",
    "        pred_rssi = T_i - 10 * n_p * np.log10(dists + 1e-5)\n",
    "        return np.sum((pred_rssi[valid_mask] - rssi[valid_mask]) ** 2)\n",
    "    bounds = [(-100, -30), (2, 6)]\n",
    "    result = minimize(squared_error, init_guess, args=(d_ij, P_j),\n",
    "                      method='L-BFGS-B', bounds=bounds)\n",
    "    return result.x if result.success else init_guess\n",
    "\n",
    "# Fused Localization Function\n",
    "# -------------------------------\n",
    "def fused_localization_ref(data_df, anchor_point_df,\n",
    "                       sigma_noise=4, coarse_res=2, fine_res=5, fine_radius=3,\n",
    "                       rssi_threshold=-95, strong_rssi_threshold=-75,\n",
    "                       top_k_anchors=5, roi_margin=8, top_coarse_points=200, topN_ratio=0.05,\n",
    "                       map_x_bounds=(0, 65), map_y_bounds=(0, 28),\n",
    "                       epsilon=1e-12,\n",
    "                       min_beacons_opt=3, max_beacons_opt=20,\n",
    "                       expansion_radius=1, expansion_points=100, top_expansion_points=15,\n",
    "                       enable_refinement=True):\n",
    "    \n",
    "    results = []\n",
    "    beacon_positions = anchor_point_df[[\"x\",\"y\",\"Mac\"]].set_index(\"Mac\")[[\"x\",\"y\"]].to_dict(orient=\"index\")\n",
    "    has_timestamp = 'timestamp' in data_df.columns\n",
    "\n",
    "    for idx, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "        # ---------------------------\n",
    "        # Extract RSSI\n",
    "        # ---------------------------\n",
    "        rssis = row.drop(['Zone_id','Room_name','x','y','tagId','timestamp'], errors='ignore').values.astype(float)\n",
    "        anchor_coords = anchor_point_df[['x','y']].values\n",
    "\n",
    "        # ---------------------------\n",
    "        # MLE\n",
    "        # ---------------------------\n",
    "        mask_mle = rssis > rssi_threshold\n",
    "        signal_strengths = rssis[mask_mle]\n",
    "        dp_coords = anchor_coords[mask_mle]\n",
    "\n",
    "        if len(signal_strengths) < 1:\n",
    "            signal_strengths = rssis\n",
    "            dp_coords = anchor_coords\n",
    "\n",
    "        strong_mask = signal_strengths > strong_rssi_threshold\n",
    "        if np.sum(strong_mask) < 2:\n",
    "            dp_coords_selected = dp_coords\n",
    "            signal_strengths_selected = signal_strengths\n",
    "        else:\n",
    "            dp_coords_selected = dp_coords[strong_mask]\n",
    "            signal_strengths_selected = signal_strengths[strong_mask]\n",
    "\n",
    "        min_rssi, max_rssi = np.min(signal_strengths_selected), np.max(signal_strengths_selected)\n",
    "        anchor_weights = (signal_strengths_selected - min_rssi + 1) / (max_rssi - min_rssi + 1e-5)\n",
    "        sorted_idx = np.argsort(-signal_strengths_selected)\n",
    "        top_k = min(top_k_anchors, len(sorted_idx))\n",
    "        top_coords = dp_coords_selected[sorted_idx[:top_k]]\n",
    "\n",
    "        x_min, y_min = np.min(top_coords, axis=0)\n",
    "        x_max, y_max = np.max(top_coords, axis=0)\n",
    "        x_min = max(x_min - roi_margin, map_x_bounds[0])\n",
    "        x_max = min(x_max + roi_margin, map_x_bounds[1])\n",
    "        y_min = max(y_min - roi_margin, map_y_bounds[0])\n",
    "        y_max = min(y_max + roi_margin, map_y_bounds[1])\n",
    "\n",
    "        coarse_grid = np.stack(np.meshgrid(np.arange(x_min, x_max, 1/coarse_res),\n",
    "                                           np.arange(y_min, y_max, 1/coarse_res)), axis=-1).reshape(-1,2)\n",
    "\n",
    "        strongest_coord = top_coords[0]\n",
    "        dists_for_fit = euclidean_dist(strongest_coord, dp_coords_selected)\n",
    "        T_global, n_global = find_mle_params(signal_strengths_selected, dists_for_fit)\n",
    "\n",
    "        coarse_likelihoods = compute_likelihood_weighted(\n",
    "            coarse_grid, dp_coords_selected, signal_strengths_selected,\n",
    "            T_global, n_global, sigma_noise, anchor_weights\n",
    "        )\n",
    "\n",
    "        top_indices = np.argpartition(coarse_likelihoods, -top_coarse_points)[-top_coarse_points:]\n",
    "        top_candidates = coarse_grid[top_indices]\n",
    "\n",
    "        fine_candidates, fine_likelihoods = [], []\n",
    "        for center in top_candidates:\n",
    "            fine_grid = generate_grid(center, resolution=fine_res, radius=fine_radius)\n",
    "            likelihoods_fine = compute_likelihood_weighted(\n",
    "                fine_grid, dp_coords_selected, signal_strengths_selected,\n",
    "                T_global, n_global, sigma_noise, anchor_weights\n",
    "            )\n",
    "            fine_candidates.append(fine_grid)\n",
    "            fine_likelihoods.append(likelihoods_fine)\n",
    "\n",
    "        fine_candidates = np.vstack(fine_candidates)\n",
    "        fine_likelihoods = np.hstack(fine_likelihoods)\n",
    "        fine_likelihoods += epsilon\n",
    "        fine_likelihoods /= np.sum(fine_likelihoods)\n",
    "\n",
    "        N = max(1, min(100, int(topN_ratio * len(fine_candidates))))\n",
    "        top_idx = np.argpartition(fine_likelihoods, -N)[-N:]\n",
    "        top_points = fine_candidates[top_idx]\n",
    "        top_weights = fine_likelihoods[top_idx]\n",
    "        top_weights /= np.sum(top_weights)\n",
    "        pred_mle = np.average(top_points, axis=0, weights=top_weights)\n",
    "        conf_mle = np.max(top_weights)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optimization\n",
    "        # ---------------------------\n",
    "        rssi_values_opt = dict(sorted(filter_rssi(row, beacon_positions, rssi_threshold).items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(rssi_values_opt) < min_beacons_opt:\n",
    "            beacon_coords_opt = anchor_coords\n",
    "            distances_opt = np.ones(anchor_coords.shape[0])\n",
    "        else:\n",
    "            beacon_coords_opt = np.array([list(beacon_positions[b].values()) for b in rssi_values_opt.keys()])\n",
    "            distances_opt = np.array([rssi_to_distance(rssi) for rssi in rssi_values_opt.values()])\n",
    "\n",
    "        close_beacons = beacon_coords_opt[distances_opt <= 1]\n",
    "        if len(close_beacons) > 0:\n",
    "            initial_guess_opt = close_beacons[np.argmin(distances_opt[distances_opt <= 1])]\n",
    "        else:\n",
    "            top_n = min(5, len(distances_opt))\n",
    "            initial_guess_opt = trilateration(beacon_coords_opt[:top_n], distances_opt[:top_n])\n",
    "\n",
    "        best_positions_opt, total_errors_opt, num_beacons_used = [], [], []\n",
    "        for num_beacons in range(min_beacons_opt, min(len(distances_opt), max_beacons_opt)+1):\n",
    "            top_n_coords = beacon_coords_opt[:num_beacons]\n",
    "            top_n_dists = distances_opt[:num_beacons]\n",
    "\n",
    "            expansion_area = generate_expansion_area(initial_guess_opt, radius=expansion_radius, num_points=expansion_points)\n",
    "            quick_errors = np.array([localization_error(p, top_n_coords, top_n_dists) for p in expansion_area])\n",
    "            filtered_expansion_area = expansion_area[np.argsort(quick_errors)[:top_expansion_points]]\n",
    "\n",
    "            best_err, best_pos = float(\"inf\"), None\n",
    "            for point in filtered_expansion_area:\n",
    "                res = minimize(localization_error, point, args=(top_n_coords, top_n_dists), method='L-BFGS-B', options={'maxiter':100})\n",
    "                if res.success:\n",
    "                    est_pos = res.x\n",
    "                    total_err = np.sum(np.linalg.norm(top_n_coords - est_pos, axis=1))\n",
    "                    if total_err < best_err:\n",
    "                        best_err = total_err\n",
    "                        best_pos = est_pos\n",
    "            best_positions_opt.append(best_pos)\n",
    "            total_errors_opt.append(best_err)\n",
    "            num_beacons_used.append(num_beacons)\n",
    "\n",
    "        # Final selection based on total error\n",
    "        best_idx_opt = np.argmin([calculate_total_error_to_all_beacons(p, beacon_coords_opt) for p in best_positions_opt])\n",
    "        pred_opt = best_positions_opt[best_idx_opt]\n",
    "        conf_opt = 1 / (1 + total_errors_opt[best_idx_opt])\n",
    "\n",
    "        # ---------------------------\n",
    "        # Final Refinement (matches original code)\n",
    "        # ---------------------------\n",
    "        pre_refined_pos = pred_opt.copy()\n",
    "        if enable_refinement:\n",
    "            refinement_rssi_threshold = -75\n",
    "            strong_rssi_indices = [i for i, rssi in enumerate(rssi_values_opt.values()) if rssi > refinement_rssi_threshold]\n",
    "\n",
    "            if len(strong_rssi_indices) >= 3:\n",
    "                filtered_coords = beacon_coords_opt[strong_rssi_indices]\n",
    "                filtered_distances = distances_opt[strong_rssi_indices]\n",
    "\n",
    "                result = minimize(\n",
    "                    localization_error,\n",
    "                    pred_opt,\n",
    "                    args=(filtered_coords, filtered_distances),\n",
    "                    method='L-BFGS-B',\n",
    "                    options={'maxiter': 100, 'gtol': 1e-8, 'disp': False}\n",
    "                )\n",
    "                if result.success:\n",
    "                    pred_opt = result.x\n",
    "\n",
    "        refinement_shift = np.linalg.norm(pred_opt - pre_refined_pos)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Fused Results\n",
    "        # ---------------------------\n",
    "        alpha_dynamic = conf_mle / (conf_mle + conf_opt)\n",
    "        pred_fused_fixed = 0.5 * pred_mle + 0.5 * pred_opt\n",
    "        pred_fused_dynamic = alpha_dynamic * pred_mle + (1-alpha_dynamic) * pred_opt\n",
    "\n",
    "        results.append({\n",
    "            'original_index': idx,\n",
    "            'Zone_id': row.get('Zone_id', np.nan),\n",
    "            'Room_name': row.get('Room_name', np.nan),\n",
    "            'Tag_id': row.get('tagId', np.nan),\n",
    "            'timestamp': row.get('timestamp', np.nan),\n",
    "            'Predicted_MLE': pred_mle,\n",
    "            'Predicted_Optimisation': pred_opt,\n",
    "            'Predicted_NLOS': pred_fused_fixed,\n",
    "            'Predicted_NLOS_Dynamic': pred_fused_dynamic,\n",
    "            'MLE_Confidence': conf_mle,\n",
    "            'Opt_Confidence': conf_opt,\n",
    "            'Ground_Truth': np.array([row['x'], row['y']])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter() \n",
    "\n",
    "result= fused_localization_ref(data_set_df, anchor_point_df)\n",
    "\n",
    "save_folder = \"Result_Duress\"\n",
    "save_name = f\"{filename.replace('.json', '_NLOS_Real_Time.csv')}\" \n",
    "save_path = os.path.join(save_folder, save_name)\n",
    "\n",
    "result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter() \n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_row = total_time / len(data_set_df)\n",
    "print(avg_time_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b168022",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result= results_df.copy()\n",
    "# result= pd.read_csv(\"Result_Duress/data_duress_access_Jun23_Fused.csv\")\n",
    "# result['Predicted_MLE'] = result['Predicted_MLE'].apply(convert_coordinates)\n",
    "# result['Ground_Truth'] = result['Ground_Truth'].apply(convert_coordinates)\n",
    "\n",
    "\n",
    "# result[\"Predicted_Optimisation\"]= result['Predicted_Optimisation'].apply(convert_coordinates)\n",
    "# result[\"Predicted_Fused\"]= result['Predicted_Fused'].apply(convert_coordinates)\n",
    "# result[\"Fused_Alpha_0.5\"]= result['Fused_Alpha_0.5'].apply(convert_coordinates)\n",
    "\n",
    "# result.Zone_id= result.Zone_id.astype(str)\n",
    "# # result.Tag_id= result.Tag_id.astype(str)\n",
    "# result= result[result.Room_name !='Womens Restroom']\n",
    "# result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def plot_predicted_fused_dynamic(result_df, ground_truth_df, map_file_location,\n",
    "                                 fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "                                 output_file=\"compare_plot_MLE_Optim_Fused.png\"):\n",
    "    \"\"\"\n",
    "    Plot predicted locations from MLE, Optimisation, and fused methods, showing inside-room accuracy.\n",
    "    Computes overall accuracy and returns per-room statistics including inside-point counts and total points.\n",
    "    Also provides room-type aggregated accuracy.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    total_inside = {\"MLE\": 0, \"Optimisation\": 0}\n",
    "    total_inside.update({col: 0 for col in fused_cols})\n",
    "    total_points = 0  # only one total points count\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        # Room polygon\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Helper function to parse coordinates\n",
    "        def parse_coords(col_name):\n",
    "            x_list, y_list = [], []\n",
    "            for coord in room_data[col_name]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_list.append(float(coord[0]))\n",
    "                    y_list.append(float(coord[1]))\n",
    "                except:\n",
    "                    pass\n",
    "            return x_list, y_list\n",
    "\n",
    "        # Predictions\n",
    "        x_mle, y_mle = parse_coords(\"Predicted_MLE\")\n",
    "        x_opt, y_opt = parse_coords(\"Predicted_Optimisation\")\n",
    "        fused_data = {col: parse_coords(col) for col in fused_cols if col in room_data.columns}\n",
    "\n",
    "        # Count points inside polygon\n",
    "        def count_inside(x_list, y_list):\n",
    "            return sum(1 for x, y in zip(x_list, y_list) if Point(x, y).within(polygon))\n",
    "\n",
    "        inside_mle = count_inside(x_mle, y_mle)\n",
    "        inside_opt = count_inside(x_opt, y_opt)\n",
    "        inside_fused = {k: count_inside(*v) for k, v in fused_data.items()}\n",
    "\n",
    "        # Update totals\n",
    "        total_inside[\"MLE\"] += inside_mle\n",
    "        total_inside[\"Optimisation\"] += inside_opt\n",
    "        for k, v in fused_data.items():\n",
    "            total_inside[k] += inside_fused[k]\n",
    "        total_points += len(x_mle)  # same for all methods\n",
    "\n",
    "        # Save per-room results\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"MLE_Accuracy\": inside_mle / max(len(x_mle), 1) * 100,\n",
    "            \"Optimisation_Accuracy\": inside_opt / max(len(x_opt), 1) * 100,\n",
    "            **{f\"{k}_Accuracy\": inside_fused[k] / max(len(fused_data[k][0]), 1) * 100 for k in fused_data},\n",
    "            \"MLE_Inside_Points\": inside_mle,\n",
    "            \"Optimisation_Inside_Points\": inside_opt,\n",
    "            **{f\"{k}_Inside_Points\": inside_fused[k] for k in fused_data},\n",
    "            \"Total_Points\": len(x_mle)\n",
    "        })\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_mle, y_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_opt, y_opt, color='green', s=8, label='Optimisation')\n",
    "        colors = ['orange', 'purple', 'red', 'cyan']\n",
    "        for j, (fcol, (x_f, y_f)) in enumerate(fused_data.items()):\n",
    "            ax.scatter(x_f, y_f, color=colors[j % len(colors)], s=8, label=f\"{fcol}\")\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        \n",
    "        # Build title using percentage accuracy instead of counts\n",
    "        title_str = (\n",
    "            f\"{room_name} - \"\n",
    "            f\"MLE: {inside_mle / max(len(x_mle), 1) * 100:.1f}%, \"\n",
    "            f\"Opt: {inside_opt / max(len(x_opt), 1) * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        for fcol, (x_f, y_f) in fused_data.items():\n",
    "            acc = inside_fused[fcol] / max(len(x_f), 1) * 100\n",
    "            clean_name = fcol.replace(\"Predicted_\", \"\")  # <--- removes the prefix\n",
    "            title_str += f\", {clean_name}: {acc:.1f}%\"\n",
    "\n",
    "\n",
    "            \n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "\n",
    "    # --- Overall accuracy ---\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    for method in total_inside.keys():\n",
    "        overall = total_inside[method] / max(total_points, 1) * 100\n",
    "        print(f\"{method}: {overall:.2f}%\")\n",
    "\n",
    "    # --- Room-type aggregated accuracy ---\n",
    "    room_type_stats = accuracy_df.groupby('Room_Type').agg({\n",
    "        'MLE_Inside_Points': 'sum',\n",
    "        'Optimisation_Inside_Points': 'sum',\n",
    "        **{f\"{col}_Inside_Points\": 'sum' for col in fused_cols},\n",
    "        'Total_Points': 'sum'\n",
    "    })\n",
    "\n",
    "    for method in ['MLE', 'Optimisation'] + fused_cols:\n",
    "        room_type_stats[f\"{method}_Accuracy\"] = room_type_stats[f\"{method}_Inside_Points\"] / room_type_stats['Total_Points'] * 100\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = plot_predicted_fused_dynamic(\n",
    "    result_df=result,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location= map_file,\n",
    "    fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "#     output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb441bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"Predicted_NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df[\"MLE_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df[\"Optimisation_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df[\"Predicted_NLOS_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Duress/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_accuracy_per_room(\n",
    "    accuracy_df,\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Fused\"),\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise numeric accuracies for three models in different colors.\n",
    "    First line: MLE/Optimisation (no spaces)\n",
    "    Second line: Fused\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge with ground truth polygons\n",
    "    merged_df = pd.merge(accuracy_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Polygon coordinates\n",
    "        x_coords = [row.get(f\"x{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"x{i+1}\", None))]\n",
    "        y_coords = [row.get(f\"y{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"y{i+1}\", None))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Draw polygon outline\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "\n",
    "        # Accuracy values\n",
    "        acc1 = int(row.get(f\"{labels[0]}_Accuracy\", 0))\n",
    "        acc2 = int(row.get(f\"{labels[1]}_Accuracy\", 0))\n",
    "        acc3 = int(row.get(f\"{labels[2]}_Accuracy\", 0))\n",
    "\n",
    "        centroid = polygon.centroid\n",
    "\n",
    "        # Line 1: MLE / Optimisation\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y + 0.15,\n",
    "            f\"{acc1}/{acc2}\",\n",
    "            color=\"black\", fontsize=10, ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "        # Individual colors\n",
    "        ax.text(centroid.x - 0.55, centroid.y + 0.15, f\"{acc1}\", color=colors[0], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "        ax.text(centroid.x, centroid.y + 0.15, \"/\", color=\"black\", fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4)\n",
    "        ax.text(centroid.x + 0.55, centroid.y + 0.15, f\"{acc2}\", color=colors[1], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "\n",
    "        # Line 2: Fused\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y - 0.45,\n",
    "            f\"{acc3}\",\n",
    "            color=colors[2], fontsize=10, ha=\"center\", va=\"center\", zorder=3, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    # Automatically scale axes\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    #  Weighted overall accuracies\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "    overall_acc = []\n",
    "    for label in labels:\n",
    "        inside_col = f\"{label}_Inside_Points\"\n",
    "        if inside_col in accuracy_df:\n",
    "            overall = (accuracy_df[inside_col].sum() / total_points) * 100\n",
    "            overall_acc.append(overall)\n",
    "        else:\n",
    "            overall_acc.append(0)\n",
    "\n",
    "    overall_text = \" | \".join([f\"{label}: {val:.1f}%\" for label, val in zip(labels, overall_acc)])\n",
    "    ax.set_title(f\"{title_text}\\nOverall Accuracy: {overall_text}\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [Patch(color=color, label=label) for color, label in zip(colors, labels)]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room(\n",
    "    accuracy_df=accuracy_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location=map_file,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Predicted_Fused\"),\n",
    "    title_text=\"DURESS_Normal Case_Single Data Packet\",\n",
    "#     output_file=\"Result_Duress/compare_plot_Fused_data_duress_access_Jun23.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_per_tag(\n",
    "    result_df,\n",
    "    ground_truth_df,\n",
    "    fused_cols=['Predicted_NLOS', 'Predicted_NLOS_Dynamic']\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes accuracy per Tag_id for MLE, Optimisation, and fused models.\n",
    "    Returns a DataFrame containing inside counts, total points, and accuracy (%)\n",
    "    for each tag in each room.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge predictions with polygon definitions\n",
    "    merged = pd.merge(result_df, ground_truth_df,\n",
    "                      on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate room by room\n",
    "    for (zone_id, room_name), room_df in merged.groupby([\"Zone_id\", \"Room_name\"]):\n",
    "\n",
    "        # Extract polygon\n",
    "        box = room_df.iloc[0]\n",
    "        x_coords = [box.get(f\"x{i+1}\") for i in range(8) if pd.notnull(box.get(f\"x{i+1}\"))]\n",
    "        y_coords = [box.get(f\"y{i+1}\") for i in range(8) if pd.notnull(box.get(f\"y{i+1}\"))]\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords))).buffer(0)\n",
    "\n",
    "        # Group by tag\n",
    "        for tag_id, tag_df in room_df.groupby(\"Tag_id\"):\n",
    "\n",
    "            # Helper to parse prediction coordinates\n",
    "            def parse_coords(series):\n",
    "                xs, ys = [], []\n",
    "                for v in series:\n",
    "                    try:\n",
    "                        v = ast.literal_eval(v) if isinstance(v, str) else v\n",
    "                        xs.append(float(v[0]))\n",
    "                        ys.append(float(v[1]))\n",
    "                    except:\n",
    "                        pass\n",
    "                return xs, ys\n",
    "\n",
    "            # --- Base models ---\n",
    "            x_mle, y_mle = parse_coords(tag_df[\"Predicted_MLE\"])\n",
    "            x_opt, y_opt = parse_coords(tag_df[\"Predicted_Optimisation\"])\n",
    "\n",
    "            def inside_count(xs, ys):\n",
    "                return sum(Point(x, y).within(polygon) for x, y in zip(xs, ys))\n",
    "\n",
    "            mle_inside = inside_count(x_mle, y_mle)\n",
    "            opt_inside = inside_count(x_opt, y_opt)\n",
    "\n",
    "            # Save base results\n",
    "            result_row = {\n",
    "                \"Zone_id\": zone_id,\n",
    "                \"Room_name\": room_name,\n",
    "                \"Tag_id\": tag_id,\n",
    "                \"MLE_Inside\": mle_inside,\n",
    "                \"Optimisation_Inside\": opt_inside,\n",
    "                \"Total_Points\": len(x_mle),\n",
    "                \"MLE_Accuracy\": 100 * mle_inside / max(len(x_mle), 1),\n",
    "                \"Optimisation_Accuracy\": 100 * opt_inside / max(len(x_opt), 1),\n",
    "            }\n",
    "\n",
    "            # --- Fused Methods ---\n",
    "            for col in fused_cols:\n",
    "                if col in tag_df.columns:\n",
    "                    xf, yf = parse_coords(tag_df[col])\n",
    "                    inside_f = inside_count(xf, yf)\n",
    "                    result_row[f\"{col}_Inside\"] = inside_f\n",
    "                    result_row[f\"{col}_Accuracy\"] = 100 * inside_f / max(len(xf), 1)\n",
    "\n",
    "            results.append(result_row)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_overall_accuracy_per_tag(tag_accuracy_df):\n",
    "    \"\"\"\n",
    "    Compute overall accuracy per Tag_id by summing inside points and total points,\n",
    "    then dividing to get accuracy (%) for each method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Columns for models\n",
    "    base_models = [\"MLE\", \"Optimisation\"]\n",
    "    fused_models = [col.replace(\"_Inside\", \"\") for col in tag_accuracy_df.columns if col.endswith(\"_Inside\") \n",
    "                    and col not in [\"MLE_Inside\", \"Optimisation_Inside\"]]\n",
    "\n",
    "    all_models = base_models + fused_models\n",
    "\n",
    "    # Group by Tag_id\n",
    "    grouped = tag_accuracy_df.groupby(\"Tag_id\").agg(\n",
    "        {f\"{m}_Inside\": \"sum\" for m in all_models} | {\"Total_Points\": \"sum\"}\n",
    "    ).reset_index()\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    for m in all_models:\n",
    "        grouped[f\"{m}_Overall_Accuracy\"] = grouped[f\"{m}_Inside\"] / grouped[\"Total_Points\"] * 100\n",
    "\n",
    "    return grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad89a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_accuracy_df = compute_accuracy_per_tag(result, ground_truth_df)\n",
    "overall_df = compute_overall_accuracy_per_tag(tag_accuracy_df)\n",
    "overall_df[[\"Tag_id\", \"MLE_Overall_Accuracy\", \"Optimisation_Overall_Accuracy\", \"Predicted_NLOS_Overall_Accuracy\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tag_location_accuracy(tag_accuracy_df):\n",
    "    \"\"\"\n",
    "    Map tags to body location and compute overall accuracy per location.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mapping\n",
    "    location_map = {\n",
    "        'At the Ankle': ['7001337', '7001263'],\n",
    "        'In the Pocket': ['7000480', '7000286'],\n",
    "        'At the Front': ['7000140', '7000589', '7001084', '7001399'],\n",
    "        'In the Back': ['7000730', '7000484', '7001235', '7000256']\n",
    "    }\n",
    "\n",
    "    # Reverse mapping: Tag_id -> location\n",
    "    tag_to_location = {}\n",
    "    for loc, tags in location_map.items():\n",
    "        for t in tags:\n",
    "            tag_to_location[str(t)] = loc\n",
    "\n",
    "    df = tag_accuracy_df.copy()\n",
    "\n",
    "    # Ensure Tag_id is string for mapping\n",
    "    df['Tag_id'] = df['Tag_id'].astype(str)\n",
    "    df['Location'] = df['Tag_id'].map(tag_to_location)\n",
    "\n",
    "    # Drop rows without location mapping\n",
    "    df = df.dropna(subset=['Location'])\n",
    "\n",
    "    # Compute overall accuracy per location (weighted by Total_Points if available)\n",
    "    models = [col for col in df.columns if col.endswith(\"_Overall_Accuracy\")]\n",
    "\n",
    "    location_acc = df.groupby('Location').apply(\n",
    "        lambda x: pd.Series({m: (x[m] * x['Total_Points']).sum() / x['Total_Points'].sum() for m in models})\n",
    "    ).reset_index()\n",
    "\n",
    "    return location_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_accuracy_df = map_tag_location_accuracy(overall_df)\n",
    "location_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rssi_box_by_zone_location(df, location_map, top_n=5):\n",
    "    \n",
    "    rssi_cols = [c for c in df.columns if c[0].isdigit()]  \n",
    "    \n",
    "    tag_to_location = {str(t): loc for loc, tags in location_map.items() for t in tags}\n",
    "    df['Tag_id'] = df['tagId'].astype(str)\n",
    "    df['Location'] = df['Tag_id'].map(tag_to_location)\n",
    "    df = df.dropna(subset=['Location'])\n",
    "    \n",
    "    plot_data = []\n",
    "    x_labels = []\n",
    "    \n",
    "    colors = {'At the Front':'green', 'In the Back':'orange', 'In the Pocket':'red', 'At the Ankle':'blue'}\n",
    "    \n",
    "    for zone_id, zone_df in df.groupby('Zone_id'):\n",
    "        # Compute top RSSI features per zone, excluding -100\n",
    "        mean_rssi = zone_df[rssi_cols].where(zone_df[rssi_cols] != -100).mean()\n",
    "        top_features = mean_rssi.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "        \n",
    "        for loc in ['At the Front', 'In the Back', 'In the Pocket', 'At the Ankle']:\n",
    "            loc_df = zone_df[zone_df['Location'] == loc]\n",
    "            if loc_df.empty:\n",
    "                plot_data.append([np.nan]*top_n)\n",
    "            else:\n",
    "                # Collect only values != -100\n",
    "                values = loc_df[top_features].applymap(lambda x: x if x != -100 else np.nan)\n",
    "                # Flatten all non-NaN values\n",
    "                values_flat = values.values.flatten()\n",
    "                values_flat = values_flat[~np.isnan(values_flat)]  # remove NaNs\n",
    "                if len(values_flat) == 0:\n",
    "                    values_flat = [np.nan] * top_n  # keep placeholder so box is drawn\n",
    "                plot_data.append(values_flat)\n",
    "\n",
    "            \n",
    "            # Only zone ID in x-axis label\n",
    "            x_labels.append(f\"Zone {zone_id}\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    box = plt.boxplot(plot_data, patch_artist=True, labels=x_labels)\n",
    "    \n",
    "    # Color each box according to location order\n",
    "    for patch, label, idx in zip(box['boxes'], x_labels, range(len(x_labels))):\n",
    "        loc = ['At the Front', 'In the Back', 'In the Pocket', 'At the Ankle'][idx % 4]\n",
    "        patch.set_facecolor(colors.get(loc, 'grey'))\n",
    "\n",
    "    # Add location legend\n",
    "    legend_handles = [\n",
    "    plt.Line2D([0], [0], color=color, lw=8, label=loc)\n",
    "    for loc, color in colors.items()\n",
    "    ]\n",
    "\n",
    "    plt.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Tag Location\",\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, 0.0),\n",
    "        ncol=4\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"RSSI\")\n",
    "    plt.title(f\"Top {top_n} RSSI Features per Zone by Tag Location\")\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "        'At the Ankle': ['7001337', '7001263'],\n",
    "        'In the Pocket': ['7000480', '7000286'],\n",
    "        'At the Front': ['7000140', '7000589', '7001084', '7001399'],\n",
    "        'In the Back': ['7000730', '7000484', '7001235', '7000256']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rssi_box_by_zone_location(data_set_df, location_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fa221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ef73aa",
   "metadata": {},
   "source": [
    "## C. Using 5 dp\n",
    "Using 5 dp and apply the Centroid for location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def safe_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        return literal_eval(x)\n",
    "    return x\n",
    "\n",
    "def compute_centroid(points):\n",
    "    xs, ys = zip(*points)\n",
    "    return [np.mean(xs), np.mean(ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_by_window(result, window_sizes=range(1, 11)):\n",
    "    result = result.copy()\n",
    "\n",
    "    # Safely evaluate string lists\n",
    "    result['Predicted_MLE'] = result['Predicted_MLE'].apply(safe_eval)\n",
    "    result['Ground_Truth'] = result['Ground_Truth'].apply(safe_eval)\n",
    "\n",
    "    # Handle naming differences\n",
    "    if 'Predicted_Optimisation' in result.columns:\n",
    "        optimisation_col = 'Predicted_Optimisation'\n",
    "    elif 'Predicted_Opt' in result.columns:\n",
    "        optimisation_col = 'Predicted_Opt'\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Predicted_Optimisation' nor 'Predicted_Opt' found in DataFrame\")\n",
    "\n",
    "    result[optimisation_col] = result[optimisation_col].apply(safe_eval)\n",
    "\n",
    "    # Optional fused predictions\n",
    "    has_fused = 'Predicted_NLOS' in result.columns\n",
    "    if has_fused:\n",
    "        result['Predicted_NLOS'] = result['Predicted_NLOS'].apply(safe_eval)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    group_cols = ['Zone_id', 'Room_name', 'Tag_id']\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        for group_keys, group in result.groupby(group_cols):\n",
    "            group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "            n = len(group)\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                # ----------- CORRECTED WINDOW LOGIC ---------------\n",
    "                if i < window_size:\n",
    "                    # BEGINNING: grow window\n",
    "                    start = 0\n",
    "                    end = i + 1\n",
    "                else:\n",
    "                    # SLIDING WINDOW: always full windows\n",
    "                    start = i - window_size + 1\n",
    "                    end = i + 1\n",
    "                # --------------------------------------------------\n",
    "\n",
    "                window = group.iloc[start:end]\n",
    "\n",
    "                # Extract points\n",
    "                mle_points = list(window['Predicted_MLE'])\n",
    "                optimisation_points = list(window[optimisation_col])\n",
    "                ground_truth_points = list(window['Ground_Truth'])\n",
    "\n",
    "                mle_centroid = compute_centroid(mle_points)\n",
    "                optimisation_centroid = compute_centroid(optimisation_points)\n",
    "                ground_truth_centroid = compute_centroid(ground_truth_points)\n",
    "\n",
    "                result_row = {\n",
    "                    'Zone_id': group_keys[0],\n",
    "                    'Room_name': group_keys[1],\n",
    "                    'tagId': group_keys[2],\n",
    "                    'Window_Size': window_size,\n",
    "                    'Predicted_MLE': mle_centroid,\n",
    "                    'Predicted_Optimisation': optimisation_centroid,\n",
    "                    'Ground_Truth': ground_truth_centroid\n",
    "                }\n",
    "\n",
    "                if has_fused:\n",
    "                    fused_points = list(window['Predicted_NLOS'])\n",
    "                    fused_centroid = compute_centroid(fused_points)\n",
    "                    result_row['Predicted_NLOS'] = fused_centroid\n",
    "\n",
    "                all_results.append(result_row)\n",
    "\n",
    "    centroid_df = pd.DataFrame(all_results)\n",
    "    return centroid_df[centroid_df['Room_name'] != 'Womens Restroom']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a982ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_result = compute_centroids_by_window(result, window_sizes=range(5, 6))\n",
    "centroid_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df_centroid_5 = plot_predicted_all(centroid_result[centroid_result.Window_Size==5], \\\n",
    "                                            ground_truth_df, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df_centroid_5\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df_centroid_5[\"MLE_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df_centroid_5[\"Optimisation_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df_centroid_5[\"NLOS_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Duress/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cefd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(1, 11))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(centroid_df, ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "                           on=['Zone_id', 'Room_name'], how='left')\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Patch: ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        acc_df = plot_predicted_all(\n",
    "            result=df_w,\n",
    "            ground_truth_df=ground_truth_df,\n",
    "            map_file_location=map_file_location,\n",
    "#             output_file=f\"Result_Duress_Eric_Data_Jun25/temp_plot_ws_{w}.png\"\n",
    "        )\n",
    "\n",
    "        # Overall accuracy\n",
    "        mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "        opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "        fused_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "        grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\", \"Optimisation_Accuracy\",\\\n",
    "                                              'NLOS_Accuracy']].mean()\n",
    "\n",
    "        row = {\n",
    "            \"Window_Size\": w,\n",
    "            \"MLE_Overall\": mle_overall,\n",
    "            \"Optimisation_Overall\": opt_overall,\n",
    "            \"NLOS_Overall\": fused_overall,\n",
    "        }\n",
    "\n",
    "        for room_type in grouped.index:\n",
    "            row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "            row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "            row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "        accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d714bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(1, 11))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(\n",
    "        centroid_df,\n",
    "        ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Patch: ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Group by tagId to get per-tag accuracy\n",
    "        for tag_id, df_tag in df_w.groupby(\"tagId\"):\n",
    "\n",
    "            acc_df = plot_predicted_all(\n",
    "                result=df_tag,\n",
    "                ground_truth_df=ground_truth_df,\n",
    "                map_file_location=map_file_location,\n",
    "            )\n",
    "\n",
    "            # Overall accuracy per tag\n",
    "            mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "            opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "            NLOS_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "            # Room-type aggregated accuracy\n",
    "            grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\", \"Optimisation_Accuracy\", 'NLOS_Accuracy']].mean()\n",
    "\n",
    "            row = {\n",
    "                \"Window_Size\": w,\n",
    "                \"tagId\": tag_id,              # <-- include tagId\n",
    "                \"MLE_Overall\": mle_overall,\n",
    "                \"Optimisation_Overall\": opt_overall,\n",
    "                \"NLOS_Overall\": NLOS_overall,\n",
    "            }\n",
    "\n",
    "            for room_type in grouped.index:\n",
    "                row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "                row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "                row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "            accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window = compute_accuracy_by_window(result, ground_truth_df, map_file)\n",
    "\n",
    "accuracy_vs_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "accuracy_vs_window_tag = compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file)\n",
    "\n",
    "accuracy_vs_window_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355727d1",
   "metadata": {},
   "source": [
    "### Notes\n",
    "accuracy_by_window will calaute all tags combine so we can have global accuracy\n",
    "\n",
    "accuracy_vs_window_tag will calaute per tag so we can see indiviudal tag performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Zone_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result[result.Zone_id=='35181'].copy()\n",
    "\n",
    "# 1. Sort the data\n",
    "df = df.sort_values([\"Tag_id\", \"timestamp\"])\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Expand list columns into x/y numeric columns\n",
    "# -------------------------------------------------------\n",
    "def expand_xy(df, col):\n",
    "    df[[f\"{col}_x\", f\"{col}_y\"]] = pd.DataFrame(df[col].tolist(), index=df.index)\n",
    "\n",
    "expand_xy(df, \"Predicted_MLE\")\n",
    "expand_xy(df, \"Predicted_Optimisation\")\n",
    "expand_xy(df, \"Predicted_NLOS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Apply sliding window = 5 to get centroid\n",
    "# -------------------------------------------------------\n",
    "window = 5\n",
    "\n",
    "# MLE centroid\n",
    "df[\"MLE_centroid_x\"] = df.groupby(\"Tag_id\")[\"Predicted_MLE_x\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "df[\"MLE_centroid_y\"] = df.groupby(\"Tag_id\")[\"Predicted_MLE_y\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Optimisation centroid\n",
    "df[\"Opt_centroid_x\"] = df.groupby(\"Tag_id\")[\"Predicted_Optimisation_x\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "df[\"Opt_centroid_y\"] = df.groupby(\"Tag_id\")[\"Predicted_Optimisation_y\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# NLOS centroid\n",
    "df[\"NLOS_centroid_x\"] = df.groupby(\"Tag_id\")[\"Predicted_NLOS_x\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "df[\"NLOS_centroid_y\"] = df.groupby(\"Tag_id\")[\"Predicted_NLOS_y\"].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Combine x + y back into a single list feature\n",
    "# -------------------------------------------------------\n",
    "df[\"Centroid_MLE\"]  = df[[\"MLE_centroid_x\", \"MLE_centroid_y\"]].values.tolist()\n",
    "df[\"Centroid_Opt\"]  = df[[\"Opt_centroid_x\", \"Opt_centroid_y\"]].values.tolist()\n",
    "df[\"Centroid_NLOS\"] = df[[\"NLOS_centroid_x\", \"NLOS_centroid_y\"]].values.tolist()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Output with same-format features\n",
    "# -------------------------------------------------------\n",
    "final_df = df[[\n",
    "    \"Tag_id\", \"timestamp\",\n",
    "    \"Centroid_MLE\",\n",
    "    \"Centroid_Opt\",\n",
    "    \"Centroid_NLOS\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_map = {\n",
    "#         'At the Ankle': ['7001337', '7001263'],\n",
    "#         'In the Pocket': ['7000480', '7000286'],\n",
    "#         'At the Front': ['7000140', '7000589', '7001084', '7001399'],\n",
    "#         'In the Back': ['7000730', '7000484', '7001235', '7000256']\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "#         'At the Ankle': ['7001337', '7001263'],\n",
    "        'In the Pocket': ['7000480', '7000286'],\n",
    "#         'At the Front': ['7000140', '7000589', '7001084', '7001399'],\n",
    "#         'In the Back': ['7000730', '7000484', '7001235', '7000256']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "\n",
    "def plot_centroids_on_map(\n",
    "        df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        NLOS_cols=[\"Centroid_NLOS\"],\n",
    "        title=\"NLOS_ Tag \",\n",
    "        output=\"centroid_tracking_eat.png\",\n",
    "        highlight_zones=None,\n",
    "        ground_truth_df=None,\n",
    "        location_map=location_map  # dict mapping locations to tag IDs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Plot NLOS centroid locations for a tag over time on the map,\n",
    "    mark start/end points, show movement direction with arrows,\n",
    "    dynamically zoom to the trajectory, highlight specified zones,\n",
    "    and show tag location in the title.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter dataset for the tag\n",
    "    tag_df = df[df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if tag_df.empty:\n",
    "        print(f\"No data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine tag location from location_map\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_ids in location_map.items():\n",
    "            if tag_id in tag_ids:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Convert timestamp to datetime if not already\n",
    "    tag_df[\"timestamp\"] = pd.to_datetime(tag_df[\"timestamp\"])\n",
    "    tag_df[\"timestamp_eat\"] = tag_df[\"timestamp\"] - pd.Timedelta(hours=5)\n",
    "\n",
    "    # Expand centroid list to x, y columns\n",
    "    def split_xy(column):\n",
    "        xy = tag_df[column].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "        tag_df[f\"{column}_x\"] = xy.apply(lambda v: v[0] if isinstance(v, (list, tuple)) else np.nan)\n",
    "        tag_df[f\"{column}_y\"] = xy.apply(lambda v: v[1] if isinstance(v, (list, tuple)) else np.nan)\n",
    "\n",
    "    for col in NLOS_cols:\n",
    "        split_xy(col)\n",
    "\n",
    "    # Load map\n",
    "    image = mpimg.imread(map_file)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    # --- Highlight zones ---\n",
    "    if highlight_zones and ground_truth_df is not None:\n",
    "        for zone_id in highlight_zones:\n",
    "            room_row = ground_truth_df[ground_truth_df['Zone_id'] == zone_id]\n",
    "            if not room_row.empty:\n",
    "                room_row = room_row.iloc[0]\n",
    "                x_coords = [room_row.get(f'x{i+1}') for i in range(8) if pd.notnull(room_row.get(f'x{i+1}'))]\n",
    "                y_coords = [room_row.get(f'y{i+1}') for i in range(8) if pd.notnull(room_row.get(f'y{i+1}'))]\n",
    "                if x_coords and y_coords:\n",
    "                    coords = list(zip(x_coords, y_coords))\n",
    "                    polygon_patch = mplPolygon(\n",
    "                        coords,\n",
    "                        closed=True,\n",
    "                        fill=True,\n",
    "                        facecolor='yellow',\n",
    "                        alpha=0.3,\n",
    "                        edgecolor='orange',\n",
    "                        linewidth=2\n",
    "                    )\n",
    "                    plt.gca().add_patch(polygon_patch)\n",
    "\n",
    "    # Collect all valid points to determine zoom\n",
    "    all_x, all_y = [], []\n",
    "    for col in NLOS_cols:\n",
    "        x_col = f\"{col}_x\"\n",
    "        y_col = f\"{col}_y\"\n",
    "        x_vals = tag_df[x_col].values\n",
    "        y_vals = tag_df[y_col].values\n",
    "        valid_mask = (~np.isnan(x_vals)) & (~np.isnan(y_vals))\n",
    "        all_x.extend(x_vals[valid_mask])\n",
    "        all_y.extend(y_vals[valid_mask])\n",
    "\n",
    "    # Compute dynamic zoom with margin\n",
    "    if len(all_x) > 0 and len(all_y) > 0:\n",
    "        margin_x = (max(all_x) - min(all_x)) * 0.05 + 1\n",
    "        margin_y = (max(all_y) - min(all_y)) * 0.05 + 1\n",
    "        x_min, x_max = min(all_x) - margin_x, max(all_x) + margin_x\n",
    "        y_min, y_max = min(all_y) - margin_y, max(all_y) + margin_y\n",
    "    else:\n",
    "        x_min, x_max = 30, 65\n",
    "        y_min, y_max = 0, 28\n",
    "\n",
    "    # Time progression for color\n",
    "    t_norm = np.linspace(0, 1, len(tag_df))\n",
    "    cmap = plt.cm.viridis\n",
    "\n",
    "    # Plot each centroid column\n",
    "    for col in NLOS_cols:\n",
    "        x_col = f\"{col}_x\"\n",
    "        y_col = f\"{col}_y\"\n",
    "\n",
    "        x = tag_df[x_col].values\n",
    "        y = tag_df[y_col].values\n",
    "        valid_mask = (~np.isnan(x)) & (~np.isnan(y))\n",
    "        x = x[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        t_norm_valid = t_norm[valid_mask]\n",
    "\n",
    "        if len(x) == 0:\n",
    "            continue\n",
    "\n",
    "        # Scatter points colored by time\n",
    "        plt.scatter(x, y, c=t_norm_valid, s=20, cmap=cmap, marker=\"s\", label=f\"{col}\")\n",
    "\n",
    "        # Connect points\n",
    "        plt.plot(x, y, linewidth=1, alpha=0.7, color='grey')\n",
    "\n",
    "        # Draw arrows for movement direction\n",
    "        if len(x) > 1:\n",
    "            dx = np.diff(x)\n",
    "            dy = np.diff(y)\n",
    "            plt.quiver(\n",
    "                x[:-1], y[:-1], dx, dy,\n",
    "                scale_units='xy', angles='xy', scale=1,\n",
    "                width=0.003, color='blue', alpha=0.7\n",
    "            )\n",
    "\n",
    "        # Start and end markers\n",
    "        plt.scatter(x[0], y[0], color='red', s=100, marker='*', label=f\"Start {col}\")\n",
    "        plt.scatter(x[-1], y[-1], color='red', s=100, marker='X', label=f\"End {col}\")\n",
    "\n",
    "    # Start and end time\n",
    "    start_time = tag_df[\"timestamp_eat\"].iloc[0].strftime(\"%H:%M:%S\")\n",
    "    end_time   = tag_df[\"timestamp_eat\"].iloc[-1].strftime(\"%H:%M:%S\")\n",
    "    plt.text(x_min, y_max + 0.5, f\"Time: {start_time}  {end_time}\", fontsize=12, color=\"black\")\n",
    "\n",
    "    # Include tag location in title\n",
    "    plt.title(f\"{title}{tag_id} ({location_str})\")\n",
    "    plt.xlabel(\"X coordinate\")\n",
    "    plt.ylabel(\"Y coordinate\")\n",
    "    plt.colorbar(label=\"Time progression (early-->late)\")\n",
    "    plt.legend(loc='lower right', bbox_to_anchor=(0.25,0.1))\n",
    "\n",
    "    # Apply dynamic zoom to focus on trajectory\n",
    "    plt.xlim([x_min, x_max])\n",
    "    plt.ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62485870",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_zones = [\"30558\",  \"30509\", \"30553\", \"35182\", \"30514\", \"35181\", \"30516\",\\\n",
    "                    \"30512\",  \"30518\", \"30533\",\\\n",
    "                  \"30520\", \"30524\", \"30525\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Tag_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "\n",
    "plot_centroids_on_map(\n",
    "    df=final_df,\n",
    "    map_file=map_file,\n",
    "    tag_id=\"7000286\",\n",
    "    NLOS_cols=[\"Centroid_NLOS\"],\n",
    "    output=\"tag_7000480.png\",\n",
    "    highlight_zones=highlight_zones,\n",
    "    ground_truth_df=ground_truth_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56174a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"Plot_Nov25\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all unique tags\n",
    "for tag in final_df['Tag_id'].unique():\n",
    "    output_file = os.path.join(output_folder, f\"NLOS_{tag}.png\")\n",
    "    \n",
    "    plot_centroids_on_map(\n",
    "        df=final_df,\n",
    "        map_file=map_file,\n",
    "        tag_id=tag,\n",
    "        NLOS_cols=[\"Centroid_NLOS\"],\n",
    "        output=output_file,\n",
    "        highlight_zones=highlight_zones,\n",
    "        ground_truth_df=ground_truth_df\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea0d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbaee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "import ast\n",
    "\n",
    "def animate_centroids_video(\n",
    "        df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        NLOS_cols=[\"Centroid_NLOS\"],\n",
    "        title=\"NLOS_Tag\",\n",
    "        output=\"centroid_tracking.mp4\",\n",
    "        highlight_zones=None,\n",
    "        ground_truth_df=None,\n",
    "        location_map=None,\n",
    "        x_min_fixed=None  # optional fixed minimum x-axis\n",
    "    ):\n",
    "    tag_df = df[df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if tag_df.empty:\n",
    "        print(f\"No data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine tag location from location_map\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    full_title = f\"{title} {tag_id} ({location_str})\"\n",
    "\n",
    "    # Convert timestamps\n",
    "    tag_df[\"timestamp\"] = pd.to_datetime(tag_df[\"timestamp\"])\n",
    "    tag_df[\"timestamp_eat\"] = tag_df[\"timestamp\"] - pd.Timedelta(hours=5)\n",
    "\n",
    "    # Expand centroid columns\n",
    "    def split_xy(col):\n",
    "        xy = tag_df[col].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "        tag_df[f\"{col}_x\"] = xy.apply(lambda v: v[0] if isinstance(v, (list, tuple)) else np.nan)\n",
    "        tag_df[f\"{col}_y\"] = xy.apply(lambda v: v[1] if isinstance(v, (list, tuple)) else np.nan)\n",
    "    for col in NLOS_cols:\n",
    "        split_xy(col)\n",
    "\n",
    "    # Load map: support both path and pre-loaded image\n",
    "    if isinstance(map_file, str):\n",
    "        image = mpimg.imread(map_file)\n",
    "    else:\n",
    "        image = map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # Highlight zones\n",
    "    if highlight_zones and ground_truth_df is not None:\n",
    "        for zone_id in highlight_zones:\n",
    "            row = ground_truth_df[ground_truth_df['Zone_id']==zone_id]\n",
    "            if not row.empty:\n",
    "                row = row.iloc[0]\n",
    "                x_coords = [row[f\"x{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "                y_coords = [row[f\"y{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "                if x_coords and y_coords:\n",
    "                    poly = mplPolygon(list(zip(x_coords,y_coords)), closed=True, fill=True,\n",
    "                                      facecolor='yellow', alpha=0.3, edgecolor='orange', linewidth=2)\n",
    "                    ax.add_patch(poly)\n",
    "\n",
    "    # Scatter placeholders\n",
    "    scatters = []\n",
    "    for col in NLOS_cols:\n",
    "        s = ax.scatter([], [], s=30, label=col, color='red')\n",
    "\n",
    "        scatters.append(s)\n",
    "\n",
    "    arrows_all = [[] for _ in NLOS_cols]\n",
    "\n",
    "    # Animation update\n",
    "    def update(frame):\n",
    "        for i, col in enumerate(NLOS_cols):\n",
    "            x = tag_df[f\"{col}_x\"].values[:frame+1]\n",
    "            y = tag_df[f\"{col}_y\"].values[:frame+1]\n",
    "            mask = (~np.isnan(x)) & (~np.isnan(y))\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            scatters[i].set_offsets(np.column_stack((x,y)) if len(x)>0 else np.empty((0,2)))\n",
    "\n",
    "            # Remove previous arrows\n",
    "            for arr in arrows_all[i]:\n",
    "                arr.remove()\n",
    "            arrows_all[i] = []\n",
    "\n",
    "            if len(x) > 1:\n",
    "                for j in range(len(x)-1):\n",
    "                    arr = FancyArrowPatch((x[j], y[j]), (x[j+1], y[j+1]),\n",
    "                                          arrowstyle='->', color='red', mutation_scale=10, alpha=0.7)\n",
    "                    ax.add_patch(arr)\n",
    "                    arrows_all[i].append(arr)\n",
    "        return scatters + sum(arrows_all, [])\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(tag_df), interval=200, blit=True)\n",
    "\n",
    "    # Axis limits\n",
    "    if x_min_fixed is not None:\n",
    "        ax.set_xlim(left=x_min_fixed)\n",
    "    ax.set_ylim([0,28])\n",
    "\n",
    "    ax.set_title(full_title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    # Save video\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "    print(f\"Video saved as {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_all_tags(\n",
    "        df,\n",
    "        map_file=map_file,\n",
    "        NLOS_cols=[\"Centroid_NLOS\"],\n",
    "        title=\"NLOS_Tag\",\n",
    "        output_folder=\"videos_Nov25\",\n",
    "        highlight_zones=highlight_zones,\n",
    "        ground_truth_df=ground_truth_df,\n",
    "        location_map=location_map,\n",
    "        x_min_fixed=0  # set x-axis start\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Animate all tags in the dataframe and save each as an MP4 video\n",
    "    in the specified output folder, including tag location in the title.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    tag_ids = df[\"Tag_id\"].unique()\n",
    "\n",
    "    for tag_id in tag_ids:\n",
    "        # Determine tag location from location_map\n",
    "        location_str = \"\"\n",
    "        if location_map:\n",
    "            for loc, tag_list in location_map.items():\n",
    "                if tag_id in tag_list:\n",
    "                    location_str = loc\n",
    "                    break\n",
    "\n",
    "        video_title = f\"{title}\"\n",
    "        output_path = os.path.join(output_folder, f\"{title}_{tag_id}_{location_str}.mp4\")\n",
    "        print(f\"Processing Tag {tag_id}  {output_path}\")\n",
    "\n",
    "        # Animate the video\n",
    "        animate_centroids_video(\n",
    "            df=df,\n",
    "            map_file=map_file,\n",
    "            tag_id=tag_id,\n",
    "            NLOS_cols=NLOS_cols,\n",
    "            title=video_title,\n",
    "            output=output_path,\n",
    "            highlight_zones=highlight_zones,\n",
    "            ground_truth_df=ground_truth_df,\n",
    "            location_map=location_map,\n",
    "            x_min_fixed=x_min_fixed  # pass the fixed x_min\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_all_tags(final_df, map_file=map_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2311f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4efcda1",
   "metadata": {},
   "source": [
    "## Location AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/Duress/2_data_duress_access_Jun23.json\"\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "X1 = process_training(filepath)\n",
    "df2 = pd.DataFrame(X1)\n",
    "df2 = df2.fillna(MISSING_VALUE)  \n",
    "\n",
    "float_cols = df2.select_dtypes(include=['float']).columns\n",
    "df2[float_cols] = df2[float_cols].astype(np.int8)\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'], utc=True, errors='coerce')\n",
    "df2 = df2.sort_values(by='timestamp')\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df2 = df2.reindex(columns=new_column_order)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df2['Room_name'] = df2['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# Fix specific zone_id\n",
    "df2.loc[df2['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# Beacon processing\n",
    "beacon_cols = [col for col in df2.columns if str(col).startswith('0')]\n",
    "df2 = df2.fillna(MISSING_VALUE)\n",
    "df2['beacon_count'] = (df2[beacon_cols] != -100).sum(axis=1)\n",
    "print(df2.shape)\n",
    "\n",
    "df2 = df2[df2['beacon_count'] >= 5]\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21574ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop= anchor_point_df[anchor_point_df.Remove==\"remove\"].Mac.tolist()\n",
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c064bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df2.drop(columns= columns_to_drop)\n",
    "\n",
    "# Find common columns\n",
    "common_cols = df2.columns.intersection(data_set_df.columns)\n",
    "\n",
    "# Keep only common columns in df1\n",
    "df2_aligned = df2[common_cols].copy()\n",
    "df2_aligned[\"parent_zone_id\"]=df2[\"parent_zone_id\"]\n",
    "\n",
    "train_data, test_data = train_test_split(df2_aligned, test_size=0.2, random_state=42, \\\n",
    "                                         stratify=df2_aligned[\"Zone_id\"])\n",
    "\n",
    "X_train = train_data[[col for col in train_data.columns if col.startswith(\"0\")]]\n",
    "y_train_floor = train_data['parent_zone_id'] \n",
    "y_train = train_data['Zone_id']\n",
    "\n",
    "X_test = df1[[col for col in train_data.columns if col.startswith(\"0\")]] \n",
    "y_test_floor = df1['parent_zone_id'] \n",
    "y_test = df1['Zone_id'] \n",
    "df2_aligned.shape, df2.shape, df1.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, clf_rooms, clf_floor = train_variable(X_train, y_train_floor, y_train, save_models = False)\n",
    "predicted_rooms, predicted_floors = predict_variable(X_test, clf_floor, clf_rooms, selected_features)\n",
    "score = accuracy_score(y_test, predicted_rooms)\n",
    "print('Room Accuracy: {:.2f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d99151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape, data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb30265",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d = data_set_df[[\"Room_name\", 'tagId', 'Zone_id', \"timestamp\"]]\\\n",
    "    .merge(ground_truth_df[[\"Zone_id\", \"Room_Type\"]], on = \"Zone_id\", how=\"left\")\n",
    "result_d[\"Prediction\"] = predicted_rooms\n",
    "result_d[\"Accuracy\"] = np.where(result_d.Zone_id == result_d.Prediction, 100, 0)\n",
    "result_d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sliding_window_voting(df, window=5):\n",
    "    \"\"\"\n",
    "    Perform sliding window majority vote per Tag_id on 'Prediction'.\n",
    "    Tie-breaker: latest occurrence in the window.\n",
    "    \n",
    "    Returns dataframe with: Tag_id, timestamp, voted_prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by Tag_id and timestamp\n",
    "    df = df.sort_values([\"tagId\", \"timestamp\"]).copy()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for tag, group in df.groupby(\"tagId\"):\n",
    "        preds = group[\"Prediction\"].tolist()\n",
    "        timestamps = group[\"timestamp\"].tolist()\n",
    "        \n",
    "        # Sliding window\n",
    "        for i in range(len(preds)):\n",
    "            start_idx = max(0, i - window + 1)\n",
    "            window_preds = preds[start_idx:i+1]\n",
    "            \n",
    "            # Count occurrences\n",
    "            counts = Counter(window_preds)\n",
    "            max_count = max(counts.values())\n",
    "            # Candidates with max votes\n",
    "            candidates = [k for k, v in counts.items() if v == max_count]\n",
    "            \n",
    "            # Tie-breaker: select the latest occurrence in the window\n",
    "            for val in reversed(window_preds):\n",
    "                if val in candidates:\n",
    "                    voted = val\n",
    "                    break\n",
    "            \n",
    "            results.append({\n",
    "                \"tagId\": tag,\n",
    "                \"timestamp\": timestamps[i],\n",
    "                \"voted_prediction\": voted\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11483891",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df = sliding_window_voting(result_d, window=5)\n",
    "# voted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798168b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map_with_zones(\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    highlight_zones=None,\n",
    "    output_file=\"map_zones.png\",\n",
    "    title=\"Map with Zones\",\n",
    "    location_str=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the map with zones highlighted.\n",
    "    Only shows the map and zone polygons.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    image = plt.imread(map_file)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    if highlight_zones is None:\n",
    "        highlight_zones = []\n",
    "\n",
    "    # Draw all zones\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row.get(f'x{i+1}') for i in range(8) if pd.notnull(row.get(f'x{i+1}'))]\n",
    "        y_coords = [row.get(f'y{i+1}') for i in range(8) if pd.notnull(row.get(f'y{i+1}'))]\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = 'yellow' if zone_id in highlight_zones else 'none'\n",
    "            edge_color = 'red' if zone_id in highlight_zones else 'black'\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!='none'),\n",
    "                              facecolor=color, edgecolor=edge_color,\n",
    "                              alpha=0.3 if color!='none' else 0.8, linewidth=2)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "    # Set title\n",
    "    if location_str:\n",
    "        ax.set_title(f\"{title} ({location_str})\")\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "\n",
    "#     ax.set_xlabel(\"X coordinate\")\n",
    "#     ax.set_ylabel(\"Y coordinate\")\n",
    "    ax.set_xlim([0, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_with_zones(\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    highlight_zones=highlight_zones,\n",
    "    output_file=\"map_zones.png\",\n",
    "    title=None,\n",
    "    location_str=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697655b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voted_trajectory_with_time(\n",
    "    voted_df,\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    location_map=location_map,      # dict of locations\n",
    "    output_file=\"tag_trajectory_time.png\",\n",
    "    highlight_zones=None,\n",
    "    title=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot tag trajectories with:\n",
    "    - Dwell counts at zone centroids\n",
    "    - Transition counts along arrows\n",
    "    - Start (*) and end (X) markers\n",
    "    - Arrow colors show time progression (early  late)\n",
    "    - Highlight specified zones\n",
    "    - Include tag location in the title if location_map provided\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    image = plt.imread(map_file)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    if highlight_zones is None:\n",
    "        highlight_zones = []\n",
    "\n",
    "    # Draw all zones with optional highlight\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row.get(f'x{i+1}') for i in range(8) if pd.notnull(row.get(f'x{i+1}'))]\n",
    "        y_coords = [row.get(f'y{i+1}') for i in range(8) if pd.notnull(row.get(f'y{i+1}'))]\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            if zone_id in highlight_zones:\n",
    "                polygon_patch = mplPolygon(coords, closed=True, fill=True,\n",
    "                                           facecolor='yellow', edgecolor='red',\n",
    "                                           alpha=0.3, linewidth=2)\n",
    "            else:\n",
    "                polygon_patch = mplPolygon(coords, closed=True, fill=False,\n",
    "                                           edgecolor='black', linewidth=1, alpha=0.8)\n",
    "            ax.add_patch(polygon_patch)\n",
    "\n",
    "    cmap = plt.cm.viridis\n",
    "\n",
    "    for tag, group in voted_df.groupby('tagId'):\n",
    "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "        zones_visited = list(group['voted_prediction'])\n",
    "\n",
    "        # Determine location from location_map\n",
    "        location_str = \"\"\n",
    "        if location_map:\n",
    "            for loc, tag_ids in location_map.items():\n",
    "                if tag in tag_ids:\n",
    "                    location_str = loc\n",
    "                    break\n",
    "\n",
    "        # Extract centroids for zones\n",
    "        zone_centroids = {}\n",
    "        for zone in set(zones_visited):\n",
    "            room_row = ground_truth_df[ground_truth_df['Zone_id'] == zone].iloc[0]\n",
    "            x_coords = [room_row.get(f'x{i+1}') for i in range(8) if pd.notnull(room_row.get(f'x{i+1}'))]\n",
    "            y_coords = [room_row.get(f'y{i+1}') for i in range(8) if pd.notnull(room_row.get(f'y{i+1}'))]\n",
    "            if x_coords and y_coords:\n",
    "                polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "                if not polygon.is_valid:\n",
    "                    polygon = polygon.buffer(0)\n",
    "                centroid = polygon.centroid\n",
    "                zone_centroids[zone] = (centroid.x, centroid.y)\n",
    "\n",
    "        # Dwell and transition counts\n",
    "        from collections import defaultdict, Counter\n",
    "        dwell_counts = defaultdict(int)\n",
    "        i = 0\n",
    "        while i < len(zones_visited):\n",
    "            current_zone = zones_visited[i]\n",
    "            count = 1\n",
    "            while i + count < len(zones_visited) and zones_visited[i + count] == current_zone:\n",
    "                count += 1\n",
    "            dwell_counts[current_zone] += count\n",
    "            i += count\n",
    "\n",
    "        transition_counts = Counter()\n",
    "        for i in range(len(zones_visited)-1):\n",
    "            pair = (zones_visited[i], zones_visited[i+1])\n",
    "            transition_counts[pair] += 1\n",
    "\n",
    "        # Time normalization for arrows\n",
    "        timestamps = pd.to_datetime(group['timestamp'])\n",
    "        t_norm = (timestamps - timestamps.min()) / (timestamps.max() - timestamps.min())\n",
    "\n",
    "        # Draw arrows with time color\n",
    "        for i in range(len(zones_visited)-1):\n",
    "            z1, z2 = zones_visited[i], zones_visited[i+1]\n",
    "            x1, y1 = zone_centroids[z1]\n",
    "            x2, y2 = zone_centroids[z2]\n",
    "            arrow_color = cmap(t_norm.iloc[i])\n",
    "            arrow = FancyArrowPatch((x1, y1), (x2, y2),\n",
    "                                    arrowstyle='->', color=arrow_color,\n",
    "                                    mutation_scale=15, linewidth=2.5, alpha=0.8)\n",
    "            ax.add_patch(arrow)\n",
    "            # Annotate transition count\n",
    "            count = transition_counts[(z1, z2)]\n",
    "            dx, dy = x2 - x1, y2 - y1\n",
    "            xm, ym = x1 + 0.4*dx - 0.3*dy/np.hypot(dx, dy), y1 + 0.4*dy + 0.3*dx/np.hypot(dx, dy)\n",
    "            ax.text(xm, ym, str(count), color='purple', fontsize=12, fontweight='bold',\n",
    "                    ha='center', va='center')\n",
    "\n",
    "        # Dwell counts at centroids\n",
    "        for zone, count in dwell_counts.items():\n",
    "            x, y = zone_centroids[zone]\n",
    "            ax.text(x, y + 0.5, str(count), color='blue', fontsize=14,\n",
    "                    ha='center', va='center')\n",
    "\n",
    "        # Start/end markers\n",
    "        start_zone = zones_visited[0]\n",
    "        end_zone = zones_visited[-1]\n",
    "        ax.scatter(*zone_centroids[start_zone], color='red', s=100, marker='*', label=f\"Start Tag {tag}\")\n",
    "        ax.scatter(*zone_centroids[end_zone], color='red', s=100, marker='X', label=f\"End Tag {tag}\")\n",
    "\n",
    "        # Title includes tag id and location\n",
    "        ax.set_title(f\"LocationAI_ Tag {tag} ({location_str})\")\n",
    "\n",
    "    ax.set_xlabel(\"X coordinate\")\n",
    "    ax.set_ylabel(\"Y coordinate\")\n",
    "    ax.set_xlim([0, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "    plt.colorbar(plt.cm.ScalarMappable(cmap=cmap), ax=ax, label=\"Time progression (early  late)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800e6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadc712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_zones = [\"30543\", \"30512\", \"30513\", \"30515\", \"30540\", \"30518\", \"30519\", \\\n",
    "#                   \"30520\", \"30525\", \"30526\", \"30538\", \"30539\"]  # zones to highlight\n",
    "\n",
    "plot_voted_trajectory_with_time(\n",
    "    voted_df=voted_df[voted_df.tagId==\"7000480\"],\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file=map_file,\n",
    "#     output_file=\"tag_voted_trajectory_highlight.png\",\n",
    "    highlight_zones=highlight_zones\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67481c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"Plot_LocationAI_Nov25\"\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "for tag_id, tag_df in voted_df.groupby(\"tagId\"):\n",
    "    output_file = os.path.join(output_folder, f\"LocationAI_tag_{tag_id}.png\")\n",
    "    \n",
    "    plot_voted_trajectory_with_time(\n",
    "        voted_df=tag_df,\n",
    "        ground_truth_df=ground_truth_df,\n",
    "        map_file=map_file,\n",
    "        output_file=output_file,\n",
    "        highlight_zones=highlight_zones,\n",
    "        title=f\"LocationAI_ Tag {tag_id}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97469258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dfa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# Set embed limit to unlimited\n",
    "mpl.rcParams['animation.embed_limit'] = 2**30  # ~1 GB (effectively unlimited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from PIL import Image\n",
    "\n",
    "def animate_voted_trajectory(tag_df, ground_truth_df, map_file,\n",
    "                             highlight_zones=None,\n",
    "                             output_file=\"trajectory.mp4\",\n",
    "                             fps=5,\n",
    "                             location_str=\"\"):\n",
    "    \"\"\"\n",
    "    Animate tag trajectory with dwell/transition counts and optional zone highlights.\n",
    "    \n",
    "    Parameters:\n",
    "    - tag_df: DataFrame containing tag trajectory with 'voted_prediction' and 'timestamp'.\n",
    "    - ground_truth_df: DataFrame containing zone coordinates and 'Zone_id'.\n",
    "    - map_file: either file path to image OR a pre-loaded NumPy array (image).\n",
    "    - highlight_zones: list of Zone_ids to highlight.\n",
    "    - output_file: file path to save the animation (mp4).\n",
    "    - fps: frames per second for animation.\n",
    "    - location_str: optional string to include in the title.\n",
    "    \n",
    "    Features:\n",
    "    - Zones drawn with optional highlights\n",
    "    - Trajectory plotted as line\n",
    "    - Start marker: '*', End marker: 'X'\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Load image if map_file is a path, otherwise assume it's an array\n",
    "    if isinstance(map_file, str):\n",
    "        image = plt.imread(map_file)\n",
    "    else:\n",
    "        image = map_file\n",
    "\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    if highlight_zones is None:\n",
    "        highlight_zones = []\n",
    "\n",
    "    # Draw zones and compute centroids\n",
    "    zone_centroids = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row.get(f'x{i+1}') for i in range(8) if pd.notnull(row.get(f'x{i+1}'))]\n",
    "        y_coords = [row.get(f'y{i+1}') for i in range(8) if pd.notnull(row.get(f'y{i+1}'))]\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = 'yellow' if zone_id in highlight_zones else 'none'\n",
    "            edge_color = 'red' if zone_id in highlight_zones else 'black'\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color != 'none'),\n",
    "                              facecolor=color, edgecolor=edge_color,\n",
    "                              alpha=0.3 if color != 'none' else 0.8)\n",
    "            ax.add_patch(poly)\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "    # Prepare trajectory line and markers\n",
    "    zones_visited = list(tag_df.sort_values('timestamp')['voted_prediction'])\n",
    "    x = [zone_centroids[z][0] for z in zones_visited]\n",
    "    y = [zone_centroids[z][1] for z in zones_visited]\n",
    "\n",
    "    traj_line, = ax.plot([], [], lw=2, color='red')\n",
    "    start_marker = ax.scatter([], [], s=100, marker='*', color='red')\n",
    "    end_marker = ax.scatter([], [], s=100, marker='X', color='red')\n",
    "\n",
    "    def init():\n",
    "        traj_line.set_data([], [])\n",
    "        start_marker.set_offsets([[0, 0]])\n",
    "        end_marker.set_offsets([[0, 0]])\n",
    "        return traj_line, start_marker, end_marker\n",
    "\n",
    "    def update(frame):\n",
    "        traj_line.set_data(x[:frame+1], y[:frame+1])\n",
    "        start_marker.set_offsets([x[0], y[0]])\n",
    "        end_marker.set_offsets([x[frame], y[frame]])\n",
    "        return traj_line, start_marker, end_marker\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(x), init_func=init,\n",
    "                         blit=True, repeat=False)\n",
    "\n",
    "    ax.set_xlim([0, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "\n",
    "    # Set title with tag ID and location\n",
    "    tag_id = tag_df['tagId'].iloc[0]\n",
    "    if location_str:\n",
    "        ax.set_title(f\"LocationAI_ Tag {tag_id} ({location_str})\")\n",
    "    else:\n",
    "        ax.set_title(f\"Animated trajectory for Tag {tag_id}\")\n",
    "\n",
    "    if output_file:\n",
    "        anim.save(output_file, fps=fps, dpi=200)\n",
    "\n",
    "    plt.show()\n",
    "    return anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c67b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file1 = mpimg.imread(\"map_file1.png\")\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(map_file1)\n",
    "plt.axis('off')  # hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d021af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "\n",
    "\n",
    "anim = animate_voted_trajectory(\n",
    "    voted_df[voted_df.tagId==\"7000480\"],\n",
    "    ground_truth_df,\n",
    "    map_file1\n",
    ")\n",
    "\n",
    "HTML(anim.to_jshtml())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04909d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b77ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_multiple_tags(voted_df, ground_truth_df, map_file, location_map,\n",
    "                                        output_folder=\"tag_videos\", highlight_zones=None, fps=5):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    tag_ids = voted_df['tagId'].unique()\n",
    "    for tag in tag_ids:\n",
    "        print(f\"Processing Tag {tag}...\")\n",
    "        tag_df = voted_df[voted_df['tagId'] == tag].sort_values('timestamp')\n",
    "        if tag_df.empty:\n",
    "            print(f\"No data for Tag {tag}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Determine location from location_map\n",
    "        location_str = \"\"\n",
    "        for loc, tags in location_map.items():\n",
    "            if tag in tags:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "        output_file = os.path.join(output_folder, f\"LocationAI_Tag_{tag}_{location_str}.mp4\")\n",
    "\n",
    "        # Pass location_str to the animation\n",
    "        animate_voted_trajectory(\n",
    "            tag_df,\n",
    "            ground_truth_df,\n",
    "            map_file,\n",
    "            highlight_zones=highlight_zones,\n",
    "            output_file=output_file,\n",
    "            fps=fps,\n",
    "            location_str=location_str\n",
    "        )\n",
    "\n",
    "        print(f\"Saved animation  {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7be5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_multiple_tags(\n",
    "    voted_df=voted_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file=map_file1,\n",
    "    location_map=location_map,\n",
    "    output_folder=\"videos_Nov25\",\n",
    "    highlight_zones=highlight_zones,\n",
    "    fps=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8ef12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e682e99",
   "metadata": {},
   "source": [
    "## Both Videos Vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def merge_vertical(video_top, video_bottom, output_video):\n",
    "    import os, subprocess\n",
    "\n",
    "    out_dir = os.path.dirname(output_video)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Use ffmpeg scale filter to match widths\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", video_top,\n",
    "        \"-i\", video_bottom,\n",
    "        \"-filter_complex\",\n",
    "        \"[1:v]scale=2800:-1[v1];[0:v][v1]vstack=inputs=2\",\n",
    "        \"-c:v\", \"libx264\", \"-crf\", \"18\", \"-preset\", \"medium\",\n",
    "        output_video\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Generated vertical comparison  {output_video}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_merge_all_pairs_vertical(videos_folder=\"videos_Nov25\"):\n",
    "    files = os.listdir(videos_folder)\n",
    "\n",
    "    # Regex to extract components\n",
    "    pattern = r\"(LocationAI|NLOS)_Tag_(\\d+)_(.*)\\.mp4\"\n",
    "\n",
    "    # Store discovered files\n",
    "    locationAI_files = {}\n",
    "    nlos_files = {}\n",
    "\n",
    "    for f in files:\n",
    "        match = re.match(pattern, f)\n",
    "        if match:\n",
    "            prefix, tag, location = match.groups()\n",
    "            key = (tag, location)\n",
    "\n",
    "            if prefix == \"LocationAI\":\n",
    "                locationAI_files[key] = os.path.join(videos_folder, f)\n",
    "            else:\n",
    "                nlos_files[key] = os.path.join(videos_folder, f)\n",
    "\n",
    "    # Match pairs and merge vertically\n",
    "    for key in locationAI_files:\n",
    "        if key in nlos_files:\n",
    "            tag, location = key\n",
    "            top = locationAI_files[key]\n",
    "            bottom = nlos_files[key]\n",
    "\n",
    "            output_name = f\"Duress_{tag}_{location}.mp4\"\n",
    "            output_path = os.path.join(videos_folder, output_name)\n",
    "\n",
    "            print(f\"\\n Pair found for Tag {tag} ({location})\")\n",
    "            print(f\"   TOP   : {top}\")\n",
    "            print(f\"   BOTTOM: {bottom}\")\n",
    "            print(\"    generating vertical comparison...\")\n",
    "\n",
    "            merge_vertical(top, bottom, output_path)\n",
    "        else:\n",
    "            print(f\" Missing NLOS file for Tag {key[0]} ({key[1]})\")\n",
    "\n",
    "    print(\"\\n All available pairs processed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fce068",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merge_all_pairs_vertical(videos_folder=\"videos_Nov25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168120d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output=\"combined_video.mp4\",\n",
    "        x_min_fixed=None\n",
    "    ):\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "    import pandas as pd\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location name\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand NLOS centroid (assume one column: \"Centroid_NLOS\")\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] if isinstance(v, (list,tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] if isinstance(v, (list,tuple)) else np.nan)\n",
    "\n",
    "    # Load map\n",
    "    if isinstance(map_file, str):\n",
    "        image = mpimg.imread(map_file)\n",
    "    else:\n",
    "        image = map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # Draw zones\n",
    "    zone_centroids = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row[f\"x{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row[f\"y{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = \"yellow\" if highlight_zones and zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "    # Prepare NLOS scatter + arrows\n",
    "    nlos_scatter = ax.scatter([], [], s=30, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # Prepare LocationAI stepwise path\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"red\", linewidth=2)\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"blue\")\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\")\n",
    "\n",
    "    # Animation update\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # remove old arrows\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # draw arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI UPDATE (stepwise movement)\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames,\n",
    "                        interval=200, blit=True)\n",
    "\n",
    "    # Axis setup\n",
    "    if x_min_fixed is not None:\n",
    "        ax.set_xlim(left=x_min_fixed)\n",
    "    ax.set_ylim([0,28])\n",
    "\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red) Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    # Save\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718cd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ed5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.Tag_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_min_fixed=None\n",
    "    ):\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Final output path (tag_id.mp4)\n",
    "    output = f\"{output_folder}/{tag_id}.mp4\"\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location name\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand NLOS centroid\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] if isinstance(v, (list,tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] if isinstance(v, (list,tuple)) else np.nan)\n",
    "\n",
    "    # Load map\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # Draw zones\n",
    "    zone_centroids = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row[f\"x{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row[f\"y{i+1}\"] for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = \"yellow\" if highlight_zones and zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Compute zone centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)     # BLUE line\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"blue\")  # BLUE start\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\")  # BLUE end\n",
    "\n",
    "    # Animation update\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # Remove previous arrows\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw new arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI UPDATE (Blue) ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Axis setup\n",
    "    if x_min_fixed is not None:\n",
    "        ax.set_xlim(left=x_min_fixed)\n",
    "    ax.set_ylim([0,28])\n",
    "\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    # Save the animation\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72556e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_combined_results(\n",
    "        final_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file1,\n",
    "        tag_id='7000140',\n",
    "        location_map=location_map,\n",
    "        highlight_zones=highlight_zones,\n",
    "        output_folder=\"videos\",\n",
    "        x_min_fixed=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128440c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = final_df[\"Tag_id\"].unique()\n",
    "\n",
    "for tag in all_tags:\n",
    "    print(f\"Processing Tag {tag}...\")\n",
    "    animate_combined_results(\n",
    "        final_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file1,\n",
    "        tag_id=tag,\n",
    "        location_map=location_map,\n",
    "        highlight_zones=highlight_zones,\n",
    "        output_folder=\"videos\",\n",
    "        x_min_fixed=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb5748",
   "metadata": {},
   "source": [
    "## USE THIS to combine both Videso into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440abc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_offset=0,\n",
    "        y_offset=0\n",
    "    ):\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Final output path (tag_id.mp4)\n",
    "    output = os.path.join(output_folder, f\"{tag_id}.mp4\")\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location name\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand NLOS centroid and apply offsets\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] + x_offset if isinstance(v, (list,tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] + y_offset if isinstance(v, (list,tuple)) else np.nan)\n",
    "\n",
    "    # Load map\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # Draw zones and compute centroids\n",
    "    zone_centroids = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row[f\"x{i+1}\"] + x_offset for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row[f\"y{i+1}\"] + y_offset for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = \"yellow\" if highlight_zones and zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Compute zone centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)          # BLUE line\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"blue\") # BLUE start\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\") # BLUE end\n",
    "\n",
    "    # Animation update\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # Remove previous arrows\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw new arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI UPDATE (Blue) ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Axis setup\n",
    "    ax.set_xlim([0,65])\n",
    "    ax.set_ylim([0,28])\n",
    "\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    # Save the animation\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file1 = mpimg.imread(\"map_file2.png\")\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(map_file1)\n",
    "plt.axis('off')  # hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58ffa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c69e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c460b5bd",
   "metadata": {},
   "source": [
    "### Add strat and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_offset=0,\n",
    "        y_offset=0\n",
    "    ):\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Final output path (tag_id.mp4)\n",
    "    output = os.path.join(output_folder, f\"{tag_id}.mp4\")\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location name\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand NLOS centroid and apply offsets\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] + x_offset if isinstance(v, (list,tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] + y_offset if isinstance(v, (list,tuple)) else np.nan)\n",
    "\n",
    "    # Load map\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "    \n",
    "    \n",
    " #################       # ----- Start & End Point Overlay -----\n",
    "    # Example: replace with your real coordinates\n",
    "    start_x, start_y = 6, 16   # Start point\n",
    "    end_x, end_y     = 12, 17  # End point\n",
    "\n",
    "    start_marker = ax.scatter(start_x, start_y, \n",
    "                              s=200, color=\"green\", marker=\"o\", edgecolor=\"black\", linewidth=1.5)\n",
    "    ax.text(start_x, start_y + 0.5, \"Start\", color=\"green\", fontsize=12, weight=\"bold\")\n",
    "\n",
    "    end_marker = ax.scatter(end_x, end_y, \n",
    "                            s=200, color=\"purple\", marker=\"s\", edgecolor=\"black\", linewidth=1.5)\n",
    "    ax.text(end_x, end_y + 0.5, \"End\", color=\"purple\", fontsize=12, weight=\"bold\")\n",
    "\n",
    "################\n",
    "\n",
    "    # Draw zones and compute centroids\n",
    "    zone_centroids = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = row['Zone_id']\n",
    "        x_coords = [row[f\"x{i+1}\"] + x_offset for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row[f\"y{i+1}\"] + y_offset for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "            color = \"yellow\" if highlight_zones and zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Compute zone centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)          # BLUE line\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"blue\") # BLUE start\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\") # BLUE end\n",
    "\n",
    "    # Animation update\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # Remove previous arrows\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw new arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI UPDATE (Blue) ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Axis setup\n",
    "    ax.set_xlim([0,65])\n",
    "    ax.set_ylim([0,28])\n",
    "\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    # Save the animation\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b672a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc3649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc626c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cacffc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7aa6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558dfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_offset=0,\n",
    "        y_offset=0\n",
    "    ):\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Final output path\n",
    "    output = os.path.join(output_folder, f\"{tag_id}.mp4\")\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location string\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand centroid values\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] + x_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] + y_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "\n",
    "    # Load floor map image\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "        # --- Draw highlighted zones + store centroids ---\n",
    "    zone_centroids = {}\n",
    "\n",
    "    # Manual numbering for highlight zones\n",
    "    manual_zone_numbers = {\n",
    "        '30558': 1,\n",
    "        '30509': 2,\n",
    "        '30553': 4,\n",
    "        '35182': 3,\n",
    "        '30514': 6,\n",
    "        '35181': 7,\n",
    "        '30516': 8,\n",
    "        '30512': 5,\n",
    "        '30518': 9,\n",
    "        '30533': 13,\n",
    "        '30520': 10,\n",
    "        '30524': 11,\n",
    "        '30525': 12\n",
    "    }\n",
    "\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = str(row['Zone_id'])  # convert to string to match manual_zone_numbers keys\n",
    "        x_coords = [row.get(f\"x{i+1}\") + x_offset for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") + y_offset for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "\n",
    "            color = \"yellow\" if not highlight_zones or zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Save centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "            # ---- Add manual zone numbering ----\n",
    "            if zone_id in manual_zone_numbers:\n",
    "                num = manual_zone_numbers[zone_id]\n",
    "                cx, cy = zone_centroids[zone_id]\n",
    "\n",
    "                ax.text(cx, cy, str(num),\n",
    "                        color=\"blue\",\n",
    "                        fontsize=12,\n",
    "                        fontweight=\"bold\",\n",
    "                        ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"blue\")\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\")\n",
    "\n",
    "    # Animation update function\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        if len(x_nlos) > 0:\n",
    "            nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)))\n",
    "        else:\n",
    "            nlos_scatter.set_offsets(np.empty((0,2)))\n",
    "\n",
    "        # Remove arrows from previous frame\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw movement arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\",\n",
    "                                      color=\"red\",\n",
    "                                      mutation_scale=10,\n",
    "                                      alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI Update ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    # Run animation\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Final configuration\n",
    "    ax.set_xlim([0,65])\n",
    "    ax.set_ylim([0,28])\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4fdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_offset=0,\n",
    "        y_offset=0,\n",
    "        start_coord=None,   # optional start coordinate (x, y)\n",
    "        end_coord=None,     # optional end coordinate (x, y)\n",
    "        zone_number_offset=(0.5, 0.5)  # fixed offset for all zone numbers\n",
    "    ):\n",
    "\n",
    "    import os\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output = os.path.join(output_folder, f\"{tag_id}.mp4\")\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location string\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand centroid values\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] + x_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] + y_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "\n",
    "    # Load floor map image\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # --- Draw highlighted zones + store centroids ---\n",
    "    zone_centroids = {}\n",
    "\n",
    "    # Manual numbering for highlight zones\n",
    "    manual_zone_numbers = {\n",
    "        '30558': 1,\n",
    "        '30509': 2,\n",
    "        '30553': 4,\n",
    "        '35182': 3,\n",
    "        '30514': 6,\n",
    "        '35181': 7,\n",
    "        '30516': 8,\n",
    "        '30512': 5,\n",
    "        '30518': 9,\n",
    "        '30533': 13,\n",
    "        '30520': 10,\n",
    "        '30524': 11,\n",
    "        '30525': 12,\n",
    "        \"30530\":13,\n",
    "        \"30544\": 14,\n",
    "        \"30545\":15\n",
    "    }\n",
    "\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = str(row['Zone_id'])\n",
    "        x_coords = [row.get(f\"x{i+1}\") + x_offset for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") + y_offset for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "\n",
    "            color = \"yellow\" if not highlight_zones or zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Save centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "            # Add manual zone numbering with fixed offset\n",
    "            if zone_id in manual_zone_numbers:\n",
    "                num = manual_zone_numbers[zone_id]\n",
    "                cx, cy = zone_centroids[zone_id]\n",
    "                dx, dy = zone_number_offset\n",
    "                ax.text(cx + dx, cy + dy, str(num),\n",
    "                        color=\"blue\", fontsize=12, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"green\")\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"red\")\n",
    "\n",
    "    # Add custom start/end labels if coordinates provided\n",
    "    if start_coord:\n",
    "        ax.scatter(*start_coord, s=100, marker=\"*\", color=\"green\")\n",
    "        ax.text(start_coord[0], start_coord[1], \"Start\", color=\"green\", fontsize=12, fontweight=\"bold\",\n",
    "                ha=\"left\", va=\"bottom\")\n",
    "    if end_coord:\n",
    "        ax.scatter(*end_coord, s=100, marker=\"X\", color=\"red\")\n",
    "        ax.text(end_coord[0], end_coord[1], \"End\", color=\"red\", fontsize=12, fontweight=\"bold\",\n",
    "                ha=\"left\", va=\"bottom\")\n",
    "\n",
    "    # Animation update function\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # Remove arrows from previous frame\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw movement arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI Update ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            if not start_coord:\n",
    "                loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            if not end_coord:\n",
    "                loc_end.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end] + nlos_arrows\n",
    "\n",
    "    # Run animation\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Final configuration\n",
    "    ax.set_xlim([0,65])\n",
    "    ax.set_ylim([0,28])\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0dab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ac233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_zones=['30558',\n",
    " '30509',\n",
    " '30553',\n",
    " '35182',\n",
    " '30514',\n",
    " '35181',\n",
    " '30516',\n",
    " '30512',\n",
    " '30518',\n",
    " '30533',\n",
    " '30520',\n",
    " '30524',\n",
    " '30525', \"30530\", \"30544\", \"30545\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16200ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=None,\n",
    "        highlight_zones=None,\n",
    "        output_folder=\"outputs\",\n",
    "        x_offset=0,\n",
    "        y_offset=0,\n",
    "        start_coord=None,   # optional start coordinate (x, y)\n",
    "        end_coord=None,     # optional end coordinate (x, y)\n",
    "        zone_number_offset=(0.5, 0.5)  # fixed offset for all zone numbers\n",
    "    ):\n",
    "\n",
    "    import os\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "    from matplotlib.patches import Polygon as mplPolygon, FancyArrowPatch\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output = os.path.join(output_folder, f\"{tag_id}.mp4\")\n",
    "\n",
    "    # Extract NLOS data\n",
    "    df_nlos = nlos_df[nlos_df[\"Tag_id\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_nlos.empty:\n",
    "        print(f\"No NLOS data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Extract LocationAI data\n",
    "    df_loc = voted_df[voted_df[\"tagId\"] == tag_id].sort_values(\"timestamp\").copy()\n",
    "    if df_loc.empty:\n",
    "        print(f\"No LocationAI data for Tag {tag_id}\")\n",
    "        return\n",
    "\n",
    "    # Determine location string\n",
    "    location_str = \"\"\n",
    "    if location_map:\n",
    "        for loc, tag_list in location_map.items():\n",
    "            if tag_id in tag_list:\n",
    "                location_str = loc\n",
    "                break\n",
    "\n",
    "    # Expand centroid values\n",
    "    xy = df_nlos[\"Centroid_NLOS\"].apply(lambda v: ast.literal_eval(v) if isinstance(v, str) else v)\n",
    "    df_nlos[\"x\"] = xy.apply(lambda v: v[0] + x_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "    df_nlos[\"y\"] = xy.apply(lambda v: v[1] + y_offset if isinstance(v, (list, tuple)) else np.nan)\n",
    "\n",
    "    # Load floor map image\n",
    "    image = mpimg.imread(map_file) if isinstance(map_file, str) else map_file\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.imshow(image, extent=[0,65,0,28], aspect='auto')\n",
    "\n",
    "    # --- Draw highlighted zones + store centroids ---\n",
    "    zone_centroids = {}\n",
    "\n",
    "    # Manual numbering for highlight zones\n",
    "    manual_zone_numbers = {\n",
    "        '30558': 1,\n",
    "        '30509': 2,\n",
    "        '30553': 4,\n",
    "        '35182': 3,\n",
    "        '30514': 6,\n",
    "        '35181': 7,\n",
    "        '30516': 8,\n",
    "        '30512': 5,\n",
    "        '30518': 9,\n",
    "        '30533': 13,\n",
    "        '30520': 10,\n",
    "        '30524': 11,\n",
    "        '30525': 12,\n",
    "        \"30530\":14,\n",
    "        \"30544\": 15,\n",
    "        \"30545\":16\n",
    "    }\n",
    "\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        zone_id = str(row['Zone_id'])\n",
    "        x_coords = [row.get(f\"x{i+1}\") + x_offset for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") + y_offset for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if x_coords and y_coords:\n",
    "            coords = list(zip(x_coords, y_coords))\n",
    "\n",
    "            color = \"yellow\" if not highlight_zones or zone_id in highlight_zones else \"none\"\n",
    "            edge_color = \"orange\" if highlight_zones and zone_id in highlight_zones else \"black\"\n",
    "\n",
    "            poly = mplPolygon(coords, closed=True, fill=(color!=\"none\"),\n",
    "                              facecolor=color, edgecolor=edge_color, alpha=0.3)\n",
    "            ax.add_patch(poly)\n",
    "\n",
    "            # Save centroid\n",
    "            zone_centroids[zone_id] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "            # Add manual zone numbering with fixed offset\n",
    "            if zone_id in manual_zone_numbers:\n",
    "                num = manual_zone_numbers[zone_id]\n",
    "                cx, cy = zone_centroids[zone_id]\n",
    "                dx, dy = zone_number_offset\n",
    "                ax.text(cx + dx, cy + dy, str(num),\n",
    "                        color=\"green\", fontsize=12, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # -------- NLOS Scatter (Red) --------\n",
    "    nlos_scatter = ax.scatter([], [], s=20, color=\"red\")\n",
    "    nlos_arrows = []\n",
    "\n",
    "    # -------- LocationAI Path (Blue) --------\n",
    "    zone_list = list(df_loc[\"voted_prediction\"])\n",
    "    loc_x = [zone_centroids[z][0] for z in zone_list]\n",
    "    loc_y = [zone_centroids[z][1] for z in zone_list]\n",
    "\n",
    "    loc_line, = ax.plot([], [], color=\"blue\", linewidth=2)        # path line\n",
    "    loc_start = ax.scatter([], [], s=100, marker=\"*\", color=\"green\")  # start\n",
    "    loc_end   = ax.scatter([], [], s=100, marker=\"X\", color=\"red\")    # end\n",
    "    loc_current = ax.scatter([], [], s=100, marker=\"X\", color=\"blue\") # moving current position\n",
    "\n",
    "    # Add custom start/end points if coordinates provided\n",
    "    if start_coord:\n",
    "        ax.scatter(*start_coord, s=100, marker=\"*\", color=\"green\")\n",
    "        ax.text(start_coord[0], start_coord[1], \"Start\", color=\"green\", fontsize=12, fontweight=\"bold\",\n",
    "                ha=\"left\", va=\"bottom\")\n",
    "    if end_coord:\n",
    "        ax.scatter(*end_coord, s=100, marker=\"X\", color=\"red\")\n",
    "        ax.text(end_coord[0], end_coord[1], \"End\", color=\"red\", fontsize=12, fontweight=\"bold\",\n",
    "                ha=\"left\", va=\"bottom\")\n",
    "\n",
    "    # Animation update function\n",
    "    max_frames = max(len(df_nlos), len(loc_x))\n",
    "\n",
    "    def update(frame):\n",
    "        # --- NLOS UPDATE ---\n",
    "        x_nlos = df_nlos[\"x\"].values[:frame+1]\n",
    "        y_nlos = df_nlos[\"y\"].values[:frame+1]\n",
    "        mask = (~np.isnan(x_nlos)) & (~np.isnan(y_nlos))\n",
    "        x_nlos = x_nlos[mask]\n",
    "        y_nlos = y_nlos[mask]\n",
    "\n",
    "        nlos_scatter.set_offsets(np.column_stack((x_nlos, y_nlos)) if len(x_nlos)>0 else np.empty((0,2)))\n",
    "\n",
    "        # Remove arrows from previous frame\n",
    "        for arr in nlos_arrows:\n",
    "            arr.remove()\n",
    "        nlos_arrows.clear()\n",
    "\n",
    "        # Draw NLOS movement arrows\n",
    "        if len(x_nlos) > 1:\n",
    "            for i in range(len(x_nlos)-1):\n",
    "                arr = FancyArrowPatch((x_nlos[i], y_nlos[i]),\n",
    "                                      (x_nlos[i+1], y_nlos[i+1]),\n",
    "                                      arrowstyle=\"->\", color=\"red\",\n",
    "                                      mutation_scale=10, alpha=0.7)\n",
    "                ax.add_patch(arr)\n",
    "                nlos_arrows.append(arr)\n",
    "\n",
    "        # --- LocationAI Update ---\n",
    "        if frame < len(loc_x):\n",
    "            loc_line.set_data(loc_x[:frame+1], loc_y[:frame+1])\n",
    "            \n",
    "            if not start_coord:\n",
    "                loc_start.set_offsets([loc_x[0], loc_y[0]])\n",
    "            if not end_coord:\n",
    "                loc_end.set_offsets([loc_x[-1], loc_y[-1]])\n",
    "            \n",
    "            # Moving current location marker (blue cross)\n",
    "            loc_current.set_offsets([loc_x[frame], loc_y[frame]])\n",
    "\n",
    "        return [nlos_scatter, loc_line, loc_start, loc_end, loc_current] + nlos_arrows\n",
    "\n",
    "    # Run animation\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
    "\n",
    "    # Final configuration\n",
    "    ax.set_xlim([0,65])\n",
    "    ax.set_ylim([0,28])\n",
    "    ax.set_title(f\"LocationAI (Blue) + NLOS (Red)  Tag {tag_id} ({location_str})\")\n",
    "\n",
    "    writer = FFMpegWriter(fps=5)\n",
    "    ani.save(output, writer=writer)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined video  {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e973120",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_combined_results(\n",
    "    final_df,\n",
    "    voted_df,\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    tag_id='7000140',\n",
    "    location_map=location_map,\n",
    "    highlight_zones=highlight_zones,\n",
    "    output_folder=\"video_Nov25_data_new\",\n",
    "    start_coord=(5,16),\n",
    "    end_coord=(10,17.3),\n",
    "    zone_number_offset=(0, 0.5) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507da9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6668deb9",
   "metadata": {},
   "source": [
    "## For all Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_all_tags_combined(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        location_map=location_map,\n",
    "        highlight_zones=highlight_zones,\n",
    "        output_folder=\"video_Nov19_data\",\n",
    "        x_offset=0,\n",
    "        y_offset=0,\n",
    "        start_coord=None,   # optional start coordinate (x, y)\n",
    "        end_coord=None,     # optional end coordinate (x, y)\n",
    "        zone_number_offset=(0.5, 0.5)\n",
    "    ):\n",
    "    import os\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all unique tag IDs from NLOS and voted data\n",
    "    tags_nlos = nlos_df[\"Tag_id\"].unique()\n",
    "    tags_loc  = voted_df[\"tagId\"].unique()\n",
    "    all_tags = sorted(set(tags_nlos) | set(tags_loc))  # union of tags\n",
    "    \n",
    "    for tag_id in all_tags:\n",
    "        print(f\"Processing Tag {tag_id}...\")\n",
    "\n",
    "        animate_combined_results(\n",
    "        nlos_df,\n",
    "        voted_df,\n",
    "        ground_truth_df,\n",
    "        map_file,\n",
    "        tag_id,\n",
    "        location_map=location_map,\n",
    "        highlight_zones=highlight_zones,\n",
    "        output_folder=\"video_Nov19_data_new\",\n",
    "        x_offset=0,\n",
    "        y_offset=0,\n",
    "        start_coord=None,   # optional start coordinate (x, y)\n",
    "        end_coord=None,     # optional end coordinate (x, y)\n",
    "        zone_number_offset=(0.5, 0.5)  # fixed offset for all zone numbers\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_all_tags_combined(\n",
    "    final_df,\n",
    "    voted_df,\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    location_map=location_map,\n",
    "    highlight_zones=highlight_zones,\n",
    "    output_folder=\"video_Nov25_data_new\",\n",
    "    start_coord=(5,16),\n",
    "    end_coord=(10,17.3),\n",
    "    zone_number_offset=(0, 0.5) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1abfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1WPcT_OjSaguNAQ9AnkpmmUV_GDiijIq5",
     "timestamp": 1717696268126
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa6ad618d5445e7a2343db292d10e8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a8df1c1c1f1460695b2fdd40ab5d774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3887145dd10742d2aaec48b48fbf209f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841ae5e6f05644f4a06df98a4b3d3e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "868e78ed1be844ff89c007db86f1c77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8810a66e33784737a04f2d4de120a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a78544536d5d4e87b691714c15e379a5",
       "IPY_MODEL_ece131d2b91b40e28153ed88cb397481",
       "IPY_MODEL_919d61916e484f10a410b26b643574bf"
      ],
      "layout": "IPY_MODEL_3887145dd10742d2aaec48b48fbf209f"
     }
    },
    "89cc3e4005b640cab78311861d917f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919d61916e484f10a410b26b643574bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841ae5e6f05644f4a06df98a4b3d3e56",
      "placeholder": "",
      "style": "IPY_MODEL_0aa6ad618d5445e7a2343db292d10e8b",
      "value": "0/100[00:00&lt;?,?it/s]"
     }
    },
    "a78544536d5d4e87b691714c15e379a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89cc3e4005b640cab78311861d917f9c",
      "placeholder": "",
      "style": "IPY_MODEL_e5079de1d27140eb9f6c4fabe6f199f5",
      "value": "0%"
     }
    },
    "e5079de1d27140eb9f6c4fabe6f199f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece131d2b91b40e28153ed88cb397481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a8df1c1c1f1460695b2fdd40ab5d774",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_868e78ed1be844ff89c007db86f1c77e",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
