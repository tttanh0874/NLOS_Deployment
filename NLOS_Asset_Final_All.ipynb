{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from typing import Type, List, Dict, Tuple, Set\n",
    "import argparse\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.externals.joblib import parallel_backend, Parallel, delayed\n",
    "except ImportError:\n",
    "    import joblib\n",
    "    from joblib import parallel_backend, Parallel, delayed\n",
    "    \n",
    "import pandas as pd\n",
    "import json, ijson\n",
    "import os, sys, uuid\n",
    "from pykalman import KalmanFilter\n",
    "from PIL import Image\n",
    "import math\n",
    "import ast\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from geopy.distance import geodesic, distance\n",
    "from geopy import Point\n",
    "from shapely.geometry import Point, Polygon as ShapelyPolygon\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "import ast\n",
    "\n",
    "from collections import defaultdict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "CHANNELS = [37,38,39]\n",
    "N_ESTIMATORS = 100\n",
    "MISSING_VALUE = -100\n",
    "DEBUG_LOGGING = False\n",
    "S3_CACHING_BUCKET = 'cognosos-ml-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_data_woc(scan: List[Dict]) -> Dict:\n",
    "    # Parse each scan to get maximum reading for each MAC address in specified channels\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            readings_by_mac_addr_and_channel[mac_addr] += readings\n",
    "    return {mac_addr: int(max(readings)) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if readings}\n",
    "\n",
    "\n",
    "def process_training(data_filepath: str) -> List[Dict]:\n",
    "    X = []\n",
    "\n",
    "    # parse it incrementally\n",
    "    with open(data_filepath, 'r') as f:\n",
    "        # reads the JSON incrementally\n",
    "        objects = ijson.items(f, 'item') \n",
    "\n",
    "        print('Done loading JSON incrementally')\n",
    "\n",
    "        for scan in objects:\n",
    "            \n",
    "            Zone_id = str(scan['zoneId'])\n",
    "            Room_name = str(scan['zoneName'])\n",
    "            parent_zone_id = str(scan['parentZoneId'])\n",
    "            tagId = scan['tagId']\n",
    "            timestamp = scan['rxAt']\n",
    "            scan_readings: List[Dict] = scan['scandata']\n",
    "            \n",
    "            row = parse_scan_data_woc(scan_readings) \n",
    "\n",
    "            row.update({\n",
    "                'Zone_id': Zone_id,\n",
    "                'Room_name': Room_name,\n",
    "                'parent_zone_id': parent_zone_id,\n",
    "                'tagId': tagId,\n",
    "                'timestamp': timestamp,\n",
    "            })\n",
    "\n",
    "            if row:\n",
    "                X.append(row)\n",
    "\n",
    "    print('Done processing data')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13926dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variable(X_train, y_train_floor, y_train, save_models=False):\n",
    "    \n",
    "    floor_pipeline = Pipeline([\n",
    "        ('rf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    floor_pipeline.fit(X_train, y_train_floor)\n",
    "\n",
    "    clf_floor = floor_pipeline.named_steps['rf']\n",
    "\n",
    "    clf_rooms = {}\n",
    "\n",
    "    selected_features = {}\n",
    "\n",
    "    for floor_num, samples in X_train.groupby(y_train_floor):\n",
    "        \n",
    "        floor_labels = y_train[samples.index]\n",
    "\n",
    "        non_all_neg_120_columns = samples.columns[~np.all(samples == -120, axis=0)]\n",
    "\n",
    "        selected_samples = samples[non_all_neg_120_columns]\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=200, random_state=100)\n",
    "\n",
    "        classifier.fit(selected_samples, floor_labels)\n",
    "\n",
    "        clf_rooms[str(floor_num)] = classifier\n",
    "\n",
    "        selected_features[str(floor_num)] = selected_samples.columns.tolist()\n",
    "\n",
    "    if save_models:\n",
    "        model = {\n",
    "        'selected_features': selected_features,\n",
    "        'clf_rooms': clf_rooms,\n",
    "        'clf_floor': clf_floor\n",
    "        }\n",
    "        joblib.dump(model, 'Hier_Features.joblib')\n",
    "        \n",
    "    return selected_features, clf_rooms, clf_floor\n",
    "\n",
    "def predict_variable(X_test, clf_floor, clf_rooms, selected_features):\n",
    "    \n",
    "    predicted_floors = clf_floor.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "    for floor_num, sample in zip(predicted_floors, X_test.values):\n",
    "        classifier = clf_rooms[str(floor_num)]\n",
    "\n",
    "        selected_names = selected_features[floor_num]\n",
    "\n",
    "        selected_sample = sample[X_test.columns.isin(selected_names)].reshape(1, -1)\n",
    "\n",
    "        predicted_room = classifier.predict(selected_sample)[0]\n",
    "#         predicted_room = predicted_room.astype(str)\n",
    "        predictions.append(predicted_room)\n",
    "\n",
    "    return predictions, predicted_floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(scan_data):\n",
    "\n",
    "    if scan_data is None:\n",
    "        return []\n",
    "    return [\n",
    "        {'macHex': entry['macHex'], 'channel': entry['channel'], 'readings': [entry['rssi'][0]]}\n",
    "        for entry in scan_data if 'macHex' in entry and 'rssi' in entry\n",
    "    ]\n",
    "\n",
    "def parse_scan_data(scan: List[Dict]) -> Dict:\n",
    "\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            channel = beacon_reading['channel']\n",
    "            readings_by_mac_addr_and_channel[f'{mac_addr}'] += readings#-{channel}\n",
    "    return {mac_addr: max(readings) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if len(readings) > 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digital_twin(Anchor_point_location_file, ground_truth_file_location, map_file_location):\n",
    "\n",
    "    anchor_df = pd.read_csv(Anchor_point_location_file)\n",
    "    anchor_df[\"x\"] = anchor_df[\"x\"].astype(int)\n",
    "    anchor_df[\"y\"] = anchor_df[\"y\"].astype(int)\n",
    "    \n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_df['Mac'] = anchor_df['Mac'].astype(str).str.zfill(12)\n",
    "    \n",
    "    macLists = anchor_df['Mac'].to_list()\n",
    "\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file_location)\n",
    "    ground_truth_df[\"Zone_id\"] = ground_truth_df[\"Zone_id\"].astype(str)\n",
    "    \n",
    "    #create a empty map with 0s for future calculation\n",
    "    map_ = np.zeros((65,28))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    image = Image.open(map_file_location)\n",
    "    \n",
    "    plt.scatter(anchor_df.x,anchor_df.y, color='blue', s=50, edgecolors='black', label='Beacons', marker='o', alpha=0.6)\n",
    "\n",
    "#     plt.scatter(ground_truth_df[\"x\"], ground_truth_df[\"y\"], color='red', s=20, label='Ground Truth', marker='^')\n",
    "#     for i, label in enumerate(ground_truth_df['Room_name']):  \n",
    "#         plt.text(ground_truth_df['x'][i], ground_truth_df['y'][i], label, fontsize=9, color='w', ha='right', va='bottom')\n",
    "    plt.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    plt.xlim(0, 65)\n",
    "    plt.ylim(0, 28)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xticks([i for i in range(0, 65, 5)])\n",
    "    plt.yticks([i for i in range(0, 28, 4)])\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.title(\"Beacon distribution in meters\")\n",
    "    plt.legend()\n",
    "    plt.savefig('beacon_map_cognosos.png')\n",
    "\n",
    "    return anchor_df, ground_truth_df, map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823e749-f1be-43ef-8323-29d6aa5ec414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beacon_file = 'ground_truth/Beacon_map_cognosos_flr3.csv'\n",
    "ground_truth_file = \"ground_truth/Ground_truth_Mar25.csv\"\n",
    "map_file = 'ground_truth/Cognosos_view.png'\n",
    "\n",
    "anchor_point_df, ground_truth_df, map_ = create_digital_twin(beacon_file, ground_truth_file, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_features(row, df1):\n",
    "\n",
    "    valid_features = {}\n",
    "    \n",
    "    for mac in df1['Mac']:\n",
    "       \n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] != -100:\n",
    "            valid_features[mac] = row[mac]\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "def convert_coordinates(coord_str):\n",
    "    if isinstance(coord_str, str):\n",
    "       \n",
    "        try:\n",
    "            coord_str = coord_str.strip(\"[]\")\n",
    "            elements = coord_str.split()\n",
    "            return [float(elem) for elem in elements] \n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "        try:\n",
    "            coord_str = coord_str.replace(\" \", \",\")\n",
    "            coord_str = coord_str.replace(\",,\", \",\")\n",
    "            coord_str = coord_str.strip(',')\n",
    "            return ast.literal_eval(coord_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error processing coordinate string: {coord_str}\")\n",
    "            return None\n",
    "    else:\n",
    "        return coord_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_all(result, ground_truth_df, map_file_location, output_file=\"compare_plot_MLE_NLOS.png\"):\n",
    "    results = []\n",
    "    total_points_mle = 0\n",
    "    total_points_Optimisation = 0\n",
    "    total_points_fuse = 0\n",
    "\n",
    "    total_inside_mle = 0\n",
    "    total_inside_Optimisation = 0\n",
    "    total_inside_fuse = 0\n",
    "\n",
    "    merged_df = pd.merge(result, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        has_fused = 'Predicted_NLOS' in room_data.columns\n",
    "\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "\n",
    "        if not polygon.is_valid:\n",
    "            print(f\"Invalid polygon for '{room_name}', attempting to fix with buffer(0).\")\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Parse MLE predictions\n",
    "        x_pred_mle, y_pred_mle = [], []\n",
    "        for coord in room_data[\"Predicted_MLE\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_mle.append(float(coord[0]))\n",
    "                y_pred_mle.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid MLE coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Optimisation predictions\n",
    "        x_pred_Optimisation, y_pred_Optimisation = [], []\n",
    "        for coord in room_data[\"Predicted_Optimisation\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_Optimisation.append(float(coord[0]))\n",
    "                y_pred_Optimisation.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid Optimisation coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Fused predictions only if available\n",
    "        x_pred_fuse, y_pred_fuse = [], []\n",
    "        if has_fused:\n",
    "            for coord in room_data[\"Predicted_NLOS\"]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_pred_fuse.append(float(coord[0]))\n",
    "                    y_pred_fuse.append(float(coord[1]))\n",
    "                except:\n",
    "                    print(f\"Invalid NLOS coord in '{room_name}': {coord}\")\n",
    "\n",
    "        inside_count_mle = sum(1 for x, y in zip(x_pred_mle, y_pred_mle) if Point(x, y).within(polygon))\n",
    "        inside_count_Optimisation = sum(1 for x, y in zip(x_pred_Optimisation, y_pred_Optimisation) if Point(x, y).within(polygon))\n",
    "        inside_count_fuse = sum(1 for x, y in zip(x_pred_fuse, y_pred_fuse) if Point(x, y).within(polygon)) if has_fused else 0\n",
    "\n",
    "        total_points_mle += len(x_pred_mle)\n",
    "        total_inside_mle += inside_count_mle\n",
    "\n",
    "        total_points_Optimisation += len(x_pred_Optimisation)\n",
    "        total_inside_Optimisation += inside_count_Optimisation\n",
    "\n",
    "        if has_fused:\n",
    "            total_points_fuse += len(x_pred_fuse)\n",
    "            total_inside_fuse += inside_count_fuse\n",
    "\n",
    "        percentage_inside_mle = (inside_count_mle / len(x_pred_mle)) * 100 if x_pred_mle else 0\n",
    "        percentage_inside_Optimisation = (inside_count_Optimisation / len(x_pred_Optimisation)) * 100 if x_pred_Optimisation else 0\n",
    "        percentage_inside_fuse = (inside_count_fuse / len(x_pred_fuse)) * 100 if has_fused and x_pred_fuse else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            'Room_name': room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            'MLE_Accuracy': percentage_inside_mle,\n",
    "            'Optimisation_Accuracy': percentage_inside_Optimisation,\n",
    "            'NLOS_Accuracy': percentage_inside_fuse if has_fused else None,\n",
    "            'MLE_Inside_Points': inside_count_mle,\n",
    "            'Optimisation_Inside_Points': inside_count_Optimisation,\n",
    "            'NLOS_Inside_Points': inside_count_fuse if has_fused else None,\n",
    "            'Total_Points': len(x_pred_mle),\n",
    "        })\n",
    "\n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_pred_mle, y_pred_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_pred_Optimisation, y_pred_Optimisation, color='green', s=8, label='Optimisation')\n",
    "        if has_fused:\n",
    "            ax.scatter(x_pred_fuse, y_pred_fuse, color='red', s=8, label='NLOS')\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        title_str = f\"{room_name} - MLE: {percentage_inside_mle:.1f}%, Optimisation: {percentage_inside_Optimisation:.1f}%\"\n",
    "        if has_fused:\n",
    "            title_str += f\", NLOS: {percentage_inside_fuse:.1f}%\"\n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    overall_mle_accuracy = (total_inside_mle / total_points_mle) * 100 if total_points_mle > 0 else 0\n",
    "    overall_Optimisation_accuracy = (total_inside_Optimisation / total_points_Optimisation) * 100 if total_points_Optimisation > 0 else 0\n",
    "    overall_fuse_accuracy = (total_inside_fuse / total_points_fuse) * 100 if total_points_fuse > 0 else 0\n",
    "\n",
    "    print(f\"\\nOverall MLE Accuracy: {overall_mle_accuracy:.2f}%\")\n",
    "    print(f\"Overall Optimisation Accuracy: {overall_Optimisation_accuracy:.2f}%\")\n",
    "    if total_points_fuse > 0:\n",
    "        print(f\"Overall NLOS Accuracy: {overall_fuse_accuracy:.2f}%\")\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89394c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MLE_data_survey_portal(df, ground_truth_df, anchor_point_df, export_unheard=False, export_path=\"unheard_anchor_points.csv\"):\n",
    "\n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_point_df['Mac'] = anchor_point_df['Mac'].astype(str).str.zfill(12)\n",
    "\n",
    "    data_set_df = pd.DataFrame()\n",
    "    merged_df = pd.merge(df, ground_truth_df, on=[\"Zone_id\", 'Room_name'], how='inner').drop(['parent_zone_id'], axis=1)\n",
    "    zones = df['Zone_id']\n",
    "    heard_anchor_points = []\n",
    "\n",
    "    for mac_addr in anchor_point_df['Mac']:\n",
    "        if mac_addr in merged_df.columns:\n",
    "            data_set_df[mac_addr] = merged_df[mac_addr]\n",
    "            heard_anchor_points.append(mac_addr)\n",
    "\n",
    "    heard_anchor_point_df = anchor_point_df[anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "    unheard_anchor_point_df = anchor_point_df[~anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "\n",
    "    heard_anchor_points_coord = heard_anchor_point_df[['x', 'y']].values\n",
    "\n",
    "    data_set_df[\"Zone_id\"] = merged_df[\"Zone_id\"]\n",
    "    data_set_df[\"Room_name\"] = merged_df[\"Room_name\"]\n",
    "    data_set_df[\"tagId\"] = merged_df[\"tagId\"]\n",
    "    data_set_df[\"timestamp\"] = merged_df[\"timestamp\"]\n",
    "    if \"channel\" in merged_df.columns:\n",
    "        data_set_df[\"channel\"] = merged_df[\"channel\"]\n",
    "    \n",
    "    data_set_df[\"x\"] = merged_df[\"x\"]\n",
    "    data_set_df[\"y\"] = merged_df[\"y\"]\n",
    "\n",
    "    return data_set_df, heard_anchor_points_coord, unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea913fad",
   "metadata": {},
   "source": [
    "### Excluding zone outside the ofiice as I remove all beacon there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones= ground_truth_df.Zone_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/Asset/Original data/1_data_Walk_Around_Mar25.json\"\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "X1 = process_training(filepath)\n",
    "df1 = pd.DataFrame(X1)\n",
    "df1 = df1.fillna(MISSING_VALUE)  \n",
    "\n",
    "float_cols = df1.select_dtypes(include=['float']).columns\n",
    "df1[float_cols] = df1[float_cols].astype(np.int8)\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], utc=True, errors='coerce')\n",
    "df1 = df1.sort_values(by='timestamp')\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# Fix specific zone_id\n",
    "df1.loc[df1['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# # Conditional exclusion based on filename\n",
    "# if filename == \"data_duress_access_Jun23.json\":\n",
    "#     rooms_to_exclude = [\"Tech Office 2\"]  # this room has issue with data as very low beacon signal, beacon count\n",
    "#     df1 = df1[~df1['Room_name'].isin(rooms_to_exclude)]\n",
    "\n",
    "# Beacon processing\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "df1= df1[df1.Zone_id.isin(zones)]\n",
    "print(df1.shape)\n",
    "\n",
    "df1 = df1[df1['beacon_count'] >= 5]\n",
    "print(df1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6588f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(df1.columns.tolist()) & set(anchor_point_df.Mac)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beca616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['beacon_count'].max(), df1['beacon_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df1, df2], axis=0, ignore_index=True, sort=False)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a49089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= df1[df1.Room_name != 'Tech Office 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv(\"data/Asset/2_data_asset_walk_around_combine_1_2.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(\"data/Asset/2_data_asset_walk_around_combine_1_2.csv\")\n",
    "df1.tagId= df1.tagId.astype(str)\n",
    "df1.Zone_id= df1.Zone_id.astype(str)\n",
    "df1= df1[df1.Zone_id.isin(zones)]\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc9d5",
   "metadata": {},
   "source": [
    "## REMOVE ALL ROWS < -90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bc260",
   "metadata": {},
   "source": [
    "### Read the data for walk around with a plastic box of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= pd.read_csv(\"data/Asset/2_data_asset_walk_around_combine_1_2.csv\")\n",
    "# df1.Zone_id= df1.Zone_id.astype(str)\n",
    "# df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9747cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "rows_all_below_90 = df1[beacon_cols].lt(-99).all(axis=1)\n",
    "df1 = df1[~rows_all_below_90]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423898f9",
   "metadata": {},
   "source": [
    "### Check if dataset have enoguh beacon heard >=-90, SELECT ONLY the number of strong features >=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1.drop(columns=[\"parent_zone_id\", \"beacon_count\"]), \\\n",
    "                                ground_truth_df[['Zone_id','x', 'y']], on=[\"Zone_id\"], how='left')\n",
    "rssi_cols = [col for col in df2.columns if col.startswith('0')]\n",
    "\n",
    "# Create a new column counting RSSIs >= -90\n",
    "df2['num_strong_features'] = (df2[rssi_cols] >= -95).sum(axis=1)\n",
    "\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df = df2[df2['num_strong_features'] >= 5].copy()\n",
    "\n",
    "data_set_df=data_set_df.drop(columns='num_strong_features').reset_index(drop=True)\n",
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b10787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.iloc[[2885]].describe().T.sort_values(by=\"max\", ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa23c3b",
   "metadata": {},
   "source": [
    "# Take 10% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_df = data_set_df_all.groupby('Zone_id', group_keys=False).\\\n",
    "#         apply(lambda x: x.sample(frac=1, random_state=42))\n",
    "# data_set_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8071023",
   "metadata": {},
   "source": [
    "# *** MAKE SURE THAT ALL BEACONS ARE BEING HEARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if any ionstalled beacon not heard by the tags\n",
    "# unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set_df.columns.intersection(anchor_point_df.Mac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779ada9",
   "metadata": {},
   "source": [
    "# Location AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"data/Asset/Original data/2_data_asset_acess_Jun23.json\"\n",
    "\n",
    "# X1 = process_training(filepath)\n",
    "# df_asset = pd.DataFrame(X1)\n",
    "# df_asset = df_asset.fillna(MISSING_VALUE)  \n",
    "\n",
    "# float_cols = df_asset.select_dtypes(include=['float']).columns\n",
    "# df_asset[float_cols] = df_asset[float_cols].astype(np.int8)\n",
    "# df_asset['timestamp'] = pd.to_datetime(df_asset['timestamp'], utc=True, errors='coerce')\n",
    "# df_asset = df_asset.sort_values(by='timestamp')\n",
    "\n",
    "# ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "# columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "# new_column_order = columns + ordered_columns\n",
    "# df_asset = df_asset.reindex(columns=new_column_order)\n",
    "\n",
    "# df_asset = df_asset.reset_index(drop=True)\n",
    "# df_asset['Room_name'] = df_asset['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# # Fix specific zone_id\n",
    "# df_asset.loc[df_asset['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# # # Conditional exclusion based on filename\n",
    "# # if filename == \"data_duress_access_Jun23.json\":\n",
    "# #     rooms_to_exclude = [\"Tech Office 2\"]  # this room has issue with data as very low beacon signal, beacon count\n",
    "# #     df_asset = df_asset[~df_asset['Room_name'].isin(rooms_to_exclude)]\n",
    "\n",
    "# # Beacon processing\n",
    "# beacon_cols = [col for col in df_asset.columns if str(col).startswith('0')]\n",
    "# df_asset = df_asset.fillna(MISSING_VALUE)\n",
    "# df_asset['beacon_count'] = (df_asset[beacon_cols] != -100).sum(axis=1)\n",
    "# df_asset= df_asset[df_asset.Zone_id.isin(zones)]\n",
    "# print(df_asset.shape)\n",
    "\n",
    "# df_asset = df_asset[df_asset['beacon_count'] >= 5]\n",
    "# print(df_asset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa76568",
   "metadata": {},
   "source": [
    "#### as the CEO data is not available for the trainign data, so I just take randomly from another data and append it into the trainign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_keep = df_asset.columns  \n",
    "\n",
    "# filtered_df = df1[df1.Zone_id == \"30519\"]\n",
    "# sampled_df = filtered_df.sample(n=150, random_state=42)\n",
    "\n",
    "# # Keep only train columns\n",
    "# sampled_df = sampled_df[columns_to_keep]\n",
    "\n",
    "# # Append to training data\n",
    "# df2 = pd.concat([df_asset, sampled_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e660a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv(\"data/Asset/Original data/2_data_asset_acess_Jun23_used_for_locAI_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d409d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c2759",
   "metadata": {},
   "source": [
    "## Took this data as for traning the LocAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15883adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop= anchor_point_df[anchor_point_df.Remove==\"remove\"].Mac.tolist()\n",
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bf65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2= pd.read_csv(\"data/Asset/Original data/2_data_asset_acess_Jun23_used_for_locAI_training.csv\")\n",
    "# df2['Zone_id']=df2['Zone_id'].astype(str)\n",
    "# df2= df2[df1.columns]\n",
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4adc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df1.drop(columns= columns_to_drop)\n",
    "df2[\"parent_zone_id\"]=df2[\"parent_zone_id\"].astype(str)\n",
    "\n",
    "train_data, test_data = train_test_split(df2, test_size=0.2, random_state=42, \\\n",
    "                                         stratify=df2[\"Zone_id\"])\n",
    "\n",
    "X_train = train_data[[col for col in train_data.columns if col.startswith(\"0\")]]\n",
    "y_train_floor = train_data['parent_zone_id'] \n",
    "y_train = train_data['Zone_id']\n",
    "\n",
    "X_test = test_data[[col for col in train_data.columns if col.startswith(\"0\")]] \n",
    "y_test_floor = test_data['parent_zone_id'] \n",
    "y_test = test_data['Zone_id'] \n",
    "df2.shape, df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, clf_rooms, clf_floor = train_variable(X_train, y_train_floor, y_train, save_models = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3baaa45",
   "metadata": {},
   "source": [
    "## Test against other dtaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72260d71-17f0-4d87-97ea-a33a06a3e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= pd.read_csv(\"data/Asset/1_data_asset_tag_stay_still_combine_all.csv\")\n",
    "# df1.tagId= df1.tagId.astype(str)\n",
    "# df1.Zone_id= df1.Zone_id.astype(str)\n",
    "# df1= df1[df1.Zone_id.isin(zones)]\n",
    "\n",
    "# ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "# columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "# new_column_order = columns + ordered_columns\n",
    "# df1 = df1.reindex(columns=new_column_order)\n",
    "# df1 = df1.reset_index(drop=True)\n",
    "# df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "# beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "# df1 = df1.fillna(MISSING_VALUE)\n",
    "# df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "# df1.shape\n",
    "\n",
    "# df_test= df1[train_data.columns].copy()\n",
    "# X_test = df_test[[col for col in df_test.columns if col.startswith(\"0\")]] \n",
    "# y_test_floor = df_test['parent_zone_id'] \n",
    "# y_test = df_test['Zone_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b24e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rooms, predicted_floors = predict_variable(X_test, clf_floor, clf_rooms, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d829001",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_test, predicted_rooms)\n",
    "print('Room Accuracy: {:.2f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ade634",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d = test_data[[\"Room_name\", 'tagId', 'Zone_id']]\\\n",
    "    .merge(ground_truth_df[[\"Zone_id\", \"Room_Type\"]], on = \"Zone_id\", how=\"left\")\n",
    "result_d[\"Prediction\"] = predicted_rooms\n",
    "result_d[\"Accuracy\"] = np.where(result_d.Zone_id == result_d.Prediction, 100, 0)\n",
    "result_d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf = pd.DataFrame(result_d.groupby(['Zone_id', \"Room_name\"]).Accuracy.mean()).reset_index()\n",
    "result_d.groupby(\"Room_Type\").Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ccd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "res= pd.DataFrame(result_d.groupby(['Zone_id', \"Room_name\", \"Room_Type\", \"tagId\"]).Accuracy.mean()).reset_index()\n",
    "# res.sort_values(by=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eaf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\"Room\": \"skyblue\", \"Open\": \"orange\"}\n",
    "\n",
    "# Ensure consistent types\n",
    "res[\"Room_Type\"] = res[\"Room_Type\"].astype(str)\n",
    "\n",
    "# 1. Sort by Zone_id FIRST\n",
    "res_sorted = res.sort_values(by=\"Zone_id\")\n",
    "\n",
    "# 2. Create a sort key for Room_Type so \"Room\" comes before \"Open\"\n",
    "type_priority = {\"Room\": 0, \"Open\": 1}\n",
    "\n",
    "# 3. Build final room order sorted by:\n",
    "#    (Zone_id, type_priority, Room_name)\n",
    "res_sorted = res_sorted.sort_values(\n",
    "    by=[\"Zone_id\", \"Room_Type\", \"Room_name\"],\n",
    "    key=lambda col: col.map(type_priority) if col.name == \"Room_Type\" else col\n",
    ")\n",
    "\n",
    "# 4. Create the ordered x-axis list\n",
    "room_order = list(res_sorted[\"Room_name\"].unique())\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=res_sorted,\n",
    "    x=\"Room_name\",\n",
    "    y=\"Accuracy\",\n",
    "    order=room_order,\n",
    "    hue=\"Room_Type\",\n",
    "    dodge=False,\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"LocationAI_Tag Accuracy Distribution per Room\")\n",
    "plt.legend(title=\"\")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_accuracy(df):\n",
    "    \"\"\"\n",
    "    df: DataFrame with columns\n",
    "        Room_name, tagId, Zone_id, Room_Type, Prediction, Accuracy\n",
    "    Returns a summary dataframe with:\n",
    "        - Overall Accuracy\n",
    "        - Accuracy per Room_Type\n",
    "    \"\"\"\n",
    "\n",
    "    # Overall accuracy (mean of Accuracy column)\n",
    "    overall_acc = df[\"Accuracy\"].mean()\n",
    "\n",
    "    # Accuracy per Room_Type\n",
    "    room_type_acc = df.groupby(\"Room_Type\")[\"Accuracy\"].mean().reset_index()\n",
    "    room_type_acc.rename(columns={\"Accuracy\": \"Accuracy_by_Room_Type\"}, inplace=True)\n",
    "\n",
    "    # Combine into a single dataframe\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Overall_Accuracy\": [overall_acc]\n",
    "    })\n",
    "\n",
    "    # Merge room_type accuracy as separate columns\n",
    "    for _, row in room_type_acc.iterrows():\n",
    "        summary_df[row[\"Room_Type\"] + \"_Accuracy\"] = row[\"Accuracy_by_Room_Type\"]\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_room= compute_summary_accuracy(result_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49049e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34f22d35",
   "metadata": {},
   "source": [
    "# NLOS: Fused of Opt and MLE: using MLE as initial for Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fe6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(point, points, height_diff=1.5):\n",
    "    return np.sqrt(np.sum((points - point) ** 2, axis=1) + height_diff ** 2)\n",
    "\n",
    "def generate_grid(center, resolution=5, radius=4):\n",
    "    step = 1 / resolution\n",
    "    x_vals = np.arange(center[0] - radius, center[0] + radius + step, step)\n",
    "    y_vals = np.arange(center[1] - radius, center[1] + radius + step, step)\n",
    "    xv, yv = np.meshgrid(x_vals, y_vals)\n",
    "    return np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "\n",
    "def rssi_to_distance(rssi, A=-40, n=3.5, scale=0.8):\n",
    "    return scale * np.exp((A - rssi) / (10 * n))\n",
    "\n",
    "def filter_rssi(row, beacon_positions, rssi_threshold=-90):\n",
    "    return {\n",
    "        mac: row[mac]\n",
    "        for mac in beacon_positions.keys()\n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] > rssi_threshold\n",
    "    }\n",
    "\n",
    "def localization_error(tag_position, beacons, distances):\n",
    "    estimated_distances = np.linalg.norm(beacons - tag_position, axis=1)\n",
    "    sigma = np.std(distances) + 1e-3\n",
    "    weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "    return np.sum(weights * (estimated_distances - distances) ** 2)\n",
    "\n",
    "def generate_expansion_area(initial_guess, std_dev=0.3, radius=0.5, num_points=50):\n",
    "    \"\"\"Compact expansion area around MLE\"\"\"\n",
    "    num_gauss = int(num_points * 0.6)\n",
    "    num_circle = num_points - num_gauss\n",
    "    gauss_points = np.random.normal(0, std_dev, size=(num_gauss, 2)) + initial_guess\n",
    "    r = radius * np.sqrt(np.random.uniform(0, 1, num_circle))\n",
    "    theta = np.random.uniform(0, 2 * np.pi, num_circle)\n",
    "    circ_points = np.column_stack((initial_guess[0] + r * np.cos(theta), initial_guess[1] + r * np.sin(theta)))\n",
    "    return np.vstack((gauss_points, circ_points))\n",
    "\n",
    "def compute_likelihood_weighted(grid_coords, anchor_coords, rssi_values, T, n, sigma_noise=4, anchor_weights=None):\n",
    "    if anchor_weights is None:\n",
    "        anchor_weights = np.ones_like(rssi_values)\n",
    "    diff = grid_coords[:, None, :] - anchor_coords[None, :, :]\n",
    "    dists = np.sqrt(np.sum(diff ** 2, axis=2) + 1.5**2)\n",
    "    pred_rssi = T - 10 * n * np.log10(dists + 1e-5)\n",
    "    residuals = pred_rssi - rssi_values\n",
    "    weighted_residuals = (residuals / sigma_noise)**2 * anchor_weights\n",
    "    likelihood = np.exp(-0.5 * weighted_residuals)\n",
    "    return np.prod(likelihood, axis=1)\n",
    "\n",
    "def find_mle_params(P_j, d_ij, init_guess=[-40, 3]):\n",
    "    def squared_error(params, dists, rssi):\n",
    "        T_i, n_p = params\n",
    "        valid_mask = rssi != -100\n",
    "        pred_rssi = T_i - 10 * n_p * np.log10(dists + 1e-5)\n",
    "        return np.sum((pred_rssi[valid_mask] - rssi[valid_mask]) ** 2)\n",
    "    bounds = [(-100, -30), (2, 6)]\n",
    "    result = minimize(squared_error, init_guess, args=(d_ij, P_j),\n",
    "                      method='L-BFGS-B', bounds=bounds)\n",
    "    return result.x if result.success else init_guess\n",
    "\n",
    "# -------------------------------\n",
    "# Fused Localization (MLE → Opt → Refine)\n",
    "# -------------------------------\n",
    "def fused_localization_mle_opt(data_df, anchor_point_df,\n",
    "                               sigma_noise=4, coarse_res=2, fine_res=5, fine_radius=3,\n",
    "                               rssi_threshold=-95, strong_rssi_threshold=-75,\n",
    "                               top_k_anchors=5, roi_margin=8, top_coarse_points=200, topN_ratio=0.05,\n",
    "                               map_x_bounds=(0, 65), map_y_bounds=(0, 28),\n",
    "                               epsilon=1e-12,\n",
    "                               expansion_radius=0.8, expansion_points=100,\n",
    "                               enable_refinement=True):\n",
    "    \n",
    "    results = []\n",
    "    beacon_positions = anchor_point_df[[\"x\",\"y\",\"Mac\"]].set_index(\"Mac\")[[\"x\",\"y\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    for idx, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "        # ---------------------------\n",
    "        # Extract RSSI\n",
    "        # ---------------------------\n",
    "        rssis = row.drop(['Zone_id','Room_name','x','y','tagId','timestamp'], errors='ignore').values.astype(float)\n",
    "        anchor_coords = anchor_point_df[['x','y']].values\n",
    "\n",
    "        # ---------------------------\n",
    "        # MLE Estimation\n",
    "        # ---------------------------\n",
    "        mask_mle = rssis > rssi_threshold\n",
    "        signal_strengths = rssis[mask_mle]\n",
    "        dp_coords = anchor_coords[mask_mle]\n",
    "\n",
    "        if len(signal_strengths) < 1:\n",
    "            signal_strengths = rssis\n",
    "            dp_coords = anchor_coords\n",
    "\n",
    "        strong_mask = signal_strengths > strong_rssi_threshold\n",
    "        if np.sum(strong_mask) < 2:\n",
    "            dp_coords_selected = dp_coords\n",
    "            signal_strengths_selected = signal_strengths\n",
    "        else:\n",
    "            dp_coords_selected = dp_coords[strong_mask]\n",
    "            signal_strengths_selected = signal_strengths[strong_mask]\n",
    "\n",
    "        min_rssi, max_rssi = np.min(signal_strengths_selected), np.max(signal_strengths_selected)\n",
    "        anchor_weights = (signal_strengths_selected - min_rssi + 1) / (max_rssi - min_rssi + 1e-5)\n",
    "        sorted_idx = np.argsort(-signal_strengths_selected)\n",
    "        top_k = min(top_k_anchors, len(sorted_idx))\n",
    "        top_coords = dp_coords_selected[sorted_idx[:top_k]]\n",
    "\n",
    "        x_min, y_min = np.min(top_coords, axis=0)\n",
    "        x_max, y_max = np.max(top_coords, axis=0)\n",
    "        x_min = max(x_min - roi_margin, map_x_bounds[0])\n",
    "        x_max = min(x_max + roi_margin, map_x_bounds[1])\n",
    "        y_min = max(y_min - roi_margin, map_y_bounds[0])\n",
    "        y_max = min(y_max + roi_margin, map_y_bounds[1])\n",
    "\n",
    "        coarse_grid = np.stack(np.meshgrid(np.arange(x_min, x_max, 1/coarse_res),\n",
    "                                           np.arange(y_min, y_max, 1/coarse_res)), axis=-1).reshape(-1,2)\n",
    "\n",
    "        strongest_coord = top_coords[0]\n",
    "        dists_for_fit = euclidean_dist(strongest_coord, dp_coords_selected)\n",
    "        T_global, n_global = find_mle_params(signal_strengths_selected, dists_for_fit)\n",
    "\n",
    "        coarse_likelihoods = compute_likelihood_weighted(\n",
    "            coarse_grid, dp_coords_selected, signal_strengths_selected,\n",
    "            T_global, n_global, sigma_noise, anchor_weights\n",
    "        )\n",
    "\n",
    "        top_indices = np.argpartition(coarse_likelihoods, -top_coarse_points)[-top_coarse_points:]\n",
    "        top_candidates = coarse_grid[top_indices]\n",
    "\n",
    "        fine_candidates, fine_likelihoods = [], []\n",
    "        for center in top_candidates:\n",
    "            fine_grid = generate_grid(center, resolution=fine_res, radius=fine_radius)\n",
    "            likelihoods_fine = compute_likelihood_weighted(\n",
    "                fine_grid, dp_coords_selected, signal_strengths_selected,\n",
    "                T_global, n_global, sigma_noise, anchor_weights\n",
    "            )\n",
    "            fine_candidates.append(fine_grid)\n",
    "            fine_likelihoods.append(likelihoods_fine)\n",
    "\n",
    "        fine_candidates = np.vstack(fine_candidates)\n",
    "        fine_likelihoods = np.hstack(fine_likelihoods)\n",
    "        fine_likelihoods += epsilon\n",
    "        fine_likelihoods /= np.sum(fine_likelihoods)\n",
    "\n",
    "        N = max(1, min(100, int(topN_ratio * len(fine_candidates))))\n",
    "        top_idx = np.argpartition(fine_likelihoods, -N)[-N:]\n",
    "        top_points = fine_candidates[top_idx]\n",
    "        top_weights = fine_likelihoods[top_idx]\n",
    "        top_weights /= np.sum(top_weights)\n",
    "        pred_mle = np.average(top_points, axis=0, weights=top_weights)\n",
    "        conf_mle = np.max(top_weights)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optimization around MLE\n",
    "        # ---------------------------\n",
    "        rssi_values_opt = dict(sorted(filter_rssi(row, beacon_positions, rssi_threshold).items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(rssi_values_opt) < 3:\n",
    "            beacon_coords_opt = anchor_coords\n",
    "            distances_opt = np.ones(anchor_coords.shape[0])\n",
    "        else:\n",
    "            beacon_coords_opt = np.array([list(beacon_positions[b].values()) for b in rssi_values_opt.keys()])\n",
    "            distances_opt = np.array([rssi_to_distance(rssi) for rssi in rssi_values_opt.values()])\n",
    "\n",
    "        expansion_area = generate_expansion_area(pred_mle, radius=expansion_radius, num_points=expansion_points)\n",
    "        quick_errors = np.array([localization_error(p, beacon_coords_opt, distances_opt) for p in expansion_area])\n",
    "        filtered_expansion_area = expansion_area[np.argsort(quick_errors)[:10]]  # top few\n",
    "\n",
    "        best_err, best_pos = float(\"inf\"), None\n",
    "        for point in filtered_expansion_area:\n",
    "            res = minimize(localization_error, point, args=(beacon_coords_opt, distances_opt),\n",
    "                           method='L-BFGS-B', options={'maxiter':100})\n",
    "            if res.success:\n",
    "                est_pos = res.x\n",
    "                total_err = np.sum(np.linalg.norm(beacon_coords_opt - est_pos, axis=1))\n",
    "                if total_err < best_err:\n",
    "                    best_err = total_err\n",
    "                    best_pos = est_pos\n",
    "        pred_opt = best_pos\n",
    "        conf_opt = 1 / (1 + best_err)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optional refinement\n",
    "        # ---------------------------\n",
    "        pre_refined_pos = pred_opt.copy()\n",
    "        if enable_refinement:\n",
    "            strong_rssi_indices = [i for i, rssi in enumerate(rssi_values_opt.values()) if rssi > -75]\n",
    "            if len(strong_rssi_indices) >= 3:\n",
    "                filtered_coords = beacon_coords_opt[strong_rssi_indices]\n",
    "                filtered_distances = distances_opt[strong_rssi_indices]\n",
    "                result = minimize(localization_error, pred_opt,\n",
    "                                  args=(filtered_coords, filtered_distances),\n",
    "                                  method='L-BFGS-B',\n",
    "                                  options={'maxiter':100, 'gtol':1e-8, 'disp':False})\n",
    "                if result.success:\n",
    "                    pred_opt = result.x\n",
    "\n",
    "        refinement_shift = np.linalg.norm(pred_opt - pre_refined_pos)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Fused Results\n",
    "        # ---------------------------\n",
    "        alpha_dynamic = conf_mle / (conf_mle + conf_opt)\n",
    "        pred_fused_fixed = 0.5 * pred_mle + 0.5 * pred_opt\n",
    "        pred_fused_dynamic = alpha_dynamic * pred_mle + (1-alpha_dynamic) * pred_opt\n",
    "\n",
    "        results.append({\n",
    "            'original_index': idx,\n",
    "            'Zone_id': row.get('Zone_id', np.nan),\n",
    "            'Room_name': row.get('Room_name', np.nan),\n",
    "            'Tag_id': row.get('tagId', np.nan),\n",
    "            'timestamp': row.get('timestamp', np.nan),\n",
    "            'Predicted_MLE': pred_mle,\n",
    "            'Predicted_Optimisation': pred_opt,\n",
    "            'Predicted_NLOS': pred_fused_fixed,\n",
    "            'Predicted_NLOS_Dynamic': pred_fused_dynamic,\n",
    "            'MLE_Confidence': conf_mle,\n",
    "            'Opt_Confidence': conf_opt,\n",
    "            'Ground_Truth': np.array([row['x'], row['y']])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca56e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename= \"1_data_asset_tag_stay_still_combine_3_4.json\"\n",
    "filename = \"2_data_asset_walk_around_combine_1_2.json\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter() \n",
    "\n",
    "result= fused_localization_mle_opt(data_set_df, anchor_point_df)\n",
    "\n",
    "save_folder = \"Result_Asset\"\n",
    "save_name = f\"{filename.replace('.json', '_NLOS.csv')}\" \n",
    "save_path = os.path.join(save_folder, save_name)\n",
    "\n",
    "result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter() \n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_row = total_time / len(data_set_df)\n",
    "print(avg_time_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9609f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result= pd.read_csv(\"Result_Asset/2_data_asset_walk_around_combine_1_2_Hybrid.csv\")\n",
    "result['Predicted_MLE'] = result['Predicted_MLE'].apply(convert_coordinates)\n",
    "result['Ground_Truth'] = result['Ground_Truth'].apply(convert_coordinates)\n",
    "\n",
    "\n",
    "result[\"Predicted_Optimisation\"]= result['Predicted_Optimisation'].apply(convert_coordinates)\n",
    "result[\"Predicted_NLOS\"]= result['Predicted_NLOS'].apply(convert_coordinates)\n",
    "result[\"Predicted_RF_Hybrid\"]= result['Predicted_RF_Hybrid'].apply(convert_coordinates)\n",
    "\n",
    "result.Zone_id= result.Zone_id.astype(str)\n",
    "result.Tag_id= result.Tag_id.astype(str)\n",
    "result= result[result.Room_name !='Womens Restroom']\n",
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502f7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3333b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_fused_dynamic(result_df, ground_truth_df, map_file_location,\n",
    "                                 fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "                                 output_file=\"compare_plot_MLE_Optim_Fused.png\"):\n",
    "    \"\"\"\n",
    "    Plot predicted locations from MLE, Optimisation, and fused methods, showing inside-room accuracy.\n",
    "    Computes overall accuracy and returns per-room statistics including inside-point counts and total points.\n",
    "    Also provides room-type aggregated accuracy.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    total_inside = {\"MLE\": 0, \"Optimisation\": 0}\n",
    "    total_inside.update({col: 0 for col in fused_cols})\n",
    "    total_points = 0  # only one total points count\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        # Room polygon\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Helper function to parse coordinates\n",
    "        def parse_coords(col_name):\n",
    "            x_list, y_list = [], []\n",
    "            for coord in room_data[col_name]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_list.append(float(coord[0]))\n",
    "                    y_list.append(float(coord[1]))\n",
    "                except:\n",
    "                    pass\n",
    "            return x_list, y_list\n",
    "\n",
    "        # Predictions\n",
    "        x_mle, y_mle = parse_coords(\"Predicted_MLE\")\n",
    "        x_opt, y_opt = parse_coords(\"Predicted_Optimisation\")\n",
    "        fused_data = {col: parse_coords(col) for col in fused_cols if col in room_data.columns}\n",
    "\n",
    "        # Count points inside polygon\n",
    "        def count_inside(x_list, y_list):\n",
    "            return sum(1 for x, y in zip(x_list, y_list) if Point(x, y).within(polygon))\n",
    "\n",
    "        inside_mle = count_inside(x_mle, y_mle)\n",
    "        inside_opt = count_inside(x_opt, y_opt)\n",
    "        inside_fused = {k: count_inside(*v) for k, v in fused_data.items()}\n",
    "\n",
    "        # Update totals\n",
    "        total_inside[\"MLE\"] += inside_mle\n",
    "        total_inside[\"Optimisation\"] += inside_opt\n",
    "        for k, v in fused_data.items():\n",
    "            total_inside[k] += inside_fused[k]\n",
    "        total_points += len(x_mle)  # same for all methods\n",
    "\n",
    "        # Save per-room results\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"MLE_Accuracy\": inside_mle / max(len(x_mle), 1) * 100,\n",
    "            \"Optimisation_Accuracy\": inside_opt / max(len(x_opt), 1) * 100,\n",
    "            **{f\"{k}_Accuracy\": inside_fused[k] / max(len(fused_data[k][0]), 1) * 100 for k in fused_data},\n",
    "            \"MLE_Inside_Points\": inside_mle,\n",
    "            \"Optimisation_Inside_Points\": inside_opt,\n",
    "            **{f\"{k}_Inside_Points\": inside_fused[k] for k in fused_data},\n",
    "            \"Total_Points\": len(x_mle)\n",
    "        })\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_mle, y_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_opt, y_opt, color='green', s=8, label='Optimisation')\n",
    "        colors = ['orange', 'purple', 'red', 'cyan']\n",
    "        for j, (fcol, (x_f, y_f)) in enumerate(fused_data.items()):\n",
    "            ax.scatter(x_f, y_f, color=colors[j % len(colors)], s=8, label=f\"{fcol}\")\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        \n",
    "        # Build title using percentage accuracy instead of counts\n",
    "        title_str = (\n",
    "            f\"{room_name} - \"\n",
    "            f\"MLE: {inside_mle / max(len(x_mle), 1) * 100:.1f}%, \"\n",
    "            f\"Opt: {inside_opt / max(len(x_opt), 1) * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        for fcol, (x_f, y_f) in fused_data.items():\n",
    "            acc = inside_fused[fcol] / max(len(x_f), 1) * 100\n",
    "            clean_name = fcol.replace(\"Predicted_\", \"\")  # <--- removes the prefix\n",
    "            title_str += f\", {clean_name}: {acc:.1f}%\"\n",
    "\n",
    "\n",
    "            \n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "\n",
    "    # --- Overall accuracy ---\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    for method in total_inside.keys():\n",
    "        overall = total_inside[method] / max(total_points, 1) * 100\n",
    "        print(f\"{method}: {overall:.2f}%\")\n",
    "\n",
    "    # --- Room-type aggregated accuracy ---\n",
    "    room_type_stats = accuracy_df.groupby('Room_Type').agg({\n",
    "        'MLE_Inside_Points': 'sum',\n",
    "        'Optimisation_Inside_Points': 'sum',\n",
    "        **{f\"{col}_Inside_Points\": 'sum' for col in fused_cols},\n",
    "        'Total_Points': 'sum'\n",
    "    })\n",
    "\n",
    "    for method in ['MLE', 'Optimisation'] + fused_cols:\n",
    "        room_type_stats[f\"{method}_Accuracy\"] = room_type_stats[f\"{method}_Inside_Points\"] / room_type_stats['Total_Points'] * 100\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "accuracy_df = plot_predicted_fused_dynamic(\n",
    "    result_df=result,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location= map_file,\n",
    "    fused_cols=['Predicted_RF_Hybrid', 'Predicted_NLOS'],\n",
    "#     output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"Predicted_NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df[\"MLE_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df[\"Optimisation_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df[\"Predicted_NLOS_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Asset/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.groupby(\"Room_Type\")[\"Room_Type\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55090a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_accuracy_per_room(\n",
    "    accuracy_df,\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Fused\"),\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise numeric accuracies for three models in different colors.\n",
    "    First line: MLE/Optimisation (no spaces)\n",
    "    Second line: Fused\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge with ground truth polygons\n",
    "    merged_df = pd.merge(accuracy_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Polygon coordinates\n",
    "        x_coords = [row.get(f\"x{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"x{i+1}\", None))]\n",
    "        y_coords = [row.get(f\"y{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"y{i+1}\", None))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Draw polygon outline\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "\n",
    "        # Accuracy values\n",
    "        acc1 = int(row.get(f\"{labels[0]}_Accuracy\", 0))\n",
    "        acc2 = int(row.get(f\"{labels[1]}_Accuracy\", 0))\n",
    "        acc3 = int(row.get(f\"{labels[2]}_Accuracy\", 0))\n",
    "\n",
    "        centroid = polygon.centroid\n",
    "\n",
    "        # Line 1: MLE / Optimisation\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y + 0.15,\n",
    "            f\"{acc1}/{acc2}\",\n",
    "            color=\"black\", fontsize=10, ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "        # Individual colors\n",
    "        ax.text(centroid.x - 0.55, centroid.y + 0.15, f\"{acc1}\", color=colors[0], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "        ax.text(centroid.x, centroid.y + 0.15, \"/\", color=\"black\", fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4)\n",
    "        ax.text(centroid.x + 0.55, centroid.y + 0.15, f\"{acc2}\", color=colors[1], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "\n",
    "        # Line 2: Fused\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y - 0.45,\n",
    "            f\"{acc3}\",\n",
    "            color=colors[2], fontsize=10, ha=\"center\", va=\"center\", zorder=3, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    # Automatically scale axes\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ✅ Weighted overall accuracies\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "    overall_acc = []\n",
    "    for label in labels:\n",
    "        inside_col = f\"{label}_Inside_Points\"\n",
    "        if inside_col in accuracy_df:\n",
    "            overall = (accuracy_df[inside_col].sum() / total_points) * 100\n",
    "            overall_acc.append(overall)\n",
    "        else:\n",
    "            overall_acc.append(0)\n",
    "\n",
    "    overall_text = \" | \".join([f\"{label}: {val:.1f}%\" for label, val in zip(labels, overall_acc)])\n",
    "    ax.set_title(f\"{title_text}\\nOverall Accuracy: {overall_text}\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [Patch(color=color, label=label) for color, label in zip(colors, labels)]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room(\n",
    "    accuracy_df=accuracy_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location=map_file,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Predicted_NLOS\"),\n",
    "    title_text=\"Asset_Stationary Tag_Nov 19_Single Data Packet\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69498ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def plot_accuracy_per_room_combined(\n",
    "    accuracy_df,\n",
    "    ground_truth_df,\n",
    "    result_d,\n",
    "    map_file_location,\n",
    "    fused_color=\"purple\",\n",
    "    result_d_color=\"blue\",\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise accuracy:\n",
    "      - Single line per room: LocationAI / NLOS (colored)\n",
    "      - Title shows overall weighted accuracy for both\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge accuracy with polygons\n",
    "    merged_df = pd.merge(\n",
    "        accuracy_df,\n",
    "        ground_truth_df,\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge result_d (LocationAI)\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_resultD\")\n",
    "    )\n",
    "\n",
    "    # Load map image\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    # ----- Draw polygons and text -----\n",
    "    for _, row in merged_df.iterrows():\n",
    "\n",
    "        x_coords = [row.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        poly = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not poly.is_valid:\n",
    "            poly = poly.buffer(0)\n",
    "\n",
    "        # Draw polygon outline\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "\n",
    "        centroid = poly.centroid\n",
    "\n",
    "        # Get accuracies\n",
    "        loc_acc = int(row.get(\"Accuracy\", 0))                     # LocationAI (result_d)\n",
    "        fused_acc = int(row.get(\"Predicted_NLOS_Accuracy\", 0))    # Fused / NLOS\n",
    "\n",
    "        # Draw colored numbers side by side\n",
    "        ax.text(\n",
    "            centroid.x - 0.5, centroid.y, f\"{loc_acc}/ \",\n",
    "            color=result_d_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "        ax.text(\n",
    "            centroid.x + 0.6, centroid.y, f\" {fused_acc}\",\n",
    "            color=fused_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "\n",
    "    # ----- Scale axes automatically -----\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ----- Overall weighted accuracies -----\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "\n",
    "    # Overall NLOS (fused)\n",
    "    overall_fused = (accuracy_df[\"Predicted_NLOS_Accuracy\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "\n",
    "    # Overall LocationAI (result_d), weighted by Total_Points from accuracy_df\n",
    "    merged_for_overall = pd.merge(\n",
    "        accuracy_df[[\"Room_name\", \"Total_Points\"]],\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\", how=\"left\"\n",
    "    )\n",
    "    overall_locai = (merged_for_overall[\"Accuracy\"] * merged_for_overall[\"Total_Points\"]).sum() / total_points\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{title_text}\\nOverall LocationAI: {overall_locai:.1f}% | Overall NLOS: {overall_fused:.1f}%\",\n",
    "        fontsize=15, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # ----- Legend -----\n",
    "    legend_handles = [\n",
    "        Patch(color=result_d_color, label=\"LocationAI\"),\n",
    "        Patch(color=fused_color, label=\"NLOS\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room_combined(\n",
    "    accuracy_df=accuracy_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    result_d= result_rf, \n",
    "    map_file_location=map_file,\n",
    "\n",
    "    title_text=\"Asset_Stationary Tag_Nov 19_Single Data Packet\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "def compute_accuracy_per_tag(result_df, ground_truth_df, fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS']):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Merge with ground truth polygons\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        zone_id = row[\"Zone_id\"]\n",
    "        room_name = row[\"Room_name\"]\n",
    "        room_type = row.get(\"Room_Type\")\n",
    "        tag_id = row[\"Tag_id\"]\n",
    "\n",
    "        # Room polygon\n",
    "        x_coords = [row.get(f'x{i+1}') for i in range(8) if pd.notnull(row.get(f'x{i+1}'))]\n",
    "        y_coords = [row.get(f'y{i+1}') for i in range(8) if pd.notnull(row.get(f'y{i+1}'))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Helper to parse prediction coordinates\n",
    "        def parse_coords(col):\n",
    "            val = row.get(col)\n",
    "\n",
    "            # Handle None, NaN, or empty\n",
    "            if val is None:\n",
    "                return [], []\n",
    "            if isinstance(val, float) and pd.isna(val):\n",
    "                return [], []\n",
    "            if isinstance(val, (list, np.ndarray)) and len(val) == 0:\n",
    "                return [], []\n",
    "\n",
    "            try:\n",
    "                # If string, evaluate to list\n",
    "                if isinstance(val, str):\n",
    "                    coords = ast.literal_eval(val)\n",
    "                else:\n",
    "                    coords = val\n",
    "\n",
    "                # Ensure list of points\n",
    "                if len(coords) == 0:\n",
    "                    return [], []\n",
    "                if isinstance(coords[0], (int, float)):\n",
    "                    coords = [coords]\n",
    "\n",
    "                x_list = [float(p[0]) for p in coords]\n",
    "                y_list = [float(p[1]) for p in coords]\n",
    "                return x_list, y_list\n",
    "            except:\n",
    "                return [], []\n",
    "\n",
    "\n",
    "        # Compute inside-polygon accuracy\n",
    "        def inside_accuracy(x_list, y_list):\n",
    "            total = len(x_list)\n",
    "            if total == 0:\n",
    "                return 0\n",
    "            count = sum(1 for x, y in zip(x_list, y_list) if Point(x, y).within(polygon))\n",
    "            return count / total * 100\n",
    "\n",
    "        # Fused methods\n",
    "        fused_acc_dict = {}\n",
    "        for col in fused_cols:\n",
    "            x_list, y_list = parse_coords(col)\n",
    "            fused_acc_dict[col] = inside_accuracy(x_list, y_list)\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone_id,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"tagId\": tag_id,\n",
    "\n",
    "            **{f\"{col}_Accuracy\": acc for col, acc in fused_acc_dict.items()}\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nlos = compute_accuracy_per_tag(result, ground_truth_df, fused_cols=['Predicted_NLOS'])\n",
    "res_nlos= res_nlos.rename(columns={\"Predicted_NLOS_Accuracy\": \"Accuracy\"})\n",
    "res_nlos = pd.DataFrame(res_nlos.groupby([\"Zone_id\", \"Room_name\", \"tagId\",\"Room_Type\"])[\"Accuracy\"].mean().sort_values()).reset_index()\n",
    "res_nlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\"Room\": \"skyblue\", \"Open\": \"orange\"}\n",
    "\n",
    "# Ensure consistent types\n",
    "res_nlos[\"Room_Type\"] = res_nlos[\"Room_Type\"].astype(str)\n",
    "\n",
    "# 1. Sort by Zone_id FIRST\n",
    "res_sorted = res_nlos.sort_values(by=\"Zone_id\")\n",
    "\n",
    "# 2. Create a sort key for Room_Type so \"Room\" comes before \"Open\"\n",
    "type_priority = {\"Room\": 0, \"Open\": 1}\n",
    "\n",
    "# 3. Build final room order sorted by:\n",
    "#    (Zone_id, type_priority, Room_name)\n",
    "res_sorted = res_sorted.sort_values(\n",
    "    by=[\"Zone_id\", \"Room_Type\", \"Room_name\"],\n",
    "    key=lambda col: col.map(type_priority) if col.name == \"Room_Type\" else col\n",
    ")\n",
    "\n",
    "# 4. Create the ordered x-axis list\n",
    "room_order = list(res_sorted[\"Room_name\"].unique())\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=res_sorted,\n",
    "    x=\"Room_name\",\n",
    "    y=\"Accuracy\",\n",
    "    order=room_order,\n",
    "    hue=\"Room_Type\",\n",
    "    dodge=False,\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"NLOS_Tag Accuracy Distribution per Room\")\n",
    "plt.legend(title=\"\")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968c9c9",
   "metadata": {},
   "source": [
    "## C. Using 3 dp\n",
    "Using 3 dp and apply the Centroid for location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def safe_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        return literal_eval(x)\n",
    "    return x\n",
    "\n",
    "def compute_centroid(points):\n",
    "    xs, ys = zip(*points)\n",
    "    return [np.mean(xs), np.mean(ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_by_window(result, window_sizes=range(1, 11)):\n",
    "    result = result.copy()\n",
    "\n",
    "    # Safely evaluate string lists\n",
    "    result['Predicted_MLE'] = result['Predicted_MLE'].apply(safe_eval)\n",
    "    result['Ground_Truth'] = result['Ground_Truth'].apply(safe_eval)\n",
    "\n",
    "    # Handle naming differences\n",
    "    if 'Predicted_Optimisation' in result.columns:\n",
    "        optimisation_col = 'Predicted_Optimisation'\n",
    "    elif 'Predicted_Opt' in result.columns:\n",
    "        optimisation_col = 'Predicted_Opt'\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Predicted_Optimisation' nor 'Predicted_Opt' found in DataFrame\")\n",
    "\n",
    "    result[optimisation_col] = result[optimisation_col].apply(safe_eval)\n",
    "\n",
    "    # Optional fused predictions\n",
    "    has_fused = 'Predicted_NLOS' in result.columns\n",
    "    if has_fused:\n",
    "        result['Predicted_NLOS'] = result['Predicted_NLOS'].apply(safe_eval)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    group_cols = ['Zone_id', 'Room_name', 'Tag_id']\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        for group_keys, group in result.groupby(group_cols):\n",
    "            group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "            n = len(group)\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                # ----------- CORRECTED WINDOW LOGIC ---------------\n",
    "                if i < window_size:\n",
    "                    # BEGINNING: grow window\n",
    "                    start = 0\n",
    "                    end = i + 1\n",
    "                else:\n",
    "                    # SLIDING WINDOW: always full windows\n",
    "                    start = i - window_size + 1\n",
    "                    end = i + 1\n",
    "                # --------------------------------------------------\n",
    "\n",
    "                window = group.iloc[start:end]\n",
    "\n",
    "                # Extract points\n",
    "                mle_points = list(window['Predicted_MLE'])\n",
    "                optimisation_points = list(window[optimisation_col])\n",
    "                ground_truth_points = list(window['Ground_Truth'])\n",
    "\n",
    "                mle_centroid = compute_centroid(mle_points)\n",
    "                optimisation_centroid = compute_centroid(optimisation_points)\n",
    "                ground_truth_centroid = compute_centroid(ground_truth_points)\n",
    "\n",
    "                result_row = {\n",
    "                    'Zone_id': group_keys[0],\n",
    "                    'Room_name': group_keys[1],\n",
    "                    'tagId': group_keys[2],\n",
    "                    'Window_Size': window_size,\n",
    "                    'Predicted_MLE': mle_centroid,\n",
    "                    'Predicted_Optimisation': optimisation_centroid,\n",
    "                    'Ground_Truth': ground_truth_centroid\n",
    "                }\n",
    "\n",
    "                if has_fused:\n",
    "                    fused_points = list(window['Predicted_NLOS'])\n",
    "                    fused_centroid = compute_centroid(fused_points)\n",
    "                    result_row['Predicted_NLOS'] = fused_centroid\n",
    "\n",
    "                all_results.append(result_row)\n",
    "\n",
    "    centroid_df = pd.DataFrame(all_results)\n",
    "    return centroid_df[centroid_df['Room_name'] != 'Womens Restroom']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30293573",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_result = compute_centroids_by_window(result, window_sizes=range(3, 4))\n",
    "centroid_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df_centroid_3 = plot_predicted_all(centroid_result[centroid_result.Window_Size==3], \\\n",
    "                                            ground_truth_df, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd81b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df_centroid_3\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df_centroid_3[\"MLE_Inside_Points\"].sum() / accuracy_df_centroid_3[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df_centroid_3[\"Optimisation_Inside_Points\"].sum() / accuracy_df_centroid_3[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df_centroid_3[\"NLOS_Inside_Points\"].sum() / accuracy_df_centroid_3[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Asset/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d846069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(1, 11))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(centroid_df, ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "                           on=['Zone_id', 'Room_name'], how='left')\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Patch: ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        acc_df = plot_predicted_all(\n",
    "            result=df_w,\n",
    "            ground_truth_df=ground_truth_df,\n",
    "            map_file_location=map_file_location,\n",
    "#             output_file=f\"Result_Duress_Eric_Data_Jun25/temp_plot_ws_{w}.png\"\n",
    "        )\n",
    "\n",
    "        # Overall accuracy\n",
    "        mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "        opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "        fused_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "        grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\", \"Optimisation_Accuracy\",\\\n",
    "                                              'NLOS_Accuracy']].mean()\n",
    "\n",
    "        row = {\n",
    "            \"Window_Size\": w,\n",
    "            \"MLE_Overall\": mle_overall,\n",
    "            \"Optimisation_Overall\": opt_overall,\n",
    "            \"NLOS_Overall\": fused_overall,\n",
    "        }\n",
    "\n",
    "        for room_type in grouped.index:\n",
    "            row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "            row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "            row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "        accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window = compute_accuracy_by_window(result, ground_truth_df, map_file)\n",
    "\n",
    "# accuracy_vs_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window[accuracy_vs_window.Window_Size==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b2b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot curves\n",
    "plt.plot(accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[\"NLOS_Room\"],\n",
    "         marker='o', label='NLOS_Room', markersize= 4)\n",
    "# plt.plot(accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[\"NLOS_Overall\"],\n",
    "#          marker='o', label='NLOS_Overall', markersize= 4)\n",
    "plt.plot(accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[\"NLOS_Open\"],\n",
    "         marker='o', label='NLOS_Open', markersize= 4)\n",
    "\n",
    "# Vertical line at x = 3\n",
    "plt.axvline(x=3, linestyle='--')\n",
    "\n",
    "# Find and annotate intersection values\n",
    "for col in [\"NLOS_Open\", \"NLOS_Room\"]:\n",
    "    y_val = np.interp(3, accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[col])\n",
    "    plt.scatter(3, y_val, s=30)  # smaller dot\n",
    "    plt.text(3.15, y_val, f\"{y_val:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "# ----- Location AI points (smaller dots) -----\n",
    "x_loc = 1\n",
    "\n",
    "plt.scatter(x_loc, result_d_room[\"Room_Accuracy\"].values[0],\n",
    "            marker='D', s=40, label='LocAI_Room')\n",
    "# plt.scatter(x_loc, result_d_room[\"Overall_Accuracy\"].values[0],\n",
    "#             marker='D', s=40, label='LocAI_Overall')\n",
    "plt.scatter(x_loc, result_d_room[\"Open_Accuracy\"].values[0],\n",
    "            marker='D', s=40, label='LocAI_Open')\n",
    "\n",
    "# ----- Annotate Location AI values -----\n",
    "plt.text(x_loc + 0.1, result_d_room[\"Room_Accuracy\"].values[0],\n",
    "         f'{result_d_room[\"Room_Accuracy\"].values[0]:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# plt.text(x_loc + 0.1, result_d_room[\"Overall_Accuracy\"].values[0],\n",
    "#          f'{result_d_room[\"Overall_Accuracy\"].values[0]:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.text(x_loc + 0.1, result_d_room[\"Open_Accuracy\"].values[0],\n",
    "         f'{result_d_room[\"Open_Accuracy\"].values[0]:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# Formatting\n",
    "plt.title('Asset_Normal Case_ NLOS Accuracy vs. # Data Packets')\n",
    "plt.xlabel('# Scans')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(70, 100)\n",
    "plt.grid(True)\n",
    "plt.legend(ncol=2, loc='upper center', bbox_to_anchor=(0.82, 0.15))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ab83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb422cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Correct column mapping for each method + metric\n",
    "column_map = {\n",
    "    \"MLE\": {\n",
    "        \"Room\": \"MLE_Room\",\n",
    "        \"Open\": \"MLE_Open\",\n",
    "        \"Overall\": \"MLE_Overall\"\n",
    "    },\n",
    "    \"Optimisation\": {\n",
    "        \"Room\": \"Optimisation_Room\",\n",
    "        \"Open\": \"Optimisation_Open\",\n",
    "        \"Overall\": \"Optimisation_Overall\"\n",
    "    },\n",
    "    \"NLOS\": {\n",
    "        \"Room\": \"NLOS_Room\",\n",
    "        \"Open\": \"NLOS_Open\",\n",
    "        \"Overall\": \"NLOS_Overall\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Style settings\n",
    "methods = {\n",
    "    'MLE': 'blue',\n",
    "    'Optimisation': 'green',\n",
    "    'NLOS': 'red'\n",
    "}\n",
    "\n",
    "metrics_style = {\n",
    "    'Room': '-',     # solid\n",
    "    'Open': ':',     # dotted\n",
    "    'Overall': '-.'  # dash-dot\n",
    "}\n",
    "\n",
    "# ---- Plot 3 curves per method ----\n",
    "for method, color in methods.items():\n",
    "    for metric, linestyle in metrics_style.items():\n",
    "        col = column_map[method][metric]\n",
    "        \n",
    "        plt.plot(\n",
    "            accuracy_vs_window[\"Window_Size\"],\n",
    "            accuracy_vs_window[col],\n",
    "            linestyle=linestyle,\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            label=f\"{method} {metric}\"\n",
    "        )\n",
    "\n",
    "# ---- Vertical line at x = 5 ----\n",
    "plt.axvline(x=3, color='black', linestyle='--', label='Scan = 3')\n",
    "\n",
    "# ---- Annotate intersections ----\n",
    "for method, color in methods.items():\n",
    "    for metric in metrics_style.keys():\n",
    "        col = column_map[method][metric]\n",
    "        \n",
    "        # interpolated value at x = 5\n",
    "        y_val = np.interp(3, accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[col])\n",
    "        \n",
    "        plt.scatter(3, y_val, color=color)\n",
    "        plt.text(3.15, y_val, f\"{y_val:.1f}%\", va='center', color=color, fontsize=9)\n",
    "\n",
    "plt.title('Stationary Tags — Asset Accuracy vs. # Data Packets')\n",
    "plt.xlabel('# Data Packets')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(80, 100)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Legend handles for line styles (Room / Open / Overall)\n",
    "room_line = mlines.Line2D([], [], color='black', linestyle='-', label='Room (solid)')\n",
    "open_line = mlines.Line2D([], [], color='black', linestyle=':', label='Open Space (dotted)')\n",
    "overall_line = mlines.Line2D([], [], color='black', linestyle='-.', label='Overall (dash-dot)')\n",
    "\n",
    "# Legend handles for colors (MLE / Optimisation / Fused)\n",
    "mle_line = mlines.Line2D([], [], color='blue', linestyle='-', label='MLE')\n",
    "opt_line = mlines.Line2D([], [], color='green', linestyle='-', label='Optimisation')\n",
    "fused_line = mlines.Line2D([], [], color='red', linestyle='-', label='NLOS')\n",
    "\n",
    "plt.legend(\n",
    "    handles=[room_line, open_line, overall_line, mle_line, opt_line, fused_line],\n",
    "    loc='lower right',\n",
    "    fontsize=10,\n",
    "    title=\"\"\n",
    ")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_Fused_asset_accuracy_vs_packets.png\", dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f607a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "def compute_accuracy_per_room(centroid_df, ground_truth_df, fused_cols=['Predicted_NLOS']):\n",
    "    \n",
    "    merged_df = pd.merge(centroid_df, ground_truth_df, on=['Zone_id', 'Room_name'], how='left')\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Room polygon\n",
    "        x_coords = [row.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Count inside points\n",
    "        def inside_count(point):\n",
    "            return int(Point(point).within(polygon))\n",
    "\n",
    "        mle_inside = inside_count(row['Predicted_MLE'])\n",
    "        opt_inside = inside_count(row['Predicted_Optimisation'])\n",
    "        fused_inside = {col: inside_count(row[col]) for col in fused_cols if col in row}\n",
    "\n",
    "        total_points = 1  # each centroid counts as one point\n",
    "\n",
    "        results_row = {\n",
    "            'Zone_id': row['Zone_id'],\n",
    "            'Room_name': row['Room_name'],\n",
    "            'MLE_Inside_Points': mle_inside,\n",
    "            'Optimisation_Inside_Points': opt_inside,\n",
    "            'Total_Points': total_points,\n",
    "            'MLE_Accuracy': mle_inside / total_points * 100,\n",
    "            'Optimisation_Accuracy': opt_inside / total_points * 100,\n",
    "#             'NLOS_Accuracy': fused_inside / total_points * 100\n",
    "        }\n",
    "\n",
    "        for col, val in fused_inside.items():\n",
    "            results_row[f\"{col}_Inside_Points\"] = val\n",
    "            results_row[f\"{col}_Accuracy\"] = val / total_points * 100\n",
    "\n",
    "        all_results.append(results_row)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca137a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_df= centroid_result[centroid_result.Window_Size==3]\n",
    "\n",
    "accuracy_df_centroid = compute_accuracy_per_room(centroid_df, ground_truth_df)\n",
    "\n",
    "# Average over all windows per room\n",
    "accuracy_df_centroid = accuracy_df_centroid.groupby(['Zone_id', 'Room_name']).mean().reset_index()\n",
    "accuracy_df_centroid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90be7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def plot_accuracy_per_room_three_datasets(\n",
    "    accuracy_df,        # original NLOS / fused\n",
    "    accuracy_df1,       # combined 3-row dataset\n",
    "    result_d,           # LocationAI\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    colors=(\"purple\", \"red\"),  # accuracy_df, accuracy_df1 colors\n",
    "    result_d_color=\"blue\",\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise accuracy for 3 datasets:\n",
    "    - Line 1: LocationAI / NLOS (accuracy_df)\n",
    "    - Line 2: accuracy_df1 (third dataset)\n",
    "    - Title shows overall weighted accuracy for all three\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge accuracy_df with ground truth polygons\n",
    "    merged_df = pd.merge(\n",
    "        accuracy_df,\n",
    "        ground_truth_df,\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge result_d (LocationAI)\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge third dataset (accuracy_df1)\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        accuracy_df1[[\"Zone_id\", \"Room_name\", \"Predicted_NLOS_Accuracy\"]],\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_third\")\n",
    "    )\n",
    "\n",
    "    # Load map\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    # ----- Draw polygons and text -----\n",
    "    for _, row in merged_df.iterrows():\n",
    "        x_coords = [row.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        poly = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not poly.is_valid:\n",
    "            poly = poly.buffer(0)\n",
    "\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "        centroid = poly.centroid\n",
    "\n",
    "        # Extract accuracies\n",
    "        loc_acc = int(row.get(\"Accuracy\", 0))                     # LocationAI\n",
    "        fused_acc = int(row.get(\"Predicted_NLOS_Accuracy\", 0))    # NLOS / fused\n",
    "        third_acc = int(row.get(\"Predicted_NLOS_Accuracy_third\", 0))  # third dataset\n",
    "\n",
    "        # Line 1: LocationAI / fused\n",
    "        ax.text(\n",
    "            centroid.x - 0.5, centroid.y, f\"{loc_acc}/\",\n",
    "            color=result_d_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "        ax.text(\n",
    "            centroid.x + 0.8, centroid.y, f\" {fused_acc}\",\n",
    "            color=colors[0], fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "\n",
    "        # Line 2: third dataset\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y - 0.75, f\"{third_acc}\",\n",
    "            color=colors[1], fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "\n",
    "    # ----- Scale axes -----\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ----- Overall weighted accuracies -----\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "\n",
    "    overall_locai = (\n",
    "        pd.merge(accuracy_df[[\"Room_name\", \"Total_Points\"]],\n",
    "                 result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "                 on=\"Room_name\")\n",
    "        .eval(\"Accuracy * Total_Points\").sum()\n",
    "    ) / total_points\n",
    "\n",
    "    overall_fused = (accuracy_df[\"Predicted_NLOS_Accuracy\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "    overall_third = (accuracy_df1.Predicted_NLOS_Accuracy.mean())\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{title_text}_LocationAI: {overall_locai:.1f}% | \"\n",
    "        f\"NLOS_Single_Data_Packet: {overall_fused:.1f}% | NLOS_3_Data_Packets: {overall_third:.1f}%\",\n",
    "        fontsize=15, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # ----- Legend -----\n",
    "    legend_handles = [\n",
    "        Patch(color=result_d_color, label=\"LocationAI\"),\n",
    "        Patch(color=colors[0], label=\"NLOS_single_data_packet\"),\n",
    "        Patch(color=colors[1], label=\"NLOS_3_data_packets\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df_centroid.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7886b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room_three_datasets(\n",
    "    accuracy_df=accuracy_df,\n",
    "    accuracy_df1=accuracy_df_centroid,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    result_d= result_rf, \n",
    "    map_file_location=map_file,\n",
    "\n",
    "    title_text=\"Asset_Moving Tags\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db455c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902ef65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98a853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(1, 11))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(\n",
    "        centroid_df,\n",
    "        ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Patch: ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Group by tagId to get per-tag accuracy\n",
    "        for tag_id, df_tag in df_w.groupby(\"tagId\"):\n",
    "\n",
    "            acc_df = plot_predicted_all(\n",
    "                result=df_tag,\n",
    "                ground_truth_df=ground_truth_df,\n",
    "                map_file_location=map_file_location,\n",
    "            )\n",
    "\n",
    "            # Overall accuracy per tag\n",
    "            mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "            opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "            NLOS_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "            # Room-type aggregated accuracy\n",
    "            grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\", \"Optimisation_Accuracy\", 'NLOS_Accuracy']].mean()\n",
    "\n",
    "            row = {\n",
    "                \"Window_Size\": w,\n",
    "                \"tagId\": tag_id,              # <-- include tagId\n",
    "                \"MLE_Overall\": mle_overall,\n",
    "                \"Optimisation_Overall\": opt_overall,\n",
    "                \"NLOS_Overall\": NLOS_overall,\n",
    "            }\n",
    "\n",
    "            for room_type in grouped.index:\n",
    "                row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "                row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "                row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "            accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window_tag = compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file)\n",
    "\n",
    "accuracy_vs_window_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_by_tag(accuracy_vs_window_tag, col= ['NLOS_Room']):\n",
    "    \"\"\"\n",
    "    Plot accuracy vs. Window_Size for each tag separately.\n",
    "\n",
    "    Parameters:\n",
    "        accuracy_vs_window_tag : pd.DataFrame\n",
    "            Must include columns 'tagId', 'Window_Size', \n",
    "            'NLOS_Overall', 'NLOS_Open', 'NLOS_Room'\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure tagId is string\n",
    "    accuracy_vs_window_tag['tagId'] = accuracy_vs_window_tag['tagId'].astype(str)\n",
    "\n",
    "    numeric_cols = col\n",
    "#     numeric_cols = ['NLOS_Overall', 'NLOS_Open', 'NLOS_Room']\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    markers = ['o', 's', '^']  # for Overall, Open, Room\n",
    "    linestyles = ['-', '--', ':']\n",
    "\n",
    "    for tag in accuracy_vs_window_tag['tagId'].unique():\n",
    "        tag_df = accuracy_vs_window_tag[accuracy_vs_window_tag['tagId'] == tag].sort_values('Window_Size')\n",
    "        \n",
    "        for col, marker, ls in zip(numeric_cols, markers, linestyles):\n",
    "            plt.plot(tag_df['Window_Size'], tag_df[col], marker=marker, linestyle=ls,\n",
    "                     label=f'Tag {tag} {col.replace(\"_\", \" \")}')\n",
    "\n",
    "            # Optional: annotate value at window_size = 5 if exists\n",
    "            if 5 in tag_df['Window_Size'].values:\n",
    "                y_val = tag_df.loc[tag_df['Window_Size'] == 3, col].values[0]\n",
    "                plt.scatter(3, y_val, color='black', marker=marker)\n",
    "                plt.text(3.2, y_val, f\"{y_val:.1f}%\", fontsize=8)\n",
    "\n",
    "    plt.axvline(x=3, color='red', linestyle='--')\n",
    "    plt.xlabel('# Data Packets')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy vs # Data Packets per Tag')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window_tag.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_by_tag(accuracy_vs_window_tag, col= ['NLOS_Room'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75498001",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.tagId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08802a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rssi_box_each_zone_grid(df, top_n=5):\n",
    "\n",
    "    # Detect RSSI feature columns (start with digit)\n",
    "    rssi_cols = [c for c in df.columns if c[0].isdigit()]\n",
    "    df['tagId'] = df['tagId'].astype(str)\n",
    "\n",
    "    unique_tags = df['tagId'].unique()\n",
    "\n",
    "    # Consistent colors for tags\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_tags))\n",
    "    tag_colors = {tag: cmap(i) for i, tag in enumerate(unique_tags)}\n",
    "\n",
    "    # All zones\n",
    "    zones = sorted(df[\"Room_name\"].unique())\n",
    "    n_zones = len(zones)\n",
    "\n",
    "    # Create grid: 2 plots per row\n",
    "    n_cols = 5\n",
    "    n_rows = math.ceil(n_zones / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, zone_id in enumerate(zones):\n",
    "        ax = axes[idx]\n",
    "        zone_df = df[df[\"Room_name\"] == zone_id]\n",
    "\n",
    "        # Identify top N RSSI features for this zone\n",
    "\n",
    "        mean_rssi = zone_df[rssi_cols].where(zone_df[rssi_cols] != -100).mean()\n",
    "\n",
    "        top_features = mean_rssi.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "\n",
    "        plot_data = []\n",
    "        tag_labels = []\n",
    "\n",
    "        # Collect RSSI values per tag\n",
    "        for tag in unique_tags:\n",
    "            tag_df = zone_df[zone_df[\"tagId\"] == tag]\n",
    "\n",
    "            if tag_df.empty:\n",
    "                values = [np.nan] * top_n\n",
    "            else:\n",
    "                values = tag_df[top_features].values.flatten()\n",
    "\n",
    "            plot_data.append(values)\n",
    "            tag_labels.append(tag)\n",
    "\n",
    "        # ---- Plot for this zone ----\n",
    "        box = ax.boxplot(plot_data, patch_artist=True, labels=tag_labels)\n",
    "\n",
    "        # Color each box by tag\n",
    "        for patch, tag in zip(box['boxes'], tag_labels):\n",
    "            patch.set_facecolor(tag_colors[tag])\n",
    "\n",
    "        ax.set_title(f\"{zone_id}\")\n",
    "        ax.set_ylabel(\"RSSI\")\n",
    "        ax.set_xticklabels(tag_labels, rotation=45, ha='right')\n",
    "        ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Remove empty axes if zones are odd\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Add legend below all plots\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color=tag_colors[tag], lw=8, label=f\"Tag {tag}\")\n",
    "        for tag in unique_tags\n",
    "    ]\n",
    "\n",
    "    fig.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Tag ID\",\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, 0.00),\n",
    "        ncol=5\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.07, 1, 1])  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rssi_box_each_zone_grid(data_set_df, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45564059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830a992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de3b6b1c",
   "metadata": {},
   "source": [
    "### Sliding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91012f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_aggregate_3rows(df, window_size=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic rows using sliding window aggregation:\n",
    "    - For each (Zone_id, Room_name, Tag_id), sorted by timestamp\n",
    "    - Sliding window of `window_size` rows\n",
    "    - For each window, compute mean, median, max per beacon, ignoring -100\n",
    "    - Keeps original x and y columns from the last row in the window\n",
    "    - Returns a DataFrame with synthetic rows only\n",
    "    \"\"\"\n",
    "    synthetic_rows = []\n",
    "    \n",
    "    # Sort by keys and timestamp\n",
    "    df_sorted = df.sort_values(by=['Zone_id', 'Room_name', 'tagId', 'timestamp'])\n",
    "    \n",
    "    # Identify beacon columns (assuming names start with '0')\n",
    "    beacon_cols = [c for c in df.columns if str(c).startswith('0')]\n",
    "    \n",
    "    # Group by Zone, Room, Tag\n",
    "    grouped = df_sorted.groupby(['Zone_id', 'Room_name', 'tagId'])\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        n_rows = len(group)\n",
    "        if n_rows < window_size:\n",
    "            continue  # skip if not enough rows\n",
    "        \n",
    "        # Sliding window\n",
    "        for start in range(n_rows - window_size + 1):\n",
    "            window = group.iloc[start:start+window_size]\n",
    "            last_row = window.iloc[-1]\n",
    "            \n",
    "            # Base info including x and y\n",
    "            base_info = {\n",
    "                'Zone_id': last_row['Zone_id'],\n",
    "                'Room_name': last_row['Room_name'],\n",
    "                'tagId': last_row['tagId'],\n",
    "                'timestamp': last_row['timestamp'],\n",
    "                'x': last_row['x'],\n",
    "                'y': last_row['y']\n",
    "            }\n",
    "            \n",
    "            # Compute mean, median, max per beacon ignoring -100\n",
    "            mean_vals, median_vals, max_vals = {}, {}, {}\n",
    "            for beacon in beacon_cols:\n",
    "                values = window[beacon].replace(-100, np.nan).dropna().values\n",
    "                if len(values) == 0:\n",
    "                    mean_vals[beacon] = -100\n",
    "                    median_vals[beacon] = -100\n",
    "                    max_vals[beacon] = -100\n",
    "                else:\n",
    "                    mean_vals[beacon] = np.mean(values)\n",
    "                    median_vals[beacon] = np.median(values)\n",
    "                    max_vals[beacon] = np.max(values)\n",
    "            \n",
    "            # Create synthetic rows\n",
    "            mean_row = {**base_info, **mean_vals, 'agg_type': 'mean'}\n",
    "            median_row = {**base_info, **median_vals, 'agg_type': 'median'}\n",
    "            max_row = {**base_info, **max_vals, 'agg_type': 'max'}\n",
    "            \n",
    "            synthetic_rows.extend([mean_row, median_row, max_row])\n",
    "    \n",
    "    return pd.DataFrame(synthetic_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48905ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df_agg = sliding_window_aggregate_3rows(data_set_df, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd50633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df_agg.shape, data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42650929",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter() \n",
    "\n",
    "result= fused_localization_mle_opt(data_set_df_agg.drop(columns='agg_type'), anchor_point_df)\n",
    "\n",
    "save_folder = \"Result_Asset\"\n",
    "save_name = f\"{filename.replace('.json', '_aggregation.csv')}\" \n",
    "save_path = os.path.join(save_folder, save_name)\n",
    "\n",
    "result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter() \n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_row = total_time / len(data_set_df)\n",
    "print(avg_time_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fixed_window_centroid_predictions(df, window_size=3):\n",
    "    \"\"\"\n",
    "    Compute centroid (mean x and y) for each fixed group of 3 rows:\n",
    "    - Group by Zone_id, Room_name, Tag_id\n",
    "    - Split into non-overlapping groups of 3\n",
    "    - Compute mean of predicted positions and ground truth\n",
    "    \"\"\"\n",
    "    centroid_rows = []\n",
    "\n",
    "    grouped = df.groupby(['Zone_id', 'Room_name', 'Tag_id'])\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        n_rows = len(group)\n",
    "        n_full_groups = n_rows // window_size\n",
    "\n",
    "        for i in range(n_full_groups):\n",
    "            start = i * window_size\n",
    "            window = group.iloc[start:start + window_size]\n",
    "\n",
    "            # Compute mean for each prediction and ground truth\n",
    "            def mean_position(col_name):\n",
    "                # Convert list-like columns to np.array and take mean along axis=0\n",
    "                arrs = np.array(window[col_name].tolist())\n",
    "                return arrs.mean(axis=0).tolist()\n",
    "\n",
    "            centroid_info = {\n",
    "                'Zone_id': window.iloc[-1]['Zone_id'],\n",
    "                'Room_name': window.iloc[-1]['Room_name'],\n",
    "                'Tag_id': window.iloc[-1]['Tag_id'],\n",
    "                'timestamp': window.iloc[-1]['timestamp'],\n",
    "                'Predicted_MLE': mean_position('Predicted_MLE'),\n",
    "                'Predicted_Optimisation': mean_position('Predicted_Optimisation'),\n",
    "                'Predicted_NLOS': mean_position('Predicted_NLOS'),\n",
    "                'Predicted_NLOS_Dynamic': mean_position('Predicted_NLOS_Dynamic'),\n",
    "                'Ground_Truth': mean_position('Ground_Truth'),\n",
    "            }\n",
    "\n",
    "            centroid_rows.append(centroid_info)\n",
    "\n",
    "    return pd.DataFrame(centroid_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_agg= fixed_window_centroid_predictions(result, window_size=3)\n",
    "result_agg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "accuracy_df_agg = plot_predicted_fused_dynamic(\n",
    "    result_df=result_agg,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location= map_file,\n",
    "    fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "#     output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2db028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df_agg\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"Predicted_NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df[\"MLE_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df[\"Optimisation_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df[\"Predicted_NLOS_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Asset/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1WPcT_OjSaguNAQ9AnkpmmUV_GDiijIq5",
     "timestamp": 1717696268126
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa6ad618d5445e7a2343db292d10e8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a8df1c1c1f1460695b2fdd40ab5d774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3887145dd10742d2aaec48b48fbf209f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841ae5e6f05644f4a06df98a4b3d3e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "868e78ed1be844ff89c007db86f1c77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8810a66e33784737a04f2d4de120a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a78544536d5d4e87b691714c15e379a5",
       "IPY_MODEL_ece131d2b91b40e28153ed88cb397481",
       "IPY_MODEL_919d61916e484f10a410b26b643574bf"
      ],
      "layout": "IPY_MODEL_3887145dd10742d2aaec48b48fbf209f"
     }
    },
    "89cc3e4005b640cab78311861d917f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919d61916e484f10a410b26b643574bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841ae5e6f05644f4a06df98a4b3d3e56",
      "placeholder": "​",
      "style": "IPY_MODEL_0aa6ad618d5445e7a2343db292d10e8b",
      "value": " 0/100 [00:00&lt;?, ?it/s]"
     }
    },
    "a78544536d5d4e87b691714c15e379a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89cc3e4005b640cab78311861d917f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_e5079de1d27140eb9f6c4fabe6f199f5",
      "value": "  0%"
     }
    },
    "e5079de1d27140eb9f6c4fabe6f199f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece131d2b91b40e28153ed88cb397481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a8df1c1c1f1460695b2fdd40ab5d774",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_868e78ed1be844ff89c007db86f1c77e",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
