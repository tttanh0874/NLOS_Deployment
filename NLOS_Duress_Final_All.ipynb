{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from typing import Type, List, Dict, Tuple, Set\n",
    "import argparse\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.externals.joblib import parallel_backend, Parallel, delayed\n",
    "except ImportError:\n",
    "    import joblib\n",
    "    from joblib import parallel_backend, Parallel, delayed\n",
    "    \n",
    "import pandas as pd\n",
    "import json, ijson\n",
    "import os, sys, uuid\n",
    "from pykalman import KalmanFilter\n",
    "from PIL import Image\n",
    "import math\n",
    "import ast\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from geopy.distance import geodesic, distance\n",
    "from geopy import Point\n",
    "from shapely.geometry import Point, Polygon as ShapelyPolygon\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "import ast\n",
    "\n",
    "from collections import defaultdict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "CHANNELS = [37,38,39]\n",
    "N_ESTIMATORS = 100\n",
    "MISSING_VALUE = -100\n",
    "DEBUG_LOGGING = False\n",
    "S3_CACHING_BUCKET = 'cognosos-ml-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_data_woc(scan: List[Dict]) -> Dict:\n",
    "    # Parse each scan to get maximum reading for each MAC address in specified channels\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            readings_by_mac_addr_and_channel[mac_addr] += readings\n",
    "    return {mac_addr: int(max(readings)) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if readings}\n",
    "\n",
    "\n",
    "def process_training(data_filepath: str) -> List[Dict]:\n",
    "    X = []\n",
    "\n",
    "    # parse it incrementally\n",
    "    with open(data_filepath, 'r') as f:\n",
    "        # reads the JSON incrementally\n",
    "        objects = ijson.items(f, 'item') \n",
    "\n",
    "        print('Done loading JSON incrementally')\n",
    "\n",
    "        for scan in objects:\n",
    "            \n",
    "            Zone_id = str(scan['zoneId'])\n",
    "            Room_name = str(scan['zoneName'])\n",
    "            parent_zone_id = str(scan['parentZoneId'])\n",
    "            tagId = scan['tagId']\n",
    "            timestamp = scan['rxAt']\n",
    "            scan_readings: List[Dict] = scan['scandata']\n",
    "            \n",
    "            row = parse_scan_data_woc(scan_readings) \n",
    "\n",
    "            row.update({\n",
    "                'Zone_id': Zone_id,\n",
    "                'Room_name': Room_name,\n",
    "                'parent_zone_id': parent_zone_id,\n",
    "                'tagId': tagId,\n",
    "                'timestamp': timestamp,\n",
    "            })\n",
    "\n",
    "            if row:\n",
    "                X.append(row)\n",
    "\n",
    "    print('Done processing data')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13926dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variable(X_train, y_train_floor, y_train, save_models=False):\n",
    "    \n",
    "    floor_pipeline = Pipeline([\n",
    "        ('rf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    floor_pipeline.fit(X_train, y_train_floor)\n",
    "\n",
    "    clf_floor = floor_pipeline.named_steps['rf']\n",
    "\n",
    "    clf_rooms = {}\n",
    "\n",
    "    selected_features = {}\n",
    "\n",
    "    for floor_num, samples in X_train.groupby(y_train_floor):\n",
    "        \n",
    "        floor_labels = y_train[samples.index]\n",
    "\n",
    "        non_all_neg_120_columns = samples.columns[~np.all(samples == -120, axis=0)]\n",
    "\n",
    "        selected_samples = samples[non_all_neg_120_columns]\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=200, random_state=100)\n",
    "\n",
    "        classifier.fit(selected_samples, floor_labels)\n",
    "\n",
    "        clf_rooms[str(floor_num)] = classifier\n",
    "\n",
    "        selected_features[str(floor_num)] = selected_samples.columns.tolist()\n",
    "\n",
    "    if save_models:\n",
    "        model = {\n",
    "        'selected_features': selected_features,\n",
    "        'clf_rooms': clf_rooms,\n",
    "        'clf_floor': clf_floor\n",
    "        }\n",
    "        joblib.dump(model, 'Hier_Features.joblib')\n",
    "        \n",
    "    return selected_features, clf_rooms, clf_floor\n",
    "\n",
    "def predict_variable(X_test, clf_floor, clf_rooms, selected_features):\n",
    "    \n",
    "    predicted_floors = clf_floor.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "    for floor_num, sample in zip(predicted_floors, X_test.values):\n",
    "        classifier = clf_rooms[str(floor_num)]\n",
    "\n",
    "        selected_names = selected_features[floor_num]\n",
    "\n",
    "        selected_sample = sample[X_test.columns.isin(selected_names)].reshape(1, -1)\n",
    "\n",
    "        predicted_room = classifier.predict(selected_sample)[0]\n",
    "#         predicted_room = predicted_room.astype(str)\n",
    "        predictions.append(predicted_room)\n",
    "\n",
    "    return predictions, predicted_floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(scan_data):\n",
    "\n",
    "    if scan_data is None:\n",
    "        return []\n",
    "    return [\n",
    "        {'macHex': entry['macHex'], 'channel': entry['channel'], 'readings': [entry['rssi'][0]]}\n",
    "        for entry in scan_data if 'macHex' in entry and 'rssi' in entry\n",
    "    ]\n",
    "\n",
    "def parse_scan_data(scan: List[Dict]) -> Dict:\n",
    "\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            channel = beacon_reading['channel']\n",
    "            readings_by_mac_addr_and_channel[f'{mac_addr}'] += readings#-{channel}\n",
    "    return {mac_addr: max(readings) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if len(readings) > 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digital_twin(Anchor_point_location_file, ground_truth_file_location, map_file_location):\n",
    "\n",
    "    anchor_df = pd.read_csv(Anchor_point_location_file)\n",
    "    anchor_df[\"x\"] = anchor_df[\"x\"].astype(int)\n",
    "    anchor_df[\"y\"] = anchor_df[\"y\"].astype(int)\n",
    "    anchor_df['Mac'] = anchor_df['Mac'].astype(str).str.zfill(12)\n",
    "    \n",
    "    macLists = anchor_df['Mac'].to_list()\n",
    "    \n",
    "    ground_truth_df = pd.read_csv(ground_truth_file_location)\n",
    "    ground_truth_df[\"Zone_id\"] = ground_truth_df[\"Zone_id\"].astype(str)\n",
    "    \n",
    "    #create a empty map with 0s for future calculation\n",
    "    map_ = np.zeros((65,28))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    image = Image.open(map_file_location)\n",
    "    \n",
    "    plt.scatter(anchor_df.x,anchor_df.y, color='blue', s=50, edgecolors='black', label='Beacons', marker='o', alpha=0.6)\n",
    "\n",
    "#     plt.scatter(ground_truth_df[\"x\"], ground_truth_df[\"y\"], color='red', s=20, label='Ground Truth', marker='^')\n",
    "#     for i, label in enumerate(ground_truth_df['Room_name']):  \n",
    "#         plt.text(ground_truth_df['x'][i], ground_truth_df['y'][i], label, fontsize=9, color='w', ha='right', va='bottom')\n",
    "    plt.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    plt.xlim(0, 65)\n",
    "    plt.ylim(0, 28)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xticks([i for i in range(0, 65, 5)])\n",
    "    plt.yticks([i for i in range(0, 28, 4)])\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.title(\"Beacon distribution in meters\")\n",
    "    plt.legend()\n",
    "    plt.savefig('beacon_map_cognosos.png')\n",
    "\n",
    "    return anchor_df, ground_truth_df, map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823e749-f1be-43ef-8323-29d6aa5ec414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beacon_file = 'ground_truth/Beacon_map_cognosos_flr3.csv'\n",
    "ground_truth_file = \"ground_truth/Ground_truth_Mar25.csv\"\n",
    "map_file = 'ground_truth/Cognosos_view.png'\n",
    "\n",
    "anchor_point_df, ground_truth_df, map_ = create_digital_twin(beacon_file, ground_truth_file, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_features(row, df1):\n",
    "\n",
    "    valid_features = {}\n",
    "    \n",
    "    for mac in df1['Mac']:\n",
    "       \n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] != -100:\n",
    "            valid_features[mac] = row[mac]\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "def convert_coordinates(coord_str):\n",
    "    if isinstance(coord_str, str):\n",
    "       \n",
    "        try:\n",
    "            coord_str = coord_str.strip(\"[]\")\n",
    "            elements = coord_str.split()\n",
    "            return [float(elem) for elem in elements] \n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "        try:\n",
    "            coord_str = coord_str.replace(\" \", \",\")\n",
    "            coord_str = coord_str.replace(\",,\", \",\")\n",
    "            coord_str = coord_str.strip(',')\n",
    "            return ast.literal_eval(coord_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error processing coordinate string: {coord_str}\")\n",
    "            return None\n",
    "    else:\n",
    "        return coord_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_all(result, ground_truth_df, map_file_location, output_file=\"compare_plot_MLE_NLOS.png\"):\n",
    "    results = []\n",
    "    total_points_mle = 0\n",
    "    total_points_Optimisation = 0\n",
    "    total_points_fuse = 0\n",
    "\n",
    "    total_inside_mle = 0\n",
    "    total_inside_Optimisation = 0\n",
    "    total_inside_fuse = 0\n",
    "\n",
    "    merged_df = pd.merge(result, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        has_fused = 'Predicted_NLOS' in room_data.columns\n",
    "\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "\n",
    "        if not polygon.is_valid:\n",
    "            print(f\"Invalid polygon for '{room_name}', attempting to fix with buffer(0).\")\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Parse MLE predictions\n",
    "        x_pred_mle, y_pred_mle = [], []\n",
    "        for coord in room_data[\"Predicted_MLE\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_mle.append(float(coord[0]))\n",
    "                y_pred_mle.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid MLE coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Optimisation predictions\n",
    "        x_pred_Optimisation, y_pred_Optimisation = [], []\n",
    "        for coord in room_data[\"Predicted_Optimisation\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_Optimisation.append(float(coord[0]))\n",
    "                y_pred_Optimisation.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid Optimisation coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Fused predictions only if available\n",
    "        x_pred_fuse, y_pred_fuse = [], []\n",
    "        if has_fused:\n",
    "            for coord in room_data[\"Predicted_NLOS\"]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_pred_fuse.append(float(coord[0]))\n",
    "                    y_pred_fuse.append(float(coord[1]))\n",
    "                except:\n",
    "                    print(f\"Invalid NLOS coord in '{room_name}': {coord}\")\n",
    "\n",
    "        inside_count_mle = sum(1 for x, y in zip(x_pred_mle, y_pred_mle) if Point(x, y).within(polygon))\n",
    "        inside_count_Optimisation = sum(1 for x, y in zip(x_pred_Optimisation, y_pred_Optimisation) if Point(x, y).within(polygon))\n",
    "        inside_count_fuse = sum(1 for x, y in zip(x_pred_fuse, y_pred_fuse) if Point(x, y).within(polygon)) if has_fused else 0\n",
    "\n",
    "        total_points_mle += len(x_pred_mle)\n",
    "        total_inside_mle += inside_count_mle\n",
    "\n",
    "        total_points_Optimisation += len(x_pred_Optimisation)\n",
    "        total_inside_Optimisation += inside_count_Optimisation\n",
    "\n",
    "        if has_fused:\n",
    "            total_points_fuse += len(x_pred_fuse)\n",
    "            total_inside_fuse += inside_count_fuse\n",
    "\n",
    "        percentage_inside_mle = (inside_count_mle / len(x_pred_mle)) * 100 if x_pred_mle else 0\n",
    "        percentage_inside_Optimisation = (inside_count_Optimisation / len(x_pred_Optimisation)) * 100 if x_pred_Optimisation else 0\n",
    "        percentage_inside_fuse = (inside_count_fuse / len(x_pred_fuse)) * 100 if has_fused and x_pred_fuse else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            'Room_name': room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            'MLE_Accuracy': percentage_inside_mle,\n",
    "            'Optimisation_Accuracy': percentage_inside_Optimisation,\n",
    "            'NLOS_Accuracy': percentage_inside_fuse if has_fused else None,\n",
    "            'MLE_Inside_Points': inside_count_mle,\n",
    "            'Optimisation_Inside_Points': inside_count_Optimisation,\n",
    "            'NLOS_Inside_Points': inside_count_fuse if has_fused else None,\n",
    "            'Total_Points': len(x_pred_mle),\n",
    "        })\n",
    "\n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_pred_mle, y_pred_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_pred_Optimisation, y_pred_Optimisation, color='green', s=8, label='Optimisation')\n",
    "        if has_fused:\n",
    "            ax.scatter(x_pred_fuse, y_pred_fuse, color='red', s=8, label='NLOS')\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        title_str = f\"{room_name} - MLE: {percentage_inside_mle:.1f}%, Optimisation: {percentage_inside_Optimisation:.1f}%\"\n",
    "        if has_fused:\n",
    "            title_str += f\", NLOS: {percentage_inside_fuse:.1f}%\"\n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    overall_mle_accuracy = (total_inside_mle / total_points_mle) * 100 if total_points_mle > 0 else 0\n",
    "    overall_Optimisation_accuracy = (total_inside_Optimisation / total_points_Optimisation) * 100 if total_points_Optimisation > 0 else 0\n",
    "    overall_fuse_accuracy = (total_inside_fuse / total_points_fuse) * 100 if total_points_fuse > 0 else 0\n",
    "\n",
    "    print(f\"\\nOverall MLE Accuracy: {overall_mle_accuracy:.2f}%\")\n",
    "    print(f\"Overall Optimisation Accuracy: {overall_Optimisation_accuracy:.2f}%\")\n",
    "    if total_points_fuse > 0:\n",
    "        print(f\"Overall NLOS Accuracy: {overall_fuse_accuracy:.2f}%\")\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89394c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MLE_data_survey_portal(df, ground_truth_df, anchor_point_df, export_unheard=False, export_path=\"unheard_anchor_points.csv\"):\n",
    "\n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_point_df['Mac'] = anchor_point_df['Mac'].astype(str).str.zfill(12)\n",
    "\n",
    "    data_set_df = pd.DataFrame()\n",
    "    merged_df = pd.merge(df, ground_truth_df, on=[\"Zone_id\", 'Room_name'], how='inner').drop(['parent_zone_id'], axis=1)\n",
    "    zones = df['Zone_id']\n",
    "    heard_anchor_points = []\n",
    "\n",
    "    for mac_addr in anchor_point_df['Mac']:\n",
    "        if mac_addr in merged_df.columns:\n",
    "            data_set_df[mac_addr] = merged_df[mac_addr]\n",
    "            heard_anchor_points.append(mac_addr)\n",
    "\n",
    "    heard_anchor_point_df = anchor_point_df[anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "    unheard_anchor_point_df = anchor_point_df[~anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "\n",
    "    heard_anchor_points_coord = heard_anchor_point_df[['x', 'y']].values\n",
    "\n",
    "    data_set_df[\"Zone_id\"] = merged_df[\"Zone_id\"]\n",
    "    data_set_df[\"Room_name\"] = merged_df[\"Room_name\"]\n",
    "    data_set_df[\"tagId\"] = merged_df[\"tagId\"]\n",
    "    data_set_df[\"timestamp\"] = merged_df[\"timestamp\"]\n",
    "    if \"channel\" in merged_df.columns:\n",
    "        data_set_df[\"channel\"] = merged_df[\"channel\"]\n",
    "    \n",
    "    data_set_df[\"x\"] = merged_df[\"x\"]\n",
    "    data_set_df[\"y\"] = merged_df[\"y\"]\n",
    "\n",
    "    return data_set_df, heard_anchor_points_coord, unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27d89e",
   "metadata": {},
   "source": [
    "### Excluding zone outside the ofiice as I remove all beacon there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones= ground_truth_df.Zone_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/Duress/data_duress_walk_real_time_Nov19.json\"\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "X1 = process_training(filepath)\n",
    "df1 = pd.DataFrame(X1)\n",
    "df1 = df1.fillna(MISSING_VALUE)  \n",
    "\n",
    "float_cols = df1.select_dtypes(include=['float']).columns\n",
    "df1[float_cols] = df1[float_cols].astype(np.int8)\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], utc=True, errors='coerce')\n",
    "df1 = df1.sort_values(by='timestamp')\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# Fix specific zone_id\n",
    "df1.loc[df1['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# Beacon processing\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "df1= df1[df1.Zone_id.isin(zones)]\n",
    "print(df1.shape)\n",
    "\n",
    "df1 = df1[df1['beacon_count'] >= 5]\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beca616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['beacon_count'].max(), df1['beacon_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3], axis=0, ignore_index=True, sort=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a49089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/Duress/1_data_duress_normalcase_combine.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(\"data/Duress/1_data_duress_normalcase_combine.csv\")\n",
    "df1.tagId= df1.tagId.astype(str)\n",
    "df1.Zone_id= df1.Zone_id.astype(str)\n",
    "df1.parent_zone_id= df1.parent_zone_id.astype(str)\n",
    "df1= df1[df1.Zone_id.isin(zones)]\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc9d5",
   "metadata": {},
   "source": [
    "## REMOVE ALL ROWS < -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "rows_all_below_90 = df1[beacon_cols].lt(-99).all(axis=1)\n",
    "df1 = df1[~rows_all_below_90]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423898f9",
   "metadata": {},
   "source": [
    "### Check if dataset have enoguh beacon heard >=-90, SELECT ONLY the number of strong features >=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1.drop(columns=[\"beacon_count\"]), \\\n",
    "                                ground_truth_df[['Zone_id','x', 'y']], on=[\"Zone_id\"], how='left')\n",
    "rssi_cols = [col for col in df2.columns if col.startswith('0')]\n",
    "\n",
    "# Create a new column counting RSSIs >= -90\n",
    "df2['num_strong_features'] = (df2[rssi_cols] >= -95).sum(axis=1)\n",
    "\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df = df2[df2['num_strong_features'] >= 4].copy()\n",
    "\n",
    "data_set_df=data_set_df.drop(columns='num_strong_features').reset_index(drop=True)\n",
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b10787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.iloc[[2885]].describe().T.sort_values(by=\"max\", ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876277d2",
   "metadata": {},
   "source": [
    "# Location AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop= anchor_point_df[anchor_point_df.Remove==\"remove\"].Mac.tolist()\n",
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AI= df2.copy()\n",
    "# df_AI= df_AI.drop(columns= columns_to_drop)\n",
    "# df_AI[\"parent_zone_id\"]=df2[\"parent_zone_id\"].astype(str)\n",
    "train_data, test_data = train_test_split(df_AI, test_size=0.2, random_state=42, \\\n",
    "                                         stratify=df_AI[\"Zone_id\"])\n",
    "\n",
    "X_train = train_data[[col for col in train_data.columns if col.startswith(\"0\")]].drop(columns= columns_to_drop)\n",
    "y_train_floor = train_data['parent_zone_id'] \n",
    "y_train = train_data['Zone_id']\n",
    "\n",
    "X_test = test_data[[col for col in train_data.columns if col.startswith(\"0\")]].drop(columns= columns_to_drop)\n",
    "y_test_floor = test_data['parent_zone_id'] \n",
    "y_test = test_data['Zone_id'] \n",
    "df_AI.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95793b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, clf_rooms, clf_floor = train_variable(X_train, y_train_floor, y_train, save_models = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5a03f",
   "metadata": {},
   "source": [
    "### For the same data of normal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rooms, predicted_floors = predict_variable(X_test, clf_floor, clf_rooms, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91d9e2",
   "metadata": {},
   "source": [
    "### Test against the edgec case file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40644db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= pd.read_csv(\"data/Duress/3_data_duress_edgecase_combine.csv\")\n",
    "df_test.tagId= df_test.tagId.astype(str)\n",
    "df_test.Zone_id= df_test.Zone_id.astype(str)\n",
    "df_test.parent_zone_id= df_test.parent_zone_id.astype(str)\n",
    "df_test= df_test[df_test.Zone_id.isin(zones)]\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df_test = df_test.reindex(columns=new_column_order)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['Room_name'] = df_test['Room_name'].str.split('-').str[-1].str.strip()\n",
    "beacon_cols = [col for col in df_test.columns if str(col).startswith('0')]\n",
    "df_test = df_test.fillna(MISSING_VALUE)\n",
    "df_test['beacon_count'] = (df_test[beacon_cols] != -100).sum(axis=1)\n",
    "\n",
    "beacon_cols = [col for col in df_test.columns if str(col).startswith('0')]\n",
    "rows_all_below_90 = df_test[beacon_cols].lt(-99).all(axis=1)\n",
    "df_test = df_test[~rows_all_below_90]\n",
    "\n",
    "\n",
    "df2 = pd.merge(df_test.drop(columns=[\"beacon_count\"]), \\\n",
    "                                ground_truth_df[['Zone_id','x', 'y']], on=[\"Zone_id\"], how='left')\n",
    "rssi_cols = [col for col in df2.columns if col.startswith('0')]\n",
    "\n",
    "# Create a new column counting RSSIs >= -90\n",
    "df2['num_strong_features'] = (df2[rssi_cols] >= -95).sum(axis=1)\n",
    "\n",
    "data_set_df = df2[df2['num_strong_features'] >= 4].copy()\n",
    "\n",
    "data_set_df=data_set_df.drop(columns='num_strong_features').reset_index(drop=True)\n",
    "data_set_df.shape\n",
    "\n",
    "data_set_df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_set_df[[col for col in train_data.columns if col.startswith(\"0\")]].drop(columns= columns_to_drop)\n",
    "y_test_floor = data_set_df['parent_zone_id'] \n",
    "y_test = data_set_df['Zone_id'] \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e72a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rooms, predicted_floors = predict_variable(X_test, clf_floor, clf_rooms, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_test, predicted_rooms)\n",
    "print('Room Accuracy: {:.2f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dde768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_d = df_test1[[\"Room_name\", 'tagId', 'Zone_id']]\\\n",
    "#     .merge(ground_truth_df[[\"Zone_id\", \"Room_Type\"]], on = \"Zone_id\", how=\"left\")\n",
    "\n",
    "result_d = data_set_df[[\"Room_name\", 'tagId', 'Zone_id', \"timestamp\"]]\\\n",
    "    .merge(ground_truth_df[[\"Zone_id\", \"Room_Type\"]], on = \"Zone_id\", how=\"left\")\n",
    "result_d[\"Prediction\"] = predicted_rooms\n",
    "result_d[\"Accuracy\"] = np.where(result_d.Zone_id == result_d.Prediction, 100, 0)\n",
    "result_d= result_d\n",
    "result_d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36964d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.groupby(\"Room_Type\").Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf= pd.DataFrame(result_d.groupby([\"Zone_id\", \"Room_name\"]).Accuracy.mean().reset_index())\n",
    "# result_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c57cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.groupby(\"Room_Type\").Zone_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269debd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac73326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def sliding_window_voting(df, window=5):\n",
    "    \"\"\"\n",
    "    Sliding window majority vote performed per Zone_id AND per tagId.\n",
    "    Sorted by timestamp inside each group.\n",
    "\n",
    "    Tie-breaker: the most recent prediction inside the window.\n",
    "\n",
    "    Returns: dataframe with columns:\n",
    "        Zone_id, tagId, timestamp, voted_prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure timestamp is datetime\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "    # Sort before grouping\n",
    "    df = df.sort_values([\"Zone_id\", \"tagId\", \"timestamp\"]).copy()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Group by Zone_id and tagId\n",
    "    for (zone, room, tag, roomtype), group in df.groupby([\"Zone_id\", \"Room_name\", \"tagId\", \"Room_Type\"]):\n",
    "\n",
    "        preds = group[\"Prediction\"].tolist()\n",
    "        timestamps = group[\"timestamp\"].tolist()\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            start_idx = max(0, i - window + 1)\n",
    "            window_preds = preds[start_idx : i + 1]\n",
    "\n",
    "            # Count occurrences\n",
    "            counts = Counter(window_preds)\n",
    "            max_count = max(counts.values())\n",
    "\n",
    "            # All candidates with equal max votes\n",
    "            candidates = [p for p, v in counts.items() if v == max_count]\n",
    "\n",
    "            # Tie-breaker: latest occurrence in the window\n",
    "            for val in reversed(window_preds):\n",
    "                if val in candidates:\n",
    "                    voted = val\n",
    "                    break\n",
    "\n",
    "            results.append({\n",
    "                \"Zone_id\": zone,\n",
    "                \"Room_name\": room,\n",
    "                \"Room_Type\": roomtype,\n",
    "                \"tagId\": tag,\n",
    "                \"timestamp\": timestamps[i],\n",
    "                \"voted_prediction\": voted\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa990452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_from_voted(voted_df):\n",
    "    df = voted_df.copy()\n",
    "\n",
    "    # Accuracy: correct = 100, incorrect = 0\n",
    "    df[\"correct\"] = (df[\"voted_prediction\"] == df[\"Zone_id\"]).astype(int) * 100\n",
    "\n",
    "    # Group by Zone_id + Room_name\n",
    "    result = (\n",
    "        df.groupby([\"Zone_id\", \"Room_name\"])[\"correct\"]\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "          .rename(columns={\"correct\": \"Accuracy\"})\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84346625",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdec297",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df1 = sliding_window_voting(result_d, window=1)\n",
    "voted_df1[\"Accuracy\"] = (voted_df1[\"voted_prediction\"] == voted_df1[\"Zone_id\"]).astype(int) * 100\n",
    "\n",
    "result_rf_1 = compute_accuracy_from_voted(voted_df1)\n",
    "result_rf_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a0a23",
   "metadata": {},
   "source": [
    "## Vote 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df = sliding_window_voting(result_d, window=5)\n",
    "voted_df[\"Accuracy\"] = (voted_df[\"voted_prediction\"] == voted_df[\"Zone_id\"]).astype(int) * 100\n",
    "voted_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23017c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_df.groupby(\"Room_Type\").Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf = compute_accuracy_from_voted(voted_df)\n",
    "result_rf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf.Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b776ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_accuracy(df):\n",
    "    \"\"\"\n",
    "    df: DataFrame with columns\n",
    "        Room_name, tagId, Zone_id, Room_Type, Prediction, Accuracy\n",
    "    Returns a summary dataframe with:\n",
    "        - Overall Accuracy\n",
    "        - Accuracy per Room_Type\n",
    "    \"\"\"\n",
    "\n",
    "    # Overall accuracy (mean of Accuracy column)\n",
    "    overall_acc = df[\"Accuracy\"].mean()\n",
    "\n",
    "    # Accuracy per Room_Type\n",
    "    room_type_acc = df.groupby(\"Room_Type\")[\"Accuracy\"].mean().reset_index()\n",
    "    room_type_acc.rename(columns={\"Accuracy\": \"Accuracy_by_Room_Type\"}, inplace=True)\n",
    "\n",
    "    # Combine into a single dataframe\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Overall_Accuracy\": [overall_acc]\n",
    "    })\n",
    "\n",
    "    # Merge room_type accuracy as separate columns\n",
    "    for _, row in room_type_acc.iterrows():\n",
    "        summary_df[row[\"Room_Type\"] + \"_Accuracy\"] = row[\"Accuracy_by_Room_Type\"]\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf52251",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_room= compute_summary_accuracy(voted_df)\n",
    "result_d_room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf66a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_room1= compute_summary_accuracy(voted_df1)\n",
    "result_d_room1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c24c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34f22d35",
   "metadata": {},
   "source": [
    "# NLOS: Fused of Opt and MLE: using MLE as initial for Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fe6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(point, points, height_diff=1.5):\n",
    "    return np.sqrt(np.sum((points - point) ** 2, axis=1) + height_diff ** 2)\n",
    "\n",
    "def generate_grid(center, resolution=5, radius=4):\n",
    "    step = 1 / resolution\n",
    "    x_vals = np.arange(center[0] - radius, center[0] + radius + step, step)\n",
    "    y_vals = np.arange(center[1] - radius, center[1] + radius + step, step)\n",
    "    xv, yv = np.meshgrid(x_vals, y_vals)\n",
    "    return np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "\n",
    "def trilateration(coords, distances):\n",
    "    def fun(x, coords, distances):\n",
    "        weights = 1 / (distances + 1e-5)\n",
    "        return weights * (np.linalg.norm(coords - x, axis=1) - distances)\n",
    "    x0 = np.mean(coords, axis=0)\n",
    "    result = least_squares(fun, x0, args=(coords, distances))\n",
    "    return result.x if result.success else x0\n",
    "\n",
    "def rssi_to_distance(rssi, A=-65, n=3.5, scale=0.8):\n",
    "    return scale * np.exp((A - rssi) / (10 * n))\n",
    "\n",
    "def filter_rssi(row, beacon_positions, rssi_threshold=-90):\n",
    "    return {\n",
    "        mac: row[mac]\n",
    "        for mac in beacon_positions.keys()\n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] > rssi_threshold\n",
    "    }\n",
    "\n",
    "def localization_error(tag_position, beacons, distances):\n",
    "    estimated_distances = np.linalg.norm(beacons - tag_position, axis=1)\n",
    "    sigma = np.std(distances) + 1e-3\n",
    "    weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "    return np.sum(weights * (estimated_distances - distances) ** 2)\n",
    "\n",
    "def generate_expansion_area(initial_guess, std_dev=0.5, radius=0.8, num_points=500, range_box=2):\n",
    "    num_gauss = int(num_points * 0.4)\n",
    "    num_box = int(num_points * 0.3)\n",
    "    num_circle = num_points - num_gauss - num_box\n",
    "    box_points = np.random.uniform(-range_box, range_box, size=(num_box, 2)) + initial_guess\n",
    "    gauss_points = np.random.normal(0, std_dev, size=(num_gauss, 2)) + initial_guess\n",
    "    r = radius * np.sqrt(np.random.uniform(0, 1, num_circle))\n",
    "    theta = np.random.uniform(0, 2 * np.pi, num_circle)\n",
    "    circ_points = np.column_stack((initial_guess[0] + r * np.cos(theta), initial_guess[1] + r * np.sin(theta)))\n",
    "    return np.vstack((gauss_points, box_points, circ_points))\n",
    "\n",
    "def calculate_total_error_to_all_beacons(best_position, beacon_coords):\n",
    "    distances = np.linalg.norm(beacon_coords - best_position, axis=1)\n",
    "    return np.sum(distances)\n",
    "\n",
    "def compute_likelihood_weighted(grid_coords, anchor_coords, rssi_values, T, n, sigma_noise=4, anchor_weights=None):\n",
    "    if anchor_weights is None:\n",
    "        anchor_weights = np.ones_like(rssi_values)\n",
    "    diff = grid_coords[:, None, :] - anchor_coords[None, :, :]\n",
    "    dists = np.sqrt(np.sum(diff ** 2, axis=2) + 1.5**2)\n",
    "    pred_rssi = T - 10 * n * np.log10(dists + 1e-5)\n",
    "    residuals = pred_rssi - rssi_values\n",
    "    weighted_residuals = (residuals / sigma_noise)**2 * anchor_weights\n",
    "    likelihood = np.exp(-0.5 * weighted_residuals)\n",
    "    return np.prod(likelihood, axis=1)\n",
    "\n",
    "def find_mle_params(P_j, d_ij, init_guess=[-45, 3]):  # Chneg from -65\n",
    "    def squared_error(params, dists, rssi):\n",
    "        T_i, n_p = params\n",
    "        valid_mask = rssi != -100\n",
    "        pred_rssi = T_i - 10 * n_p * np.log10(dists + 1e-5)\n",
    "        return np.sum((pred_rssi[valid_mask] - rssi[valid_mask]) ** 2)\n",
    "    bounds = [(-100, -30), (2, 6)]\n",
    "    result = minimize(squared_error, init_guess, args=(d_ij, P_j),\n",
    "                      method='L-BFGS-B', bounds=bounds)\n",
    "    return result.x if result.success else init_guess\n",
    "\n",
    "# -------------------------------\n",
    "# Fused Localization (MLE → Opt → Refine)\n",
    "# -------------------------------\n",
    "def fused_localization_mle_opt(data_df, anchor_point_df,\n",
    "                               sigma_noise=4, coarse_res=2, fine_res=5, fine_radius=3,\n",
    "                               rssi_threshold=-95, strong_rssi_threshold=-75,\n",
    "                               top_k_anchors=5, roi_margin=8, top_coarse_points=200, topN_ratio=0.05,\n",
    "                               map_x_bounds=(0, 65), map_y_bounds=(0, 28),\n",
    "                               epsilon=1e-12,\n",
    "                               expansion_radius=0.5, expansion_points=50,\n",
    "                               enable_refinement=True):\n",
    "    \n",
    "    results = []\n",
    "    beacon_positions = anchor_point_df[[\"x\",\"y\",\"Mac\"]].set_index(\"Mac\")[[\"x\",\"y\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    for idx, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "        # ---------------------------\n",
    "        # Extract RSSI\n",
    "        # ---------------------------\n",
    "        rssis = row.drop(['Zone_id','Room_name','x','y','tagId','timestamp'], errors='ignore').values.astype(float)\n",
    "        anchor_coords = anchor_point_df[['x','y']].values\n",
    "\n",
    "        # ---------------------------\n",
    "        # MLE Estimation\n",
    "        # ---------------------------\n",
    "        mask_mle = rssis > rssi_threshold\n",
    "        signal_strengths = rssis[mask_mle]\n",
    "        dp_coords = anchor_coords[mask_mle]\n",
    "\n",
    "        if len(signal_strengths) < 1:\n",
    "            signal_strengths = rssis\n",
    "            dp_coords = anchor_coords\n",
    "\n",
    "        strong_mask = signal_strengths > strong_rssi_threshold\n",
    "        if np.sum(strong_mask) < 2:\n",
    "            dp_coords_selected = dp_coords\n",
    "            signal_strengths_selected = signal_strengths\n",
    "        else:\n",
    "            dp_coords_selected = dp_coords[strong_mask]\n",
    "            signal_strengths_selected = signal_strengths[strong_mask]\n",
    "\n",
    "        min_rssi, max_rssi = np.min(signal_strengths_selected), np.max(signal_strengths_selected)\n",
    "        anchor_weights = (signal_strengths_selected - min_rssi + 1) / (max_rssi - min_rssi + 1e-5)\n",
    "        sorted_idx = np.argsort(-signal_strengths_selected)\n",
    "        top_k = min(top_k_anchors, len(sorted_idx))\n",
    "        top_coords = dp_coords_selected[sorted_idx[:top_k]]\n",
    "\n",
    "        x_min, y_min = np.min(top_coords, axis=0)\n",
    "        x_max, y_max = np.max(top_coords, axis=0)\n",
    "        x_min = max(x_min - roi_margin, map_x_bounds[0])\n",
    "        x_max = min(x_max + roi_margin, map_x_bounds[1])\n",
    "        y_min = max(y_min - roi_margin, map_y_bounds[0])\n",
    "        y_max = min(y_max + roi_margin, map_y_bounds[1])\n",
    "\n",
    "        coarse_grid = np.stack(np.meshgrid(np.arange(x_min, x_max, 1/coarse_res),\n",
    "                                           np.arange(y_min, y_max, 1/coarse_res)), axis=-1).reshape(-1,2)\n",
    "\n",
    "        strongest_coord = top_coords[0]\n",
    "        dists_for_fit = euclidean_dist(strongest_coord, dp_coords_selected)\n",
    "        T_global, n_global = find_mle_params(signal_strengths_selected, dists_for_fit)\n",
    "\n",
    "        coarse_likelihoods = compute_likelihood_weighted(\n",
    "            coarse_grid, dp_coords_selected, signal_strengths_selected,\n",
    "            T_global, n_global, sigma_noise, anchor_weights\n",
    "        )\n",
    "\n",
    "        top_indices = np.argpartition(coarse_likelihoods, -top_coarse_points)[-top_coarse_points:]\n",
    "        top_candidates = coarse_grid[top_indices]\n",
    "\n",
    "        fine_candidates, fine_likelihoods = [], []\n",
    "        for center in top_candidates:\n",
    "            fine_grid = generate_grid(center, resolution=fine_res, radius=fine_radius)\n",
    "            likelihoods_fine = compute_likelihood_weighted(\n",
    "                fine_grid, dp_coords_selected, signal_strengths_selected,\n",
    "                T_global, n_global, sigma_noise, anchor_weights\n",
    "            )\n",
    "            fine_candidates.append(fine_grid)\n",
    "            fine_likelihoods.append(likelihoods_fine)\n",
    "\n",
    "        fine_candidates = np.vstack(fine_candidates)\n",
    "        fine_likelihoods = np.hstack(fine_likelihoods)\n",
    "        fine_likelihoods += epsilon\n",
    "        fine_likelihoods /= np.sum(fine_likelihoods)\n",
    "\n",
    "        N = max(1, min(100, int(topN_ratio * len(fine_candidates))))\n",
    "        top_idx = np.argpartition(fine_likelihoods, -N)[-N:]\n",
    "        top_points = fine_candidates[top_idx]\n",
    "        top_weights = fine_likelihoods[top_idx]\n",
    "        top_weights /= np.sum(top_weights)\n",
    "        pred_mle = np.average(top_points, axis=0, weights=top_weights)\n",
    "        conf_mle = np.max(top_weights)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optimization around MLE\n",
    "        # ---------------------------\n",
    "        rssi_values_opt = dict(sorted(filter_rssi(row, beacon_positions, rssi_threshold).items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(rssi_values_opt) < 3:\n",
    "            beacon_coords_opt = anchor_coords\n",
    "            distances_opt = np.ones(anchor_coords.shape[0])\n",
    "        else:\n",
    "            beacon_coords_opt = np.array([list(beacon_positions[b].values()) for b in rssi_values_opt.keys()])\n",
    "            distances_opt = np.array([rssi_to_distance(rssi) for rssi in rssi_values_opt.values()])\n",
    "\n",
    "        expansion_area = generate_expansion_area(pred_mle, radius=expansion_radius, num_points=expansion_points)\n",
    "        quick_errors = np.array([localization_error(p, beacon_coords_opt, distances_opt) for p in expansion_area])\n",
    "        filtered_expansion_area = expansion_area[np.argsort(quick_errors)[:10]]  # top few\n",
    "\n",
    "        best_err, best_pos = float(\"inf\"), None\n",
    "        for point in filtered_expansion_area:\n",
    "            res = minimize(localization_error, point, args=(beacon_coords_opt, distances_opt),\n",
    "                           method='L-BFGS-B', options={'maxiter':100})\n",
    "            if res.success:\n",
    "                est_pos = res.x\n",
    "                total_err = np.sum(np.linalg.norm(beacon_coords_opt - est_pos, axis=1))\n",
    "                if total_err < best_err:\n",
    "                    best_err = total_err\n",
    "                    best_pos = est_pos\n",
    "        pred_opt = best_pos\n",
    "        conf_opt = 1 / (1 + best_err)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optional refinement\n",
    "        # ---------------------------\n",
    "        pre_refined_pos = pred_opt.copy()\n",
    "        if enable_refinement:\n",
    "            strong_rssi_indices = [i for i, rssi in enumerate(rssi_values_opt.values()) if rssi > -75]\n",
    "            if len(strong_rssi_indices) >= 3:\n",
    "                filtered_coords = beacon_coords_opt[strong_rssi_indices]\n",
    "                filtered_distances = distances_opt[strong_rssi_indices]\n",
    "                result = minimize(localization_error, pred_opt,\n",
    "                                  args=(filtered_coords, filtered_distances),\n",
    "                                  method='L-BFGS-B',\n",
    "                                  options={'maxiter':100, 'gtol':1e-8, 'disp':False})\n",
    "                if result.success:\n",
    "                    pred_opt = result.x\n",
    "\n",
    "        refinement_shift = np.linalg.norm(pred_opt - pre_refined_pos)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Fused Results\n",
    "        # ---------------------------\n",
    "        alpha_dynamic = conf_mle / (conf_mle + conf_opt)\n",
    "        pred_fused_fixed = 0.5 * pred_mle + 0.5 * pred_opt\n",
    "        pred_fused_dynamic = alpha_dynamic * pred_mle + (1-alpha_dynamic) * pred_opt\n",
    "\n",
    "        results.append({\n",
    "            'original_index': idx,\n",
    "            'Zone_id': row.get('Zone_id', np.nan),\n",
    "            'Room_name': row.get('Room_name', np.nan),\n",
    "            'Tag_id': row.get('tagId', np.nan),\n",
    "            'timestamp': row.get('timestamp', np.nan),\n",
    "            'Predicted_MLE': pred_mle,\n",
    "            'Predicted_Optimisation': pred_opt,\n",
    "            'Predicted_NLOS': pred_fused_fixed,\n",
    "            'Predicted_NLOS_Dynamic': pred_fused_dynamic,\n",
    "            'MLE_Confidence': conf_mle,\n",
    "            'Opt_Confidence': conf_opt,\n",
    "            'Ground_Truth': np.array([row['x'], row['y']])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca56e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename= \"1_data_asset_tag_stay_still_combine_3_4.json\"\n",
    "# filename = \"2_data_asset_walk_around_combine_1_2.json\"\n",
    "filename=\"3_data_duress_edgecase_combine..json\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_df= test_data.copy()\n",
    "# data_set_df=data_set_df.drop(columns=['num_strong_features', \"parent_zone_id\"]).reset_index(drop=True)\n",
    "\n",
    "# data_set_df= df_test.copy()\n",
    "data_set_df=data_set_df.drop(columns=[\"parent_zone_id\"]).reset_index(drop=True)\n",
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131917d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter() \n",
    "\n",
    "result= fused_localization_mle_opt(data_set_df, anchor_point_df)\n",
    "\n",
    "save_folder = \"Result_Duress\"\n",
    "save_name = f\"{filename.replace('.json', '_NLOS.csv')}\" \n",
    "save_path = os.path.join(save_folder, save_name)\n",
    "\n",
    "result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter() \n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_row = total_time / len(data_set_df)\n",
    "print(avg_time_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9609f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result= pd.read_csv(\"Result_Asset/1_data_asset_stationary_used_for_locationAI_too_NLOS.csv\")\n",
    "# result['Predicted_MLE'] = result['Predicted_MLE'].apply(convert_coordinates)\n",
    "# result['Ground_Truth'] = result['Ground_Truth'].apply(convert_coordinates)\n",
    "\n",
    "\n",
    "# result[\"Predicted_Optimisation\"]= result['Predicted_Optimisation'].apply(convert_coordinates)\n",
    "# result[\"Predicted_NLOS\"]= result['Predicted_NLOS'].apply(convert_coordinates)\n",
    "# result[\"Predicted_NLOS_Dynamic\"]= result['Predicted_NLOS_Dynamic'].apply(convert_coordinates)\n",
    "\n",
    "# result.Zone_id= result.Zone_id.astype(str)\n",
    "# result.Tag_id= result.Tag_id.astype(str)\n",
    "# result= result[result.Room_name !='Womens Restroom']\n",
    "# result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3333b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_fused_dynamic(result_df, ground_truth_df, map_file_location,\n",
    "                                 fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "                                 output_file=\"compare_plot_MLE_Optim_Fused.png\"):\n",
    "    \"\"\"\n",
    "    Plot predicted locations from MLE, Optimisation, and fused methods, showing inside-room accuracy.\n",
    "    Computes overall accuracy and returns per-room statistics including inside-point counts and total points.\n",
    "    Also provides room-type aggregated accuracy.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    total_inside = {\"MLE\": 0, \"Optimisation\": 0}\n",
    "    total_inside.update({col: 0 for col in fused_cols})\n",
    "    total_points = 0  # only one total points count\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        # Room polygon\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Helper function to parse coordinates\n",
    "        def parse_coords(col_name):\n",
    "            x_list, y_list = [], []\n",
    "            for coord in room_data[col_name]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_list.append(float(coord[0]))\n",
    "                    y_list.append(float(coord[1]))\n",
    "                except:\n",
    "                    pass\n",
    "            return x_list, y_list\n",
    "\n",
    "        # Predictions\n",
    "        x_mle, y_mle = parse_coords(\"Predicted_MLE\")\n",
    "        x_opt, y_opt = parse_coords(\"Predicted_Optimisation\")\n",
    "        fused_data = {col: parse_coords(col) for col in fused_cols if col in room_data.columns}\n",
    "\n",
    "        # Count points inside polygon\n",
    "        def count_inside(x_list, y_list):\n",
    "            return sum(1 for x, y in zip(x_list, y_list) if Point(x, y).within(polygon))\n",
    "\n",
    "        inside_mle = count_inside(x_mle, y_mle)\n",
    "        inside_opt = count_inside(x_opt, y_opt)\n",
    "        inside_fused = {k: count_inside(*v) for k, v in fused_data.items()}\n",
    "\n",
    "        # Update totals\n",
    "        total_inside[\"MLE\"] += inside_mle\n",
    "        total_inside[\"Optimisation\"] += inside_opt\n",
    "        for k, v in fused_data.items():\n",
    "            total_inside[k] += inside_fused[k]\n",
    "        total_points += len(x_mle)  # same for all methods\n",
    "\n",
    "        # Save per-room results\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"MLE_Accuracy\": inside_mle / max(len(x_mle), 1) * 100,\n",
    "            \"Optimisation_Accuracy\": inside_opt / max(len(x_opt), 1) * 100,\n",
    "            **{f\"{k}_Accuracy\": inside_fused[k] / max(len(fused_data[k][0]), 1) * 100 for k in fused_data},\n",
    "            \"MLE_Inside_Points\": inside_mle,\n",
    "            \"Optimisation_Inside_Points\": inside_opt,\n",
    "            **{f\"{k}_Inside_Points\": inside_fused[k] for k in fused_data},\n",
    "            \"Total_Points\": len(x_mle)\n",
    "        })\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_mle, y_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_opt, y_opt, color='green', s=8, label='Optimisation')\n",
    "        colors = ['orange', 'purple', 'red', 'cyan']\n",
    "        for j, (fcol, (x_f, y_f)) in enumerate(fused_data.items()):\n",
    "            ax.scatter(x_f, y_f, color=colors[j % len(colors)], s=8, label=f\"{fcol}\")\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        \n",
    "        # Build title using percentage accuracy instead of counts\n",
    "        title_str = (\n",
    "            f\"{room_name} - \"\n",
    "            f\"MLE: {inside_mle / max(len(x_mle), 1) * 100:.1f}%, \"\n",
    "            f\"Opt: {inside_opt / max(len(x_opt), 1) * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        for fcol, (x_f, y_f) in fused_data.items():\n",
    "            acc = inside_fused[fcol] / max(len(x_f), 1) * 100\n",
    "            clean_name = fcol.replace(\"Predicted_\", \"\")  # <--- removes the prefix\n",
    "            title_str += f\", {clean_name}: {acc:.1f}%\"\n",
    "\n",
    "\n",
    "            \n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "\n",
    "    # --- Overall accuracy ---\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    for method in total_inside.keys():\n",
    "        overall = total_inside[method] / max(total_points, 1) * 100\n",
    "        print(f\"{method}: {overall:.2f}%\")\n",
    "\n",
    "    # --- Room-type aggregated accuracy ---\n",
    "    room_type_stats = accuracy_df.groupby('Room_Type').agg({\n",
    "        'MLE_Inside_Points': 'sum',\n",
    "        'Optimisation_Inside_Points': 'sum',\n",
    "        **{f\"{col}_Inside_Points\": 'sum' for col in fused_cols},\n",
    "        'Total_Points': 'sum'\n",
    "    })\n",
    "\n",
    "    for method in ['MLE', 'Optimisation'] + fused_cols:\n",
    "        room_type_stats[f\"{method}_Accuracy\"] = room_type_stats[f\"{method}_Inside_Points\"] / room_type_stats['Total_Points'] * 100\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "accuracy_df = plot_predicted_fused_dynamic(\n",
    "    result_df=result,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location= map_file,\n",
    "    fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "#     output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"Predicted_NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df[\"MLE_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df[\"Optimisation_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df[\"Predicted_NLOS_Inside_Points\"].sum() / accuracy_df[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Duress/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.groupby(\"Room_Type\")[\"Room_Type\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55090a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_accuracy_per_room(\n",
    "    accuracy_df,\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Fused\"),\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise numeric accuracies for three models in different colors.\n",
    "    First line: MLE/Optimisation (no spaces)\n",
    "    Second line: Fused\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge with ground truth polygons\n",
    "    merged_df = pd.merge(accuracy_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Polygon coordinates\n",
    "        x_coords = [row.get(f\"x{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"x{i+1}\", None))]\n",
    "        y_coords = [row.get(f\"y{i+1}\", None) for i in range(8) if pd.notnull(row.get(f\"y{i+1}\", None))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        polygon = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Draw polygon outline\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "\n",
    "        # Accuracy values\n",
    "        acc1 = int(row.get(f\"{labels[0]}_Accuracy\", 0))\n",
    "        acc2 = int(row.get(f\"{labels[1]}_Accuracy\", 0))\n",
    "        acc3 = int(row.get(f\"{labels[2]}_Accuracy\", 0))\n",
    "\n",
    "        centroid = polygon.centroid\n",
    "\n",
    "        # Line 1: MLE / Optimisation\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y + 0.15,\n",
    "            f\"{acc1}/{acc2}\",\n",
    "            color=\"black\", fontsize=10, ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "        # Individual colors\n",
    "        ax.text(centroid.x - 0.55, centroid.y + 0.15, f\"{acc1}\", color=colors[0], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "        ax.text(centroid.x, centroid.y + 0.15, \"/\", color=\"black\", fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4)\n",
    "        ax.text(centroid.x + 0.55, centroid.y + 0.15, f\"{acc2}\", color=colors[1], fontsize=10,\n",
    "                ha=\"center\", va=\"center\", zorder=4, fontweight=\"bold\")\n",
    "\n",
    "        # Line 2: Fused\n",
    "        ax.text(\n",
    "            centroid.x, centroid.y - 0.45,\n",
    "            f\"{acc3}\",\n",
    "            color=colors[2], fontsize=10, ha=\"center\", va=\"center\", zorder=3, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    # Automatically scale axes\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0, ignore_index=True).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ✅ Weighted overall accuracies\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "    overall_acc = []\n",
    "    for label in labels:\n",
    "        inside_col = f\"{label}_Inside_Points\"\n",
    "        if inside_col in accuracy_df:\n",
    "            overall = (accuracy_df[inside_col].sum() / total_points) * 100\n",
    "            overall_acc.append(overall)\n",
    "        else:\n",
    "            overall_acc.append(0)\n",
    "\n",
    "    overall_text = \" | \".join([f\"{label}: {val:.1f}%\" for label, val in zip(labels, overall_acc)])\n",
    "    ax.set_title(f\"{title_text}\\nOverall Accuracy: {overall_text}\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [Patch(color=color, label=label) for color, label in zip(colors, labels)]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room(\n",
    "    accuracy_df=accuracy_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location=map_file,\n",
    "    colors=(\"green\", \"blue\", \"purple\"),\n",
    "    labels=(\"MLE\", \"Optimisation\", \"Predicted_NLOS\"),\n",
    "    title_text=\"Asset_Stationary Tag_Nov 19_Single Data Packet\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8e9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93861d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def plot_accuracy_per_room_combined(\n",
    "    accuracy_df,\n",
    "    ground_truth_df,\n",
    "    result_d,\n",
    "    map_file_location,\n",
    "    fused_color=\"purple\",\n",
    "    result_d_color=\"blue\",\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot room-wise accuracy:\n",
    "      - Single line per room: LocationAI / NLOS (colored)\n",
    "      - Title shows overall weighted accuracy for both\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge accuracy with polygons\n",
    "    merged_df = pd.merge(\n",
    "        accuracy_df,\n",
    "        ground_truth_df,\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge result_d (LocationAI)\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_resultD\")\n",
    "    )\n",
    "\n",
    "    # Load map image\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    # ----- Draw polygons and text -----\n",
    "    for _, row in merged_df.iterrows():\n",
    "\n",
    "        x_coords = [row.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        poly = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not poly.is_valid:\n",
    "            poly = poly.buffer(0)\n",
    "\n",
    "        # Draw polygon outline\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "\n",
    "        centroid = poly.centroid\n",
    "\n",
    "        # Get accuracies\n",
    "        loc_acc = int(row.get(\"Accuracy\", 0))                     # LocationAI (result_d)\n",
    "        fused_acc = int(row.get(\"Predicted_NLOS_Accuracy\", 0))    # Fused / NLOS\n",
    "\n",
    "        # Draw colored numbers side by side\n",
    "        ax.text(\n",
    "            centroid.x - 0.5, centroid.y, f\"{loc_acc}/ \",\n",
    "            color=result_d_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "        ax.text(\n",
    "            centroid.x + 0.6, centroid.y, f\" {fused_acc}\",\n",
    "            color=fused_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "\n",
    "    # ----- Scale axes automatically -----\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ----- Overall weighted accuracies -----\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "\n",
    "    # Overall NLOS (fused)\n",
    "    overall_fused = (accuracy_df[\"Predicted_NLOS_Accuracy\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "\n",
    "    # Overall LocationAI (result_d), weighted by Total_Points from accuracy_df\n",
    "    merged_for_overall = pd.merge(\n",
    "        accuracy_df[[\"Room_name\", \"Total_Points\"]],\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\", how=\"left\"\n",
    "    )\n",
    "    overall_locai = (merged_for_overall[\"Accuracy\"] * merged_for_overall[\"Total_Points\"]).sum() / total_points\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{title_text}\\nOverall LocationAI: {overall_locai:.1f}% | Overall NLOS: {overall_fused:.1f}%\",\n",
    "        fontsize=15, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # ----- Legend -----\n",
    "    legend_handles = [\n",
    "        Patch(color=result_d_color, label=\"LocationAI\"),\n",
    "        Patch(color=fused_color, label=\"NLOS\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_rf\n",
    "result_d.shape, result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room_combined(\n",
    "    accuracy_df=accuracy_df,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    result_d= result_rf_1, \n",
    "    map_file_location=map_file,\n",
    "\n",
    "    title_text=\"Asset_Stationary Tag_Nov 19_Single Data Packet\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fda95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0358f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64eb5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6968c9c9",
   "metadata": {},
   "source": [
    "## C. Using 5 dp\n",
    "Using 5 dp and apply the Centroid for location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def safe_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        return literal_eval(x)\n",
    "    return x\n",
    "\n",
    "def compute_centroid(points):\n",
    "    xs, ys = zip(*points)\n",
    "    return [np.mean(xs), np.mean(ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_by_window(result, window_sizes=range(1, 11)):\n",
    "    result = result.copy()\n",
    "\n",
    "    # Safely evaluate string lists\n",
    "    result['Predicted_MLE'] = result['Predicted_MLE'].apply(safe_eval)\n",
    "    result['Ground_Truth'] = result['Ground_Truth'].apply(safe_eval)\n",
    "\n",
    "    # Handle naming differences\n",
    "    if 'Predicted_Optimisation' in result.columns:\n",
    "        optimisation_col = 'Predicted_Optimisation'\n",
    "    elif 'Predicted_Opt' in result.columns:\n",
    "        optimisation_col = 'Predicted_Opt'\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Predicted_Optimisation' nor 'Predicted_Opt' found in DataFrame\")\n",
    "\n",
    "    result[optimisation_col] = result[optimisation_col].apply(safe_eval)\n",
    "\n",
    "    # Optional fused predictions\n",
    "    has_fused = 'Predicted_NLOS' in result.columns\n",
    "    if has_fused:\n",
    "        result['Predicted_NLOS'] = result['Predicted_NLOS'].apply(safe_eval)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    group_cols = ['Zone_id', 'Room_name', 'Tag_id']\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        for group_keys, group in result.groupby(group_cols):\n",
    "            group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "            n = len(group)\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                # ----------- CORRECTED WINDOW LOGIC ---------------\n",
    "                if i < window_size:\n",
    "                    # BEGINNING: grow window\n",
    "                    start = 0\n",
    "                    end = i + 1\n",
    "                else:\n",
    "                    # SLIDING WINDOW: always full windows\n",
    "                    start = i - window_size + 1\n",
    "                    end = i + 1\n",
    "                # --------------------------------------------------\n",
    "\n",
    "                window = group.iloc[start:end]\n",
    "\n",
    "                # Extract points\n",
    "                mle_points = list(window['Predicted_MLE'])\n",
    "                optimisation_points = list(window[optimisation_col])\n",
    "                ground_truth_points = list(window['Ground_Truth'])\n",
    "\n",
    "                mle_centroid = compute_centroid(mle_points)\n",
    "                optimisation_centroid = compute_centroid(optimisation_points)\n",
    "                ground_truth_centroid = compute_centroid(ground_truth_points)\n",
    "\n",
    "                result_row = {\n",
    "                    'Zone_id': group_keys[0],\n",
    "                    'Room_name': group_keys[1],\n",
    "                    'tagId': group_keys[2],\n",
    "                    'Window_Size': window_size,\n",
    "                    'Predicted_MLE': mle_centroid,\n",
    "                    'Predicted_Optimisation': optimisation_centroid,\n",
    "                    'Ground_Truth': ground_truth_centroid\n",
    "                }\n",
    "\n",
    "                if has_fused:\n",
    "                    fused_points = list(window['Predicted_NLOS'])\n",
    "                    fused_centroid = compute_centroid(fused_points)\n",
    "                    result_row['Predicted_NLOS'] = fused_centroid\n",
    "\n",
    "                all_results.append(result_row)\n",
    "\n",
    "    centroid_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    return centroid_df[centroid_df['Room_name'] != 'Womens Restroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30293573",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_result = compute_centroids_by_window(result, window_sizes=range(5, 6))\n",
    "centroid_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df_centroid_5 = plot_predicted_all(centroid_result[centroid_result.Window_Size==5], \\\n",
    "                                            ground_truth_df, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd81b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Room_Type and compute weighted (point-based) accuracy\n",
    "weighted_grouped = (\n",
    "    accuracy_df_centroid_5\n",
    "    .groupby(\"Room_Type\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "        \"NLOS_Accuracy\": (g[\"NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy (also weighted)\n",
    "overall = pd.DataFrame([{\n",
    "    \"MLE_Accuracy\": (accuracy_df_centroid_5[\"MLE_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100,\n",
    "    \"Optimisation_Accuracy\": (accuracy_df_centroid_5[\"Optimisation_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100,\n",
    "    \"NLOS_Accuracy\": (accuracy_df_centroid_5[\"NLOS_Inside_Points\"].sum() / accuracy_df_centroid_5[\"Total_Points\"].sum()) * 100\n",
    "}], index=[\"Overall\"])\n",
    "\n",
    "# Combine results\n",
    "summary_df = pd.concat([overall, weighted_grouped]).rename(index={\"Open\": \"Open Space\"})\n",
    "summary_df.to_csv(\"Result_Duress/temp_result.csv\", index= False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window(result, ground_truth_df, map_file_location, window_sizes=range(1, 11)):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    # 1️⃣ Compute centroids for all window sizes\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=window_sizes)\n",
    "\n",
    "    # Add Room_Type info\n",
    "    centroid_df = pd.merge(\n",
    "        centroid_df,\n",
    "        ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w].copy()\n",
    "\n",
    "        # Compute room-level accuracy using plot_predicted_all\n",
    "        acc_df = plot_predicted_all(\n",
    "            result=df_w,\n",
    "            ground_truth_df=ground_truth_df,\n",
    "            map_file_location=map_file_location,\n",
    "        )\n",
    "\n",
    "        # 2️⃣ Weighted overall accuracy\n",
    "        mle_overall = (acc_df[\"MLE_Inside_Points\"].sum() / acc_df[\"Total_Points\"].sum()) * 100\n",
    "        opt_overall = (acc_df[\"Optimisation_Inside_Points\"].sum() / acc_df[\"Total_Points\"].sum()) * 100\n",
    "        fused_overall = (acc_df[\"NLOS_Inside_Points\"].sum() / acc_df[\"Total_Points\"].sum()) * 100\n",
    "\n",
    "        # 3️⃣ Weighted room-level accuracy\n",
    "        grouped = acc_df.groupby(\"Room_Type\").apply(\n",
    "            lambda g: pd.Series({\n",
    "                \"MLE_Accuracy\": (g[\"MLE_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "                \"Optimisation_Accuracy\": (g[\"Optimisation_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100,\n",
    "                \"NLOS_Accuracy\": (g[\"NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "            })\n",
    "        )\n",
    "\n",
    "        # 4️⃣ Build summary row\n",
    "        row = {\n",
    "            \"Window_Size\": w,\n",
    "            \"MLE_Overall\": mle_overall,\n",
    "            \"Optimisation_Overall\": opt_overall,\n",
    "            \"NLOS_Overall\": fused_overall,\n",
    "        }\n",
    "\n",
    "        # Add room-level values\n",
    "        for room_type in grouped.index:\n",
    "            row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "            row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "            row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "        accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_all(result, ground_truth_df, map_file_location, output_file=\"compare_plot_MLE_NLOS.png\"):\n",
    "    \"\"\"\n",
    "    Plot predicted centroids (MLE, Optimisation, NLOS) vs room polygons\n",
    "    and calculate weighted accuracies per room and overall.\n",
    "\n",
    "    Returns:\n",
    "        accuracy_df: DataFrame with per-room accuracy and inside point counts\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    total_points_mle = total_points_Optimisation = total_points_fuse = 0\n",
    "    total_inside_mle = total_inside_Optimisation = total_inside_fuse = 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Ensure Room_Type exists\n",
    "    # -----------------------------\n",
    "    if 'Room_Type' not in result.columns:\n",
    "        result = pd.merge(\n",
    "            result,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "    result['Room_Type'] = result['Room_Type'].fillna('Unknown')\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        result,\n",
    "        ground_truth_df[['Zone_id', 'Room_name'] + [c for c in ground_truth_df.columns if c.startswith('x') or c.startswith('y')]].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        has_fused = 'Predicted_NLOS' in room_data.columns\n",
    "\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        # Get room polygon coordinates\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Parse predictions safely\n",
    "        # -----------------------------\n",
    "        def parse_coords(series):\n",
    "            x_list, y_list = [], []\n",
    "            for coord in series:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_list.append(float(coord[0]))\n",
    "                    y_list.append(float(coord[1]))\n",
    "                except:\n",
    "                    continue\n",
    "            return x_list, y_list\n",
    "\n",
    "        x_pred_mle, y_pred_mle = parse_coords(room_data[\"Predicted_MLE\"])\n",
    "        x_pred_opt, y_pred_opt = parse_coords(room_data[\"Predicted_Optimisation\"])\n",
    "        x_pred_fuse, y_pred_fuse = ([], [])\n",
    "        if has_fused:\n",
    "            x_pred_fuse, y_pred_fuse = parse_coords(room_data[\"Predicted_NLOS\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # Count points inside polygon\n",
    "        # -----------------------------\n",
    "        inside_count_mle = sum(1 for x, y in zip(x_pred_mle, y_pred_mle) if Point(x, y).within(polygon))\n",
    "        inside_count_opt = sum(1 for x, y in zip(x_pred_opt, y_pred_opt) if Point(x, y).within(polygon))\n",
    "        inside_count_fuse = sum(1 for x, y in zip(x_pred_fuse, y_pred_fuse) if Point(x, y).within(polygon)) if has_fused else 0\n",
    "\n",
    "        total_points_mle += len(x_pred_mle)\n",
    "        total_inside_mle += inside_count_mle\n",
    "        total_points_Optimisation += len(x_pred_opt)\n",
    "        total_inside_Optimisation += inside_count_opt\n",
    "        if has_fused:\n",
    "            total_points_fuse += len(x_pred_fuse)\n",
    "            total_inside_fuse += inside_count_fuse\n",
    "\n",
    "        # -----------------------------\n",
    "        # Percentage inside\n",
    "        # -----------------------------\n",
    "        percentage_mle = (inside_count_mle / len(x_pred_mle)) * 100 if x_pred_mle else 0\n",
    "        percentage_opt = (inside_count_opt / len(x_pred_opt)) * 100 if x_pred_opt else 0\n",
    "        percentage_fuse = (inside_count_fuse / len(x_pred_fuse)) * 100 if has_fused and x_pred_fuse else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"MLE_Accuracy\": percentage_mle,\n",
    "            \"Optimisation_Accuracy\": percentage_opt,\n",
    "            \"NLOS_Accuracy\": percentage_fuse if has_fused else None,\n",
    "            \"MLE_Inside_Points\": inside_count_mle,\n",
    "            \"Optimisation_Inside_Points\": inside_count_opt,\n",
    "            \"NLOS_Inside_Points\": inside_count_fuse if has_fused else None,\n",
    "            \"Total_Points\": len(x_pred_mle),\n",
    "        })\n",
    "\n",
    "        # -----------------------------\n",
    "        # Plot\n",
    "        # -----------------------------\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_pred_mle, y_pred_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_pred_opt, y_pred_opt, color='green', s=8, label='Optimisation')\n",
    "        if has_fused:\n",
    "            ax.scatter(x_pred_fuse, y_pred_fuse, color='red', s=8, label='NLOS')\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        title_str = f\"{room_name} - MLE: {percentage_mle:.1f}%, Opt: {percentage_opt:.1f}%\"\n",
    "        if has_fused:\n",
    "            title_str += f\", NLOS: {percentage_fuse:.1f}%\"\n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0,0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    overall_mle_accuracy = (total_inside_mle / total_points_mle) * 100 if total_points_mle else 0\n",
    "    overall_opt_accuracy = (total_inside_Optimisation / total_points_Optimisation) * 100 if total_points_Optimisation else 0\n",
    "    overall_fuse_accuracy = (total_inside_fuse / total_points_fuse) * 100 if total_points_fuse else 0\n",
    "\n",
    "    print(f\"\\nOverall MLE Accuracy: {overall_mle_accuracy:.2f}%\")\n",
    "    print(f\"Overall Optimisation Accuracy: {overall_opt_accuracy:.2f}%\")\n",
    "    if total_points_fuse > 0:\n",
    "        print(f\"Overall NLOS Accuracy: {overall_fuse_accuracy:.2f}%\")\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window = compute_accuracy_by_window(result, ground_truth_df, map_file)\n",
    "\n",
    "# accuracy_vs_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0e0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(3, 4))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(\n",
    "        centroid_df,\n",
    "        ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        acc_df = plot_predicted_all(\n",
    "            result=df_w,\n",
    "            ground_truth_df=ground_truth_df,\n",
    "            map_file_location=map_file_location,\n",
    "        )\n",
    "\n",
    "        # Overall\n",
    "        mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "        opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "        fused_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "        grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\",\n",
    "                                               \"Optimisation_Accuracy\",\n",
    "                                               \"NLOS_Accuracy\"]].mean()\n",
    "\n",
    "        row = {\n",
    "            \"Window_Size\": w,\n",
    "            \"MLE_Overall\": mle_overall,\n",
    "            \"Optimisation_Overall\": opt_overall,\n",
    "            \"NLOS_Overall\": fused_overall,\n",
    "        }\n",
    "\n",
    "        # One column per room type\n",
    "        for room_type in grouped.index:\n",
    "            row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "        accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# NEW FUNCTION:\n",
    "# ONE PLOT ONLY:\n",
    "# - Plot NLOS_Overall as a line\n",
    "# - Plot each room’s NLOS value at each window size as scatter points\n",
    "# - Label each scatter point with the accuracy value\n",
    "# =====================================================================\n",
    "def plot_nlos_overall_with_room_centroids(acc_summary_df):\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    # 1) Plot NLOS Overall (single curve)\n",
    "    plt.plot(\n",
    "        acc_summary_df[\"Window_Size\"],\n",
    "        acc_summary_df[\"NLOS_Overall\"],\n",
    "        linewidth=3,\n",
    "        marker=\"o\",\n",
    "        label=\"NLOS Overall\"\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 2) Extract each room’s NLOS accuracy columns\n",
    "    # ----------------------------------------------------------------\n",
    "    room_cols = [\n",
    "        c for c in acc_summary_df.columns\n",
    "        if c.startswith(\"NLOS_\") and c not in [\"NLOS_Overall\"]\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3) Scatter each room’s accuracy at each window size\n",
    "    # ----------------------------------------------------------------\n",
    "    for col in room_cols:\n",
    "        room_name = col.replace(\"NLOS_\", \"\")\n",
    "        y_vals = acc_summary_df[col]\n",
    "\n",
    "        # Scatter plot per room\n",
    "        plt.scatter(acc_summary_df[\"Window_Size\"], y_vals, s=60, label=f\"{room_name}\")\n",
    "\n",
    "        # Add numeric labels on each scatter point\n",
    "        for x, y in zip(acc_summary_df[\"Window_Size\"], y_vals):\n",
    "            plt.text(x, y, f\"{y:.2f}\", fontsize=9, ha='left', va='bottom')\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Final plot formatting\n",
    "    # ----------------------------------------------------------------\n",
    "    plt.title(\"NLOS Accuracy by Window Size (Overall & Room Centroids)\", fontsize=16)\n",
    "    plt.xlabel(\"Window Size\", fontsize=14)\n",
    "    plt.ylabel(\"NLOS Accuracy\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window[accuracy_vs_window.Window_Size==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smaller marker size for NLOS curves\n",
    "nlos_marker_size = 3\n",
    "\n",
    "# Plot curves (only label once)\n",
    "plt.plot(accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[\"NLOS_Room\"], marker='o', markersize=nlos_marker_size, label='NLOS_Room')\n",
    "plt.plot(accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[\"NLOS_Open\"], marker='o', markersize=nlos_marker_size, label='NLOS_Open')\n",
    "\n",
    "# Vertical reference lines\n",
    "plt.axvline(x=1, color='b', linestyle='--')\n",
    "plt.axvline(x=5, color='red', linestyle='--')\n",
    "\n",
    "# Annotate NLOS values at x=1 and x=5 (no legend for these markers)\n",
    "for col, x_val in [(\"NLOS_Room\", 1), (\"NLOS_Room\", 5), (\"NLOS_Open\", 1), (\"NLOS_Open\", 5)]:\n",
    "    y_val = np.interp(x_val, accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[col])\n",
    "    plt.scatter(x_val, y_val, color='black')\n",
    "    plt.text(x_val + 0.1, y_val, f\"{y_val:.1f}%\", va='top', fontsize=9)\n",
    "\n",
    "# -------------------------------\n",
    "# Plot and annotate LocAI points\n",
    "# -------------------------------\n",
    "# At x=5\n",
    "plt.scatter(5, result_d_room[\"Room_Accuracy\"].values[0], color='purple', marker='D', s=30, label='LocAI_Room')\n",
    "plt.text(5.1, result_d_room[\"Room_Accuracy\"].values[0], f\"{result_d_room['Room_Accuracy'].values[0]:.1f}%\", \n",
    "         va='bottom', fontsize=9, color='purple')\n",
    "\n",
    "plt.scatter(5, result_d_room[\"Open_Accuracy\"].values[0], color='green', marker='D', s=30, label='LocAI_Open')\n",
    "plt.text(5.1, result_d_room[\"Open_Accuracy\"].values[0], f\"{result_d_room['Open_Accuracy'].values[0]:.1f}%\", \n",
    "         va='bottom', fontsize=9, color='green')\n",
    "\n",
    "# At x=1 (no extra legend)\n",
    "plt.scatter(1, result_d_room1[\"Room_Accuracy\"].values[0], color='purple', marker='D', s=30, label='_nolegend_')\n",
    "plt.text(1.1, result_d_room1[\"Room_Accuracy\"].values[0], f\"{result_d_room1['Room_Accuracy'].values[0]:.1f}%\", \n",
    "         va='bottom', fontsize=9, color='purple')\n",
    "\n",
    "plt.scatter(1, result_d_room1[\"Open_Accuracy\"].values[0], color='green', marker='D', s=30, label='_nolegend_')\n",
    "plt.text(1.1, result_d_room1[\"Open_Accuracy\"].values[0], f\"{result_d_room1['Open_Accuracy'].values[0]:.1f}%\", \n",
    "         va='bottom', fontsize=9, color='green')\n",
    "\n",
    "# -------------------------------\n",
    "# Final formatting\n",
    "# -------------------------------\n",
    "plt.title('Duress NLOS Accuracy vs. # Data Packets')\n",
    "plt.xlabel('# Data Packets')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(70, 100)\n",
    "plt.grid(True)\n",
    "plt.legend(ncol=2, loc='upper center', bbox_to_anchor=(0.17, 1.01))\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd69fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d0d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb422cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Correct column mapping for each method + metric\n",
    "column_map = {\n",
    "    \"MLE\": {\n",
    "        \"Room\": \"MLE_Room\",\n",
    "        \"Open\": \"MLE_Open\",\n",
    "        \"Overall\": \"MLE_Overall\"\n",
    "    },\n",
    "    \"Optimisation\": {\n",
    "        \"Room\": \"Optimisation_Room\",\n",
    "        \"Open\": \"Optimisation_Open\",\n",
    "        \"Overall\": \"Optimisation_Overall\"\n",
    "    },\n",
    "    \"NLOS\": {\n",
    "        \"Room\": \"NLOS_Room\",\n",
    "        \"Open\": \"NLOS_Open\",\n",
    "        \"Overall\": \"NLOS_Overall\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Style settings\n",
    "methods = {\n",
    "    'MLE': 'blue',\n",
    "    'Optimisation': 'green',\n",
    "    'NLOS': 'red'\n",
    "}\n",
    "\n",
    "metrics_style = {\n",
    "    'Room': '-',     # solid\n",
    "    'Open': ':',     # dotted\n",
    "    'Overall': '-.'  # dash-dot\n",
    "}\n",
    "\n",
    "# ---- Plot 3 curves per method ----\n",
    "for method, color in methods.items():\n",
    "    for metric, linestyle in metrics_style.items():\n",
    "        col = column_map[method][metric]\n",
    "        \n",
    "        plt.plot(\n",
    "            accuracy_vs_window[\"Window_Size\"],\n",
    "            accuracy_vs_window[col],\n",
    "            linestyle=linestyle,\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            label=f\"{method} {metric}\"\n",
    "        )\n",
    "\n",
    "# ---- Vertical line at x = 5 ----\n",
    "plt.axvline(x=3, color='black', linestyle='--', label='Scan = 3')\n",
    "\n",
    "# ---- Annotate intersections ----\n",
    "for method, color in methods.items():\n",
    "    for metric in metrics_style.keys():\n",
    "        col = column_map[method][metric]\n",
    "        \n",
    "        # interpolated value at x = 5\n",
    "        y_val = np.interp(3, accuracy_vs_window[\"Window_Size\"], accuracy_vs_window[col])\n",
    "        \n",
    "        plt.scatter(3, y_val, color=color)\n",
    "        plt.text(3.15, y_val, f\"{y_val:.1f}%\", va='center', color=color, fontsize=9)\n",
    "\n",
    "plt.title('Stationary Tags — Asset Accuracy vs. # Data Packets')\n",
    "plt.xlabel('# Data Packets')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(70, 95)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Legend handles for line styles (Room / Open / Overall)\n",
    "room_line = mlines.Line2D([], [], color='black', linestyle='-', label='Room (solid)')\n",
    "open_line = mlines.Line2D([], [], color='black', linestyle=':', label='Open Space (dotted)')\n",
    "overall_line = mlines.Line2D([], [], color='black', linestyle='-.', label='Overall (dash-dot)')\n",
    "\n",
    "# Legend handles for colors (MLE / Optimisation / Fused)\n",
    "mle_line = mlines.Line2D([], [], color='blue', linestyle='-', label='MLE')\n",
    "opt_line = mlines.Line2D([], [], color='green', linestyle='-', label='Optimisation')\n",
    "fused_line = mlines.Line2D([], [], color='red', linestyle='-', label='NLOS')\n",
    "\n",
    "plt.legend(\n",
    "    handles=[room_line, open_line, overall_line, mle_line, opt_line, fused_line],\n",
    "    loc='lower right',\n",
    "    fontsize=10,\n",
    "    title=\"\"\n",
    ")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_Fused_asset_accuracy_vs_packets.png\", dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e35426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def plot_accuracy_per_room(\n",
    "    accuracy_df,       # combined 5-row dataset\n",
    "    result_d,           # LocationAI_Voting 5\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    colors=(\"red\"),  # accuracy_df, accuracy_df1 colors\n",
    "    result_d_color=\"blue\",\n",
    "    title_text=\"Room-wise Accuracy\",\n",
    "    output_file=None\n",
    "):\n",
    "\n",
    "    # Merge accuracy_df with ground truth polygons\n",
    "    merged_df = pd.merge(\n",
    "        accuracy_df,\n",
    "        ground_truth_df,\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge result_d (LocationAI)\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "        on=\"Room_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Load map\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', zorder=0)\n",
    "\n",
    "    # ----- Draw polygons and text -----\n",
    "    for _, row in merged_df.iterrows():\n",
    "        x_coords = [row.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row.get(f\"x{i+1}\"))]\n",
    "        y_coords = [row.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row.get(f\"y{i+1}\"))]\n",
    "        if not x_coords or not y_coords:\n",
    "            continue\n",
    "\n",
    "        poly = Polygon(list(zip(x_coords, y_coords)))\n",
    "        if not poly.is_valid:\n",
    "            poly = poly.buffer(0)\n",
    "\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'k-', lw=1, zorder=2)\n",
    "        centroid = poly.centroid\n",
    "\n",
    "        # Extract accuracies\n",
    "        loc_acc = int(row.get(\"Accuracy\", 0))                     # LocationAI\n",
    "        fused_acc = int(row.get(\"Predicted_NLOS_Accuracy\", 0))    # NLOS \n",
    "\n",
    "        # Line 1: LocationAI / NLOS\n",
    "        ax.text(\n",
    "            centroid.x - 0.5, centroid.y, f\"{loc_acc}/\",\n",
    "            color=result_d_color, fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "        ax.text(\n",
    "            centroid.x + 0.8, centroid.y, f\" {fused_acc}\",\n",
    "            color=colors[0], fontsize=11,\n",
    "            ha=\"center\", va=\"center\", fontweight=\"bold\", zorder=4\n",
    "        )\n",
    "\n",
    "       \n",
    "    # ----- Scale axes -----\n",
    "    all_x = pd.concat([ground_truth_df[f\"x{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    all_y = pd.concat([ground_truth_df[f\"y{i+1}\"] for i in range(8)], axis=0).dropna()\n",
    "    ax.set_xlim([all_x.min() - 1, all_x.max() + 1])\n",
    "    ax.set_ylim([all_y.min() - 1, all_y.max() + 1])\n",
    "\n",
    "    # ----- Overall weighted accuracies -----\n",
    "    total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "\n",
    "    overall_locai = (\n",
    "        pd.merge(accuracy_df[[\"Room_name\", \"Total_Points\"]],\n",
    "                 result_d[[\"Room_name\", \"Accuracy\"]],\n",
    "                 on=\"Room_name\")\n",
    "        .eval(\"Accuracy * Total_Points\").sum()\n",
    "    ) / total_points\n",
    "\n",
    "    overall_fused = (accuracy_df[\"Predicted_NLOS_Accuracy\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{title_text}_LocationAI: {overall_locai:.1f}% | \"\n",
    "        f\"NLOS: {overall_fused:.1f}%\" ,\n",
    "        fontsize=15, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # ----- Legend -----\n",
    "    legend_handles = [\n",
    "        Patch(color=result_d_color, label=\"LocationAI\"),\n",
    "        Patch(color=colors[0], label=\"NLOS\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a003291",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_result = compute_centroids_by_window(result, window_sizes=range(5, 6))\n",
    "\n",
    "accuracy_df_centroid = compute_accuracy_per_room(centroid_result, ground_truth_df)\n",
    "\n",
    "room_nlos_summary = accuracy_df_centroid.groupby([\"Zone_id\", \"Room_name\"]).apply(\n",
    "    lambda g: pd.Series({\n",
    "        \"Total_Points\": g[\"Total_Points\"].sum(),\n",
    "        \"Predicted_NLOS_Inside_Points\": g[\"Predicted_NLOS_Inside_Points\"].sum(),\n",
    "        \"Predicted_NLOS_Accuracy\": (g[\"Predicted_NLOS_Inside_Points\"].sum() / g[\"Total_Points\"].sum()) * 100\n",
    "    })\n",
    ").reset_index()\n",
    "room_nlos_summary.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_per_room(\n",
    "    accuracy_df=room_nlos_summary,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    result_d= result_rf, \n",
    "    map_file_location=map_file,\n",
    "\n",
    "    title_text=\"Duress_Overall Accuracy\",\n",
    "#     output_file=\"Result_Asset/Plot_data_asset_tag_stay_still_combine_3_4_NLOS_Single Data Packet.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00244ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aac4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_AI= pd.DataFrame(voted_df.groupby([\"Zone_id\", \"Room_name\"]).Accuracy.mean()).reset_index().\\\n",
    "            rename(columns={\"Accuracy\":\"LocAI\"})\n",
    "res_NLOS= pd.DataFrame(accuracy_df_centroid.groupby([\"Zone_id\", \"Room_name\"]).Predicted_NLOS_Accuracy.mean()).reset_index().\\\n",
    "            rename(columns={\"Predicted_NLOS_Accuracy\":\"NLOS\"})\n",
    "\n",
    "res_all = res_AI.merge(res_NLOS, on= [\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "res_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e080e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_zones_NLOS_LocationAI(result_df, ground_truth_df, locai_df, accuracy_df,\n",
    "                                   map_file_location,\n",
    "                                   nlos_cols=['Predicted_NLOS'],\n",
    "                                   output_file=\"combined_all_zones_NLOS_LocationAI.png\"):\n",
    "    \"\"\"\n",
    "    Plot all zones:\n",
    "    - Each zone has a unique color\n",
    "    - NLOS predictions plotted using different markers\n",
    "    - LocationAI predictions plotted as X markers at the centroid of predicted zones\n",
    "    \"\"\"\n",
    "\n",
    "    import ast\n",
    "\n",
    "    # Merge results with ground truth to get polygon coordinates\n",
    "    merged_df = pd.merge(result_df, ground_truth_df,\n",
    "                         on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Background map\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    # Unique color per zone\n",
    "    unique_zones = merged_df['Zone_id'].unique()\n",
    "    zone_colors = {z: plt.cm.tab20(i % 20) for i, z in enumerate(unique_zones)}\n",
    "\n",
    "    # Markers for NLOS predictions\n",
    "    markers = [\"o\", \"s\", \"D\", \"X\", \"P\", \"^\"]\n",
    "    method_markers = {col: markers[i % len(markers)] for i, col in enumerate(nlos_cols)}\n",
    "\n",
    "    # ---------- Helper to parse coordinate list columns ----------\n",
    "    def parse_coords(col, df):\n",
    "        x_list, y_list = [], []\n",
    "        for coord in df[col]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_list.append(float(coord[0]))\n",
    "                y_list.append(float(coord[1]))\n",
    "            except:\n",
    "                pass\n",
    "        return x_list, y_list\n",
    "\n",
    "    # ---------- Compute zone centroids from ground truth polygons ----------\n",
    "    zone_centroids = {}\n",
    "    for zone, g in ground_truth_df.groupby(\"Zone_id\"):\n",
    "\n",
    "        xs = [g.iloc[0][f\"x{i+1}\"] for i in range(8) if pd.notnull(g.iloc[0][f\"x{i+1}\"])]\n",
    "        ys = [g.iloc[0][f\"y{i+1}\"] for i in range(8) if pd.notnull(g.iloc[0][f\"y{i+1}\"])]\n",
    "\n",
    "        if len(xs) >= 3:\n",
    "            poly = Polygon(list(zip(xs, ys)))\n",
    "            if not poly.is_valid:\n",
    "                poly = poly.buffer(0)\n",
    "            zone_centroids[zone] = (poly.centroid.x, poly.centroid.y)\n",
    "\n",
    "    # ================================================================\n",
    "    #  PLOT ALL ZONES\n",
    "    # ================================================================\n",
    "    for zone in unique_zones:\n",
    "\n",
    "        zone_color = zone_colors[zone]\n",
    "        zdata = merged_df[merged_df[\"Zone_id\"] == zone]\n",
    "\n",
    "        # ---- Draw polygon boundary ----\n",
    "        row0 = zdata.iloc[0]\n",
    "        bx = [row0.get(f\"x{i+1}\") for i in range(8) if pd.notnull(row0.get(f\"x{i+1}\"))]\n",
    "        by = [row0.get(f\"y{i+1}\") for i in range(8) if pd.notnull(row0.get(f\"y{i+1}\"))]\n",
    "\n",
    "        if len(bx) >= 3:\n",
    "            ax.plot(bx + [bx[0]], by + [by[0]], color=zone_color, linewidth=3)\n",
    "\n",
    "        # ---- Plot NLOS predictions ----\n",
    "        for col in nlos_cols:\n",
    "            if col in zdata.columns:\n",
    "                x_vals, y_vals = parse_coords(col, zdata)\n",
    "                ax.scatter(x_vals, y_vals, s=1, color=zone_color,\n",
    "                           marker=method_markers[col],\n",
    "                           label=f\"Zone {zone} – {col}\")\n",
    "\n",
    "\n",
    "    if \"Total_Points\" in accuracy_df.columns:\n",
    "        total_points = accuracy_df[\"Total_Points\"].sum()\n",
    "        overall_locai = (accuracy_df[\"LocAI\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "        overall_nlos = (accuracy_df[\"NLOS\"] * accuracy_df[\"Total_Points\"]).sum() / total_points\n",
    "    else:\n",
    "        # Simple average\n",
    "        overall_locai = accuracy_df[\"LocAI\"].mean()\n",
    "        overall_nlos = accuracy_df[\"NLOS\"].mean()\n",
    "\n",
    "    \n",
    "    \n",
    "    # ---------- Plot LocationAI wrong predictions for each true zone ----------\n",
    "    for _, row in locai_df.iterrows():\n",
    "        true_zone = row[\"Zone_id\"]\n",
    "        predicted_zone = row[\"voted_prediction\"]\n",
    "\n",
    "        # Only wrong predictions\n",
    "        if predicted_zone == true_zone:\n",
    "            continue\n",
    "\n",
    "        # Skip if predicted zone centroid is missing\n",
    "        if predicted_zone not in zone_centroids:\n",
    "            continue\n",
    "\n",
    "        # X = centroid of predicted zone\n",
    "        cx, cy = zone_centroids[predicted_zone]\n",
    "\n",
    "        # Color = color of the true zone (not the predicted zone)\n",
    "        color = zone_colors.get(true_zone, \"black\")\n",
    "\n",
    "        ax.scatter(cx, cy, s=100, marker=\"x\",\n",
    "                   color=color,\n",
    "                   linewidth=5,\n",
    "                   label=f\"Wrong Pred: True={true_zone} -> Pred={predicted_zone}\")\n",
    "\n",
    "\n",
    "    # ================================================================\n",
    "    # ---------- Plot zone-wise accuracy ----------\n",
    "    for zone, centroid in zone_centroids.items():\n",
    "        cx, cy = centroid\n",
    "\n",
    "        # Look up accuracy for this zone\n",
    "        row_acc = accuracy_df[accuracy_df[\"Zone_id\"] == zone]\n",
    "\n",
    "        if not row_acc.empty:\n",
    "            locai_acc = row_acc.iloc[0][\"LocAI\"]\n",
    "            nlos_acc = row_acc.iloc[0][\"NLOS\"]\n",
    "\n",
    "            # Plot LocationAI accuracy (black, slightly left)\n",
    "            ax.text(cx - 0.6, cy + 0.5, f\"{locai_acc:.0f}/\",\n",
    "                    color=\"black\", fontsize=10, fontweight=\"bold\",\n",
    "                    ha=\"center\", va=\"center\", zorder=5)\n",
    "\n",
    "            # Plot NLOS accuracy (blue, slightly right)\n",
    "            ax.text(cx + 0.6, cy + 0.5, f\"{nlos_acc:.0f}\",\n",
    "                    color=\"blue\", fontsize=10, fontweight=\"bold\",\n",
    "                    ha=\"center\", va=\"center\", zorder=5)\n",
    "        \n",
    "        \n",
    "    ax.set_xlim([30, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "    ax.set_title(\"Duress_NLOS + LocationAI Predictions\")\n",
    "\n",
    "    # Clean legend (remove duplicates)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    \n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_handles = [\n",
    "        Patch(color=\"black\", label=\"LocationAI_Accuracy\"),\n",
    "        Patch(color=\"blue\", label=\"NLOS_Accuracy\")\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\")\n",
    "    \n",
    "    ax.set_title(\"Duress_ Edge Case\", fontsize=15, fontweight=\"bold\"\n",
    ")\n",
    "#     ax.set_title(\n",
    "#     f\"Duress_\"\n",
    "#     f\"Overall Accuracy: LocAI {overall_locai:.1f}% / NLOS {overall_nlos:.1f}%\",\n",
    "#     fontsize=15, fontweight=\"bold\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_zones_NLOS_LocationAI(centroid_result, ground_truth_df,voted_df, res_all,\n",
    "                               map_file, nlos_cols=['Predicted_NLOS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98a853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f36cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acef637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8d1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc511a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080fa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001a135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file_location):\n",
    "\n",
    "    accuracy_summary = []\n",
    "\n",
    "    centroid_df = compute_centroids_by_window(result, window_sizes=range(1, 11))\n",
    "    \n",
    "    # Add Room_Type to centroid_df by merging\n",
    "    centroid_df = pd.merge(\n",
    "        centroid_df,\n",
    "        ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "        on=['Zone_id', 'Room_name'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for w in sorted(centroid_df['Window_Size'].unique()):\n",
    "        df_w = centroid_df[centroid_df['Window_Size'] == w]\n",
    "\n",
    "        # Patch: ensure Room_Type is present\n",
    "        df_w = pd.merge(\n",
    "            df_w,\n",
    "            ground_truth_df[['Zone_id', 'Room_name', 'Room_Type']].drop_duplicates(),\n",
    "            on=['Zone_id', 'Room_name'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Group by tagId to get per-tag accuracy\n",
    "        for tag_id, df_tag in df_w.groupby(\"tagId\"):\n",
    "\n",
    "            acc_df = plot_predicted_all(\n",
    "                result=df_tag,\n",
    "                ground_truth_df=ground_truth_df,\n",
    "                map_file_location=map_file_location,\n",
    "            )\n",
    "\n",
    "            # Overall accuracy per tag\n",
    "            mle_overall = acc_df['MLE_Accuracy'].mean()\n",
    "            opt_overall = acc_df['Optimisation_Accuracy'].mean()\n",
    "            NLOS_overall = acc_df['NLOS_Accuracy'].mean()\n",
    "\n",
    "            # Room-type aggregated accuracy\n",
    "            grouped = acc_df.groupby(\"Room_Type\")[[\"MLE_Accuracy\", \"Optimisation_Accuracy\", 'NLOS_Accuracy']].mean()\n",
    "\n",
    "            row = {\n",
    "                \"Window_Size\": w,\n",
    "                \"tagId\": tag_id,              # <-- include tagId\n",
    "                \"MLE_Overall\": mle_overall,\n",
    "                \"Optimisation_Overall\": opt_overall,\n",
    "                \"NLOS_Overall\": NLOS_overall,\n",
    "            }\n",
    "\n",
    "            for room_type in grouped.index:\n",
    "                row[f\"MLE_{room_type}\"] = grouped.loc[room_type, \"MLE_Accuracy\"]\n",
    "                row[f\"Optimisation_{room_type}\"] = grouped.loc[room_type, \"Optimisation_Accuracy\"]\n",
    "                row[f\"NLOS_{room_type}\"] = grouped.loc[room_type, \"NLOS_Accuracy\"]\n",
    "\n",
    "            accuracy_summary.append(row)\n",
    "\n",
    "    return pd.DataFrame(accuracy_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window_tag = compute_accuracy_by_window_with_tags(result, ground_truth_df, map_file)\n",
    "\n",
    "accuracy_vs_window_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_by_tag(accuracy_vs_window_tag, col= ['NLOS_Room']):\n",
    "    \"\"\"\n",
    "    Plot accuracy vs. Window_Size for each tag separately.\n",
    "\n",
    "    Parameters:\n",
    "        accuracy_vs_window_tag : pd.DataFrame\n",
    "            Must include columns 'tagId', 'Window_Size', \n",
    "            'NLOS_Overall', 'NLOS_Open', 'NLOS_Room'\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure tagId is string\n",
    "    accuracy_vs_window_tag['tagId'] = accuracy_vs_window_tag['tagId'].astype(str)\n",
    "\n",
    "    numeric_cols = col\n",
    "#     numeric_cols = ['NLOS_Overall', 'NLOS_Open', 'NLOS_Room']\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    markers = ['o', 's', '^']  # for Overall, Open, Room\n",
    "    linestyles = ['-', '--', ':']\n",
    "\n",
    "    for tag in accuracy_vs_window_tag['tagId'].unique():\n",
    "        tag_df = accuracy_vs_window_tag[accuracy_vs_window_tag['tagId'] == tag].sort_values('Window_Size')\n",
    "        \n",
    "        for col, marker, ls in zip(numeric_cols, markers, linestyles):\n",
    "            plt.plot(tag_df['Window_Size'], tag_df[col], marker=marker, linestyle=ls,\n",
    "                     label=f'Tag {tag} {col.replace(\"_\", \" \")}')\n",
    "\n",
    "            # Optional: annotate value at window_size = 5 if exists\n",
    "            if 5 in tag_df['Window_Size'].values:\n",
    "                y_val = tag_df.loc[tag_df['Window_Size'] == 3, col].values[0]\n",
    "                plt.scatter(3, y_val, color='black', marker=marker)\n",
    "                plt.text(3.2, y_val, f\"{y_val:.1f}%\", fontsize=8)\n",
    "\n",
    "    plt.axvline(x=3, color='red', linestyle='--')\n",
    "    plt.xlabel('# Data Packets')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy vs # Data Packets per Tag')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_window_tag.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_by_tag(accuracy_vs_window_tag, col= ['NLOS_Room'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75498001",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.tagId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08802a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rssi_box_each_zone_grid(df, top_n=5):\n",
    "\n",
    "    # Detect RSSI feature columns (start with digit)\n",
    "    rssi_cols = [c for c in df.columns if c[0].isdigit()]\n",
    "    df['tagId'] = df['tagId'].astype(str)\n",
    "\n",
    "    unique_tags = df['tagId'].unique()\n",
    "\n",
    "    # Consistent colors for tags\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_tags))\n",
    "    tag_colors = {tag: cmap(i) for i, tag in enumerate(unique_tags)}\n",
    "\n",
    "    # All zones\n",
    "    zones = sorted(df[\"Room_name\"].unique())\n",
    "    n_zones = len(zones)\n",
    "\n",
    "    # Create grid: 2 plots per row\n",
    "    n_cols = 5\n",
    "    n_rows = math.ceil(n_zones / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, zone_id in enumerate(zones):\n",
    "        ax = axes[idx]\n",
    "        zone_df = df[df[\"Room_name\"] == zone_id]\n",
    "\n",
    "        # Identify top N RSSI features for this zone\n",
    "\n",
    "        mean_rssi = zone_df[rssi_cols].where(zone_df[rssi_cols] != -100).mean()\n",
    "\n",
    "        top_features = mean_rssi.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "\n",
    "        plot_data = []\n",
    "        tag_labels = []\n",
    "\n",
    "        # Collect RSSI values per tag\n",
    "        for tag in unique_tags:\n",
    "            tag_df = zone_df[zone_df[\"tagId\"] == tag]\n",
    "\n",
    "            if tag_df.empty:\n",
    "                values = [np.nan] * top_n\n",
    "            else:\n",
    "                values = tag_df[top_features].values.flatten()\n",
    "\n",
    "            plot_data.append(values)\n",
    "            tag_labels.append(tag)\n",
    "\n",
    "        # ---- Plot for this zone ----\n",
    "        box = ax.boxplot(plot_data, patch_artist=True, labels=tag_labels)\n",
    "\n",
    "        # Color each box by tag\n",
    "        for patch, tag in zip(box['boxes'], tag_labels):\n",
    "            patch.set_facecolor(tag_colors[tag])\n",
    "\n",
    "        ax.set_title(f\"{zone_id}\")\n",
    "        ax.set_ylabel(\"RSSI\")\n",
    "        ax.set_xticklabels(tag_labels, rotation=45, ha='right')\n",
    "        ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Remove empty axes if zones are odd\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Add legend below all plots\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color=tag_colors[tag], lw=8, label=f\"Tag {tag}\")\n",
    "        for tag in unique_tags\n",
    "    ]\n",
    "\n",
    "    fig.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Tag ID\",\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, 0.00),\n",
    "        ncol=5\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.07, 1, 1])  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rssi_box_each_zone_grid(data_set_df, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45564059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830a992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1WPcT_OjSaguNAQ9AnkpmmUV_GDiijIq5",
     "timestamp": 1717696268126
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa6ad618d5445e7a2343db292d10e8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a8df1c1c1f1460695b2fdd40ab5d774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3887145dd10742d2aaec48b48fbf209f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841ae5e6f05644f4a06df98a4b3d3e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "868e78ed1be844ff89c007db86f1c77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8810a66e33784737a04f2d4de120a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a78544536d5d4e87b691714c15e379a5",
       "IPY_MODEL_ece131d2b91b40e28153ed88cb397481",
       "IPY_MODEL_919d61916e484f10a410b26b643574bf"
      ],
      "layout": "IPY_MODEL_3887145dd10742d2aaec48b48fbf209f"
     }
    },
    "89cc3e4005b640cab78311861d917f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919d61916e484f10a410b26b643574bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841ae5e6f05644f4a06df98a4b3d3e56",
      "placeholder": "​",
      "style": "IPY_MODEL_0aa6ad618d5445e7a2343db292d10e8b",
      "value": " 0/100 [00:00&lt;?, ?it/s]"
     }
    },
    "a78544536d5d4e87b691714c15e379a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89cc3e4005b640cab78311861d917f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_e5079de1d27140eb9f6c4fabe6f199f5",
      "value": "  0%"
     }
    },
    "e5079de1d27140eb9f6c4fabe6f199f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece131d2b91b40e28153ed88cb397481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a8df1c1c1f1460695b2fdd40ab5d774",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_868e78ed1be844ff89c007db86f1c77e",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
