{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from typing import Type, List, Dict, Tuple, Set\n",
    "import argparse\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.externals.joblib import parallel_backend, Parallel, delayed\n",
    "except ImportError:\n",
    "    import joblib\n",
    "    from joblib import parallel_backend, Parallel, delayed\n",
    "    \n",
    "import pandas as pd\n",
    "import json, ijson\n",
    "import os, sys, uuid\n",
    "from pykalman import KalmanFilter\n",
    "from PIL import Image\n",
    "import math\n",
    "import ast\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from geopy.distance import geodesic, distance\n",
    "from geopy import Point\n",
    "from shapely.geometry import Point, Polygon as ShapelyPolygon\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "import ast\n",
    "\n",
    "from collections import defaultdict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "CHANNELS = [37,38,39]\n",
    "N_ESTIMATORS = 100\n",
    "MISSING_VALUE = -100\n",
    "DEBUG_LOGGING = False\n",
    "S3_CACHING_BUCKET = 'cognosos-ml-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_data_woc(scan: List[Dict]) -> Dict:\n",
    "    # Parse each scan to get maximum reading for each MAC address in specified channels\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            readings_by_mac_addr_and_channel[mac_addr] += readings\n",
    "    return {mac_addr: int(max(readings)) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if readings}\n",
    "\n",
    "\n",
    "def process_training(data_filepath: str) -> List[Dict]:\n",
    "    X = []\n",
    "\n",
    "    # parse it incrementally\n",
    "    with open(data_filepath, 'r') as f:\n",
    "        # reads the JSON incrementally\n",
    "        objects = ijson.items(f, 'item') \n",
    "\n",
    "        print('Done loading JSON incrementally')\n",
    "\n",
    "        for scan in objects:\n",
    "            \n",
    "            Zone_id = str(scan['zoneId'])\n",
    "            Room_name = str(scan['zoneName'])\n",
    "            parent_zone_id = str(scan['parentZoneId'])\n",
    "            tagId = scan['tagId']\n",
    "            timestamp = scan['rxAt']\n",
    "            scan_readings: List[Dict] = scan['scandata']\n",
    "            \n",
    "            row = parse_scan_data_woc(scan_readings) \n",
    "\n",
    "            row.update({\n",
    "                'Zone_id': Zone_id,\n",
    "                'Room_name': Room_name,\n",
    "                'parent_zone_id': parent_zone_id,\n",
    "                'tagId': tagId,\n",
    "                'timestamp': timestamp,\n",
    "            })\n",
    "\n",
    "            if row:\n",
    "                X.append(row)\n",
    "\n",
    "    print('Done processing data')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13926dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(scan_data):\n",
    "\n",
    "    if scan_data is None:\n",
    "        return []\n",
    "    return [\n",
    "        {'macHex': entry['macHex'], 'channel': entry['channel'], 'readings': [entry['rssi'][0]]}\n",
    "        for entry in scan_data if 'macHex' in entry and 'rssi' in entry\n",
    "    ]\n",
    "\n",
    "def parse_scan_data(scan: List[Dict]) -> Dict:\n",
    "\n",
    "    readings_by_mac_addr_and_channel = defaultdict(list)\n",
    "    for beacon_reading in scan:\n",
    "        if beacon_reading['channel'] in CHANNELS:\n",
    "            mac_addr = beacon_reading['macHex']\n",
    "            readings = beacon_reading['readings']\n",
    "            channel = beacon_reading['channel']\n",
    "            readings_by_mac_addr_and_channel[f'{mac_addr}'] += readings#-{channel}\n",
    "    return {mac_addr: max(readings) for mac_addr, readings in readings_by_mac_addr_and_channel.items() if len(readings) > 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variable(X_train, y_train_floor, y_train, save_models=False):\n",
    "    \n",
    "    floor_pipeline = Pipeline([\n",
    "        ('rf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    floor_pipeline.fit(X_train, y_train_floor)\n",
    "\n",
    "    clf_floor = floor_pipeline.named_steps['rf']\n",
    "\n",
    "    clf_rooms = {}\n",
    "\n",
    "    selected_features = {}\n",
    "\n",
    "    for floor_num, samples in X_train.groupby(y_train_floor):\n",
    "        \n",
    "        floor_labels = y_train[samples.index]\n",
    "\n",
    "        non_all_neg_120_columns = samples.columns[~np.all(samples == -120, axis=0)]\n",
    "\n",
    "        selected_samples = samples[non_all_neg_120_columns]\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=200, random_state=100)\n",
    "\n",
    "        classifier.fit(selected_samples, floor_labels)\n",
    "\n",
    "        clf_rooms[str(floor_num)] = classifier\n",
    "\n",
    "        selected_features[str(floor_num)] = selected_samples.columns.tolist()\n",
    "\n",
    "    if save_models:\n",
    "        model = {\n",
    "        'selected_features': selected_features,\n",
    "        'clf_rooms': clf_rooms,\n",
    "        'clf_floor': clf_floor\n",
    "        }\n",
    "        joblib.dump(model, 'Hier_Features.joblib')\n",
    "        \n",
    "    return selected_features, clf_rooms, clf_floor\n",
    "\n",
    "def predict_variable(X_test, clf_floor, clf_rooms, selected_features):\n",
    "    \n",
    "    predicted_floors = clf_floor.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "    for floor_num, sample in zip(predicted_floors, X_test.values):\n",
    "        classifier = clf_rooms[str(floor_num)]\n",
    "\n",
    "        selected_names = selected_features[floor_num]\n",
    "\n",
    "        selected_sample = sample[X_test.columns.isin(selected_names)].reshape(1, -1)\n",
    "\n",
    "        predicted_room = classifier.predict(selected_sample)[0]\n",
    "#         predicted_room = predicted_room.astype(str)\n",
    "        predictions.append(predicted_room)\n",
    "\n",
    "    return predictions, predicted_floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digital_twin(Anchor_point_location_file, ground_truth_file_location, map_file_location):\n",
    "\n",
    "    anchor_df = pd.read_csv(Anchor_point_location_file)\n",
    "    anchor_df[\"x\"] = anchor_df[\"x\"].astype(int)\n",
    "    anchor_df[\"y\"] = anchor_df[\"y\"].astype(int)\n",
    "    \n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_df['Mac'] = anchor_df['Mac'].astype(str).str.zfill(12)\n",
    "    \n",
    "    macLists = anchor_df['Mac'].to_list()\n",
    "\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file_location)\n",
    "    ground_truth_df[\"Zone_id\"] = ground_truth_df[\"Zone_id\"].astype(str)\n",
    "    \n",
    "    #create a empty map with 0s for future calculation\n",
    "    map_ = np.zeros((65,28))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    image = Image.open(map_file_location)\n",
    "    \n",
    "    plt.scatter(anchor_df.x,anchor_df.y, color='blue', s=50, edgecolors='black', label='Beacons', marker='o', alpha=0.6)\n",
    "\n",
    "#     plt.scatter(ground_truth_df[\"x\"], ground_truth_df[\"y\"], color='red', s=20, label='Ground Truth', marker='^')\n",
    "#     for i, label in enumerate(ground_truth_df['Room_name']):  \n",
    "#         plt.text(ground_truth_df['x'][i], ground_truth_df['y'][i], label, fontsize=9, color='w', ha='right', va='bottom')\n",
    "    plt.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    plt.xlim(0, 65)\n",
    "    plt.ylim(0, 28)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xticks([i for i in range(0, 65, 5)])\n",
    "    plt.yticks([i for i in range(0, 28, 4)])\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.title(\"Beacon distribution in meters\")\n",
    "    plt.legend()\n",
    "    plt.savefig('beacon_map_cognosos.png')\n",
    "\n",
    "    return anchor_df, ground_truth_df, map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823e749-f1be-43ef-8323-29d6aa5ec414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beacon_file = 'ground_truth/Beacon_map_cognosos_flr3.csv'\n",
    "ground_truth_file = \"ground_truth/Ground_truth_Mar25.csv\"\n",
    "map_file = 'ground_truth/Cognosos_view.png'\n",
    "\n",
    "anchor_point_df, ground_truth_df, map_ = create_digital_twin(beacon_file, ground_truth_file, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_features(row, df1):\n",
    "\n",
    "    valid_features = {}\n",
    "    \n",
    "    for mac in df1['Mac']:\n",
    "       \n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] != -100:\n",
    "            valid_features[mac] = row[mac]\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "def convert_coordinates(coord_str):\n",
    "    if isinstance(coord_str, str):\n",
    "       \n",
    "        try:\n",
    "            coord_str = coord_str.strip(\"[]\")\n",
    "            elements = coord_str.split()\n",
    "            return [float(elem) for elem in elements] \n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "        try:\n",
    "            coord_str = coord_str.replace(\" \", \",\")\n",
    "            coord_str = coord_str.replace(\",,\", \",\")\n",
    "            coord_str = coord_str.strip(',')\n",
    "            return ast.literal_eval(coord_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error processing coordinate string: {coord_str}\")\n",
    "            return None\n",
    "    else:\n",
    "        return coord_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_all(result, ground_truth_df, map_file_location, output_file=\"compare_plot_MLE_NLOS.png\"):\n",
    "    results = []\n",
    "    total_points_mle = 0\n",
    "    total_points_Optimisation = 0\n",
    "    total_points_fuse = 0\n",
    "\n",
    "    total_inside_mle = 0\n",
    "    total_inside_Optimisation = 0\n",
    "    total_inside_fuse = 0\n",
    "\n",
    "    merged_df = pd.merge(result, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        has_fused = 'Predicted_NLOS' in room_data.columns\n",
    "\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "\n",
    "        if not polygon.is_valid:\n",
    "            print(f\"Invalid polygon for '{room_name}', attempting to fix with buffer(0).\")\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Parse MLE predictions\n",
    "        x_pred_mle, y_pred_mle = [], []\n",
    "        for coord in room_data[\"Predicted_MLE\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_mle.append(float(coord[0]))\n",
    "                y_pred_mle.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid MLE coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Optimisation predictions\n",
    "        x_pred_Optimisation, y_pred_Optimisation = [], []\n",
    "        for coord in room_data[\"Predicted_Optimisation\"]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_pred_Optimisation.append(float(coord[0]))\n",
    "                y_pred_Optimisation.append(float(coord[1]))\n",
    "            except:\n",
    "                print(f\"Invalid Optimisation coord in '{room_name}': {coord}\")\n",
    "\n",
    "        # Parse Fused predictions only if available\n",
    "        x_pred_fuse, y_pred_fuse = [], []\n",
    "        if has_fused:\n",
    "            for coord in room_data[\"Predicted_NLOS\"]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_pred_fuse.append(float(coord[0]))\n",
    "                    y_pred_fuse.append(float(coord[1]))\n",
    "                except:\n",
    "                    print(f\"Invalid NLOS coord in '{room_name}': {coord}\")\n",
    "\n",
    "        inside_count_mle = sum(1 for x, y in zip(x_pred_mle, y_pred_mle) if Point(x, y).within(polygon))\n",
    "        inside_count_Optimisation = sum(1 for x, y in zip(x_pred_Optimisation, y_pred_Optimisation) if Point(x, y).within(polygon))\n",
    "        inside_count_fuse = sum(1 for x, y in zip(x_pred_fuse, y_pred_fuse) if Point(x, y).within(polygon)) if has_fused else 0\n",
    "\n",
    "        total_points_mle += len(x_pred_mle)\n",
    "        total_inside_mle += inside_count_mle\n",
    "\n",
    "        total_points_Optimisation += len(x_pred_Optimisation)\n",
    "        total_inside_Optimisation += inside_count_Optimisation\n",
    "\n",
    "        if has_fused:\n",
    "            total_points_fuse += len(x_pred_fuse)\n",
    "            total_inside_fuse += inside_count_fuse\n",
    "\n",
    "        percentage_inside_mle = (inside_count_mle / len(x_pred_mle)) * 100 if x_pred_mle else 0\n",
    "        percentage_inside_Optimisation = (inside_count_Optimisation / len(x_pred_Optimisation)) * 100 if x_pred_Optimisation else 0\n",
    "        percentage_inside_fuse = (inside_count_fuse / len(x_pred_fuse)) * 100 if has_fused and x_pred_fuse else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            'Room_name': room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            'MLE_Accuracy': percentage_inside_mle,\n",
    "            'Optimisation_Accuracy': percentage_inside_Optimisation,\n",
    "            'NLOS_Accuracy': percentage_inside_fuse if has_fused else None,\n",
    "            'MLE_Inside_Points': inside_count_mle,\n",
    "            'Optimisation_Inside_Points': inside_count_Optimisation,\n",
    "            'NLOS_Inside_Points': inside_count_fuse if has_fused else None,\n",
    "            'Total_Points': len(x_pred_mle),\n",
    "        })\n",
    "\n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_pred_mle, y_pred_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_pred_Optimisation, y_pred_Optimisation, color='green', s=8, label='Optimisation')\n",
    "        if has_fused:\n",
    "            ax.scatter(x_pred_fuse, y_pred_fuse, color='red', s=8, label='NLOS')\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        title_str = f\"{room_name} - MLE: {percentage_inside_mle:.1f}%, Optimisation: {percentage_inside_Optimisation:.1f}%\"\n",
    "        if has_fused:\n",
    "            title_str += f\", NLOS: {percentage_inside_fuse:.1f}%\"\n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    overall_mle_accuracy = (total_inside_mle / total_points_mle) * 100 if total_points_mle > 0 else 0\n",
    "    overall_Optimisation_accuracy = (total_inside_Optimisation / total_points_Optimisation) * 100 if total_points_Optimisation > 0 else 0\n",
    "    overall_fuse_accuracy = (total_inside_fuse / total_points_fuse) * 100 if total_points_fuse > 0 else 0\n",
    "\n",
    "    print(f\"\\nOverall MLE Accuracy: {overall_mle_accuracy:.2f}%\")\n",
    "    print(f\"Overall Optimisation Accuracy: {overall_Optimisation_accuracy:.2f}%\")\n",
    "    if total_points_fuse > 0:\n",
    "        print(f\"Overall NLOS Accuracy: {overall_fuse_accuracy:.2f}%\")\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89394c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MLE_data_survey_portal(df, ground_truth_df, anchor_point_df, export_unheard=False, export_path=\"unheard_anchor_points.csv\"):\n",
    "\n",
    "    # I ADD THIS TO Ensure MAC addresses are strings and zero-padded to length 12\n",
    "    anchor_point_df['Mac'] = anchor_point_df['Mac'].astype(str).str.zfill(12)\n",
    "\n",
    "    data_set_df = pd.DataFrame()\n",
    "    merged_df = pd.merge(df, ground_truth_df, on=[\"Zone_id\", 'Room_name'], how='inner').drop(['parent_zone_id'], axis=1)\n",
    "    zones = df['Zone_id']\n",
    "    heard_anchor_points = []\n",
    "\n",
    "    for mac_addr in anchor_point_df['Mac']:\n",
    "        if mac_addr in merged_df.columns:\n",
    "            data_set_df[mac_addr] = merged_df[mac_addr]\n",
    "            heard_anchor_points.append(mac_addr)\n",
    "\n",
    "    heard_anchor_point_df = anchor_point_df[anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "    unheard_anchor_point_df = anchor_point_df[~anchor_point_df['Mac'].isin(heard_anchor_points)].reset_index(drop=True)\n",
    "\n",
    "    heard_anchor_points_coord = heard_anchor_point_df[['x', 'y']].values\n",
    "\n",
    "    data_set_df[\"Zone_id\"] = merged_df[\"Zone_id\"]\n",
    "    data_set_df[\"Room_name\"] = merged_df[\"Room_name\"]\n",
    "    data_set_df[\"tagId\"] = merged_df[\"tagId\"]\n",
    "    data_set_df[\"timestamp\"] = merged_df[\"timestamp\"]\n",
    "    if \"channel\" in merged_df.columns:\n",
    "        data_set_df[\"channel\"] = merged_df[\"channel\"]\n",
    "    \n",
    "    data_set_df[\"x\"] = merged_df[\"x\"]\n",
    "    data_set_df[\"y\"] = merged_df[\"y\"]\n",
    "\n",
    "    return data_set_df, heard_anchor_points_coord, unheard_anchor_point_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/Asset/Open_area_with_ground_truth/data_asset_open_area_with_ground_truth_Nov24.json\"\n",
    "filename = os.path.basename(filepath)\n",
    "\n",
    "X1 = process_training(filepath)\n",
    "df1 = pd.DataFrame(X1)\n",
    "df1 = df1.fillna(MISSING_VALUE)  \n",
    "\n",
    "float_cols = df1.select_dtypes(include=['float']).columns\n",
    "df1[float_cols] = df1[float_cols].astype(np.int8)\n",
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'], utc=True, errors='coerce')\n",
    "df1 = df1.sort_values(by='timestamp')\n",
    "\n",
    "ordered_columns = ['timestamp', 'tagId', 'Zone_id', 'Room_name', \"parent_zone_id\"]\n",
    "\n",
    "columns = [col for col in anchor_point_df.Mac.unique().tolist() if col not in ordered_columns]\n",
    "new_column_order = columns + ordered_columns\n",
    "df1 = df1.reindex(columns=new_column_order)\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Room_name'] = df1['Room_name'].str.split('-').str[-1].str.strip()\n",
    "\n",
    "# Fix specific zone_id\n",
    "df1.loc[df1['Zone_id'] == \"30598\", 'Zone_id'] = \"30539\"\n",
    "\n",
    "# Beacon processing\n",
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "df1 = df1.fillna(MISSING_VALUE)\n",
    "df1['beacon_count'] = (df1[beacon_cols] != -100).sum(axis=1)\n",
    "\n",
    "print(df1.shape)\n",
    "\n",
    "df1 = df1[df1['beacon_count'] >= 5]\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82168a46",
   "metadata": {},
   "source": [
    "### Remove some timestamp as I forgot to turn off the survey when I move the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['timestamp'] = pd.to_datetime(df1['timestamp'])\n",
    "\n",
    "# Define the time range to remove\n",
    "start_time = pd.to_datetime(\"18:11:00\").time()\n",
    "end_time = pd.to_datetime(\"18:24:00\").time()\n",
    "\n",
    "# Filter out rows where the timestamp's time is in that range\n",
    "df = df1[~df1['timestamp'].dt.time.between(start_time, end_time)]\n",
    "df.shape, df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4dc2e",
   "metadata": {},
   "source": [
    "### I need to correct the label as I was in Lab area 1 but took the survey data for Laba rea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_zone = \"30536\"\n",
    "correct_zone = \"30537\"\n",
    "\n",
    "wrong_room = \"Lab Area 1\"\n",
    "correct_room = \"Lab Area 2\"\n",
    "\n",
    "\n",
    "df['Zone_id'] = df['Zone_id'].replace({\n",
    "    wrong_zone: 'TEMP_ZONE',\n",
    "    correct_zone: wrong_zone\n",
    "}).replace({'TEMP_ZONE': correct_zone})\n",
    "\n",
    "df['Room_name'] = df['Room_name'].replace({\n",
    "    wrong_room: 'TEMP_ROOM',\n",
    "    correct_room: wrong_room\n",
    "}).replace({'TEMP_ROOM': correct_room})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['tagId'] == '2037320') & (df['Room_name'] == 'Lab Area 2'), ['tagId', 'Room_name']] = \\\n",
    "    ['2037321', 'Lab Area 1']\n",
    "df.loc[(df['tagId'] == '2040157') & (df['Room_name'] == 'Lab Area 2'), ['tagId', 'Room_name']] = \\\n",
    "    ['2040158', 'Lab Area 1']\n",
    "df.loc[(df['tagId'] == '2038775') & (df['Room_name'] == 'Lab Area 2'), ['tagId', 'Room_name']] = \\\n",
    "    ['2038776', 'Lab Area 1']\n",
    "\n",
    "df.loc[df['Room_name'] == 'Lab Area 1', 'Zone_id'] = \"30536\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa654b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/Asset/data_asset_open_area_with_ground_truth_Nov24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beca616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['beacon_count'].max(), df1['beacon_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc9d5",
   "metadata": {},
   "source": [
    "## REMOVE ALL ROWS < -90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bc260",
   "metadata": {},
   "source": [
    "### Read the data for walk around with a plastic box of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1= pd.read_csv(\"data/Asset/data_asset_tag_stay_still_combine_3_4.csv\")\n",
    "# df1.Zone_id= df1.Zone_id.astype(str)\n",
    "# df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9747cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_cols = [col for col in df1.columns if str(col).startswith('0')]\n",
    "rows_all_below_90 = df1[beacon_cols].lt(-99).all(axis=1)\n",
    "df1 = df1[~rows_all_below_90]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423898f9",
   "metadata": {},
   "source": [
    "### Check if dataset have enoguh beacon heard >=-90, SELECT ONLY the number of strong features >=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1.drop(columns=[\"parent_zone_id\", \"beacon_count\"]), \\\n",
    "                                ground_truth_df[['Zone_id','x', 'y']], on=[\"Zone_id\"], how='left')\n",
    "rssi_cols = [col for col in df2.columns if col.startswith('0')]\n",
    "\n",
    "# Create a new column counting RSSIs >= -90\n",
    "df2['num_strong_features'] = (df2[rssi_cols] >= -95).sum(axis=1)\n",
    "\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df = df2[df2['num_strong_features'] >= 5].copy()\n",
    "\n",
    "data_set_df=data_set_df.drop(columns='num_strong_features').reset_index(drop=True)\n",
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8071023",
   "metadata": {},
   "source": [
    "# *** MAKE SURE THAT ALL BEACONS ARE BEING HEARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_df.to_csv(\"data/Asset/data_asset_open_area_with_ground_truth_Nov24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_df=pd.read_csv(\"data/Asset/data_asset_open_area_with_ground_truth_Nov24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.Room_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab65e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_point_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set_df.columns.intersection(anchor_point_df.Mac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d5fc9",
   "metadata": {},
   "source": [
    "# Location AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4923584",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop= anchor_point_df[anchor_point_df.Remove==\"remove\"].Mac.tolist()\n",
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(\"data/Asset/Original data/2_data_asset_acess_Jun23_used_for_locAI_training.csv\")\n",
    "df2['Zone_id']=df2['Zone_id'].astype(str)\n",
    "df2= df2[df1.columns]\n",
    "\n",
    "df2= df2.drop(columns= columns_to_drop)\n",
    "\n",
    "train_data, test_data = train_test_split(df2, test_size=0.2, random_state=42, \\\n",
    "                                         stratify=df2[\"Zone_id\"])\n",
    "\n",
    "X_train = train_data[[col for col in train_data.columns if col.startswith(\"0\")]]\n",
    "# y_train_floor = train_data['parent_zone_id'] \n",
    "y_train = train_data['Zone_id']\n",
    "\n",
    "X_test = data_set_df[[col for col in train_data.columns if col.startswith(\"0\")]] \n",
    "# y_test_floor = df1['parent_zone_id'] \n",
    "y_test = data_set_df['Zone_id'] \n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_room = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_room.fit(X_train, y_train)\n",
    "\n",
    "predicted_rooms = rf_room.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bae2af",
   "metadata": {},
   "source": [
    "## Need to chnage the label of tag in right area as I placed the tag right at the border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c24188",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.Zone_id= data_set_df.Zone_id.astype(str)\n",
    "result_d = data_set_df[[\"Room_name\", 'tagId', 'Zone_id']]\\\n",
    "    .merge(ground_truth_df[[\"Zone_id\", \"Room_Type\"]], on = \"Zone_id\", how=\"left\")\n",
    "# result_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a42add",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d[\"Prediction\"] = predicted_rooms.astype(str)\n",
    "result_d[\"Prediction\"]= result_d[\"Prediction\"]\n",
    "result_d[\"Accuracy\"] = np.where(result_d.Zone_id == result_d.Prediction, 100, 0)\n",
    "result_d= result_d.rename(columns={\"tagId\": \"Tag_id\"})\n",
    "result_d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d[result_d.Room_name==\"Lab Area 1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e13c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d.groupby([\"Room_name\"])[\"Accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6f1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54885272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34f22d35",
   "metadata": {},
   "source": [
    "# Fused of Opt and MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19016ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(point, points, height_diff=1.5):\n",
    "    return np.sqrt(np.sum((points - point) ** 2, axis=1) + height_diff ** 2)\n",
    "\n",
    "def generate_grid(center, resolution=5, radius=4):\n",
    "    step = 1 / resolution\n",
    "    x_vals = np.arange(center[0] - radius, center[0] + radius + step, step)\n",
    "    y_vals = np.arange(center[1] - radius, center[1] + radius + step, step)\n",
    "    xv, yv = np.meshgrid(x_vals, y_vals)\n",
    "    return np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "\n",
    "def trilateration(coords, distances):\n",
    "    def fun(x, coords, distances):\n",
    "        weights = 1 / (distances + 1e-5)\n",
    "        return weights * (np.linalg.norm(coords - x, axis=1) - distances)\n",
    "    x0 = np.mean(coords, axis=0)\n",
    "    result = least_squares(fun, x0, args=(coords, distances))\n",
    "    return result.x if result.success else x0\n",
    "\n",
    "def rssi_to_distance(rssi, A=-40, n=3.5, scale=0.8):\n",
    "    return scale * np.exp((A - rssi) / (10 * n))\n",
    "\n",
    "def filter_rssi(row, beacon_positions, rssi_threshold=-90):\n",
    "    return {\n",
    "        mac: row[mac]\n",
    "        for mac in beacon_positions.keys()\n",
    "        if mac in row.index and isinstance(row[mac], (int, float)) and row[mac] > rssi_threshold\n",
    "    }\n",
    "\n",
    "def localization_error(tag_position, beacons, distances):\n",
    "    estimated_distances = np.linalg.norm(beacons - tag_position, axis=1)\n",
    "    sigma = np.std(distances) + 1e-3\n",
    "    weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "    return np.sum(weights * (estimated_distances - distances) ** 2)\n",
    "\n",
    "def generate_expansion_area(initial_guess, std_dev=0.5, radius=0.8, num_points=500, range_box=2):\n",
    "    num_gauss = int(num_points * 0.4)\n",
    "    num_box = int(num_points * 0.3)\n",
    "    num_circle = num_points - num_gauss - num_box\n",
    "    box_points = np.random.uniform(-range_box, range_box, size=(num_box, 2)) + initial_guess\n",
    "    gauss_points = np.random.normal(0, std_dev, size=(num_gauss, 2)) + initial_guess\n",
    "    r = radius * np.sqrt(np.random.uniform(0, 1, num_circle))\n",
    "    theta = np.random.uniform(0, 2 * np.pi, num_circle)\n",
    "    circ_points = np.column_stack((initial_guess[0] + r * np.cos(theta), initial_guess[1] + r * np.sin(theta)))\n",
    "    return np.vstack((gauss_points, box_points, circ_points))\n",
    "\n",
    "def calculate_total_error_to_all_beacons(best_position, beacon_coords):\n",
    "    distances = np.linalg.norm(beacon_coords - best_position, axis=1)\n",
    "    return np.sum(distances)\n",
    "\n",
    "def compute_likelihood_weighted(grid_coords, anchor_coords, rssi_values, T, n, sigma_noise=4, anchor_weights=None):\n",
    "    if anchor_weights is None:\n",
    "        anchor_weights = np.ones_like(rssi_values)\n",
    "    diff = grid_coords[:, None, :] - anchor_coords[None, :, :]\n",
    "    dists = np.sqrt(np.sum(diff ** 2, axis=2) + 1.5**2)\n",
    "    pred_rssi = T - 10 * n * np.log10(dists + 1e-5)\n",
    "    residuals = pred_rssi - rssi_values\n",
    "    weighted_residuals = (residuals / sigma_noise)**2 * anchor_weights\n",
    "    likelihood = np.exp(-0.5 * weighted_residuals)\n",
    "    return np.prod(likelihood, axis=1)\n",
    "\n",
    "def find_mle_params(P_j, d_ij, init_guess=[-40, 3]):\n",
    "    def squared_error(params, dists, rssi):\n",
    "        T_i, n_p = params\n",
    "        valid_mask = rssi != -100\n",
    "        pred_rssi = T_i - 10 * n_p * np.log10(dists + 1e-5)\n",
    "        return np.sum((pred_rssi[valid_mask] - rssi[valid_mask]) ** 2)\n",
    "    bounds = [(-100, -30), (2, 6)]\n",
    "    result = minimize(squared_error, init_guess, args=(d_ij, P_j),\n",
    "                      method='L-BFGS-B', bounds=bounds)\n",
    "    return result.x if result.success else init_guess\n",
    "\n",
    "# Fused Localization Function\n",
    "# -------------------------------\n",
    "def fused_localization_ref(data_df, anchor_point_df,\n",
    "                       sigma_noise=4, coarse_res=2, fine_res=5, fine_radius=3,\n",
    "                       rssi_threshold=-95, strong_rssi_threshold=-75,\n",
    "                       top_k_anchors=5, roi_margin=8, top_coarse_points=200, topN_ratio=0.05,\n",
    "                       map_x_bounds=(0, 65), map_y_bounds=(0, 28),\n",
    "                       epsilon=1e-12,\n",
    "                       min_beacons_opt=3, max_beacons_opt=20,\n",
    "                       expansion_radius=1, expansion_points=100, top_expansion_points=15,\n",
    "                       enable_refinement=True):\n",
    "    \n",
    "    results = []\n",
    "    beacon_positions = anchor_point_df[[\"x\",\"y\",\"Mac\"]].set_index(\"Mac\")[[\"x\",\"y\"]].to_dict(orient=\"index\")\n",
    "    has_timestamp = 'timestamp' in data_df.columns\n",
    "\n",
    "    for idx, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "        # ---------------------------\n",
    "        # Extract RSSI\n",
    "        # ---------------------------\n",
    "        rssis = row.drop(['Zone_id','Room_name','x','y','tagId','timestamp'], errors='ignore').values.astype(float)\n",
    "        anchor_coords = anchor_point_df[['x','y']].values\n",
    "\n",
    "        # ---------------------------\n",
    "        # MLE\n",
    "        # ---------------------------\n",
    "        mask_mle = rssis > rssi_threshold\n",
    "        signal_strengths = rssis[mask_mle]\n",
    "        dp_coords = anchor_coords[mask_mle]\n",
    "\n",
    "        if len(signal_strengths) < 1:\n",
    "            signal_strengths = rssis\n",
    "            dp_coords = anchor_coords\n",
    "\n",
    "        strong_mask = signal_strengths > strong_rssi_threshold\n",
    "        if np.sum(strong_mask) < 2:\n",
    "            dp_coords_selected = dp_coords\n",
    "            signal_strengths_selected = signal_strengths\n",
    "        else:\n",
    "            dp_coords_selected = dp_coords[strong_mask]\n",
    "            signal_strengths_selected = signal_strengths[strong_mask]\n",
    "\n",
    "        min_rssi, max_rssi = np.min(signal_strengths_selected), np.max(signal_strengths_selected)\n",
    "        anchor_weights = (signal_strengths_selected - min_rssi + 1) / (max_rssi - min_rssi + 1e-5)\n",
    "        sorted_idx = np.argsort(-signal_strengths_selected)\n",
    "        top_k = min(top_k_anchors, len(sorted_idx))\n",
    "        top_coords = dp_coords_selected[sorted_idx[:top_k]]\n",
    "\n",
    "        x_min, y_min = np.min(top_coords, axis=0)\n",
    "        x_max, y_max = np.max(top_coords, axis=0)\n",
    "        x_min = max(x_min - roi_margin, map_x_bounds[0])\n",
    "        x_max = min(x_max + roi_margin, map_x_bounds[1])\n",
    "        y_min = max(y_min - roi_margin, map_y_bounds[0])\n",
    "        y_max = min(y_max + roi_margin, map_y_bounds[1])\n",
    "\n",
    "        coarse_grid = np.stack(np.meshgrid(np.arange(x_min, x_max, 1/coarse_res),\n",
    "                                           np.arange(y_min, y_max, 1/coarse_res)), axis=-1).reshape(-1,2)\n",
    "\n",
    "        strongest_coord = top_coords[0]\n",
    "        dists_for_fit = euclidean_dist(strongest_coord, dp_coords_selected)\n",
    "        T_global, n_global = find_mle_params(signal_strengths_selected, dists_for_fit)\n",
    "\n",
    "        coarse_likelihoods = compute_likelihood_weighted(\n",
    "            coarse_grid, dp_coords_selected, signal_strengths_selected,\n",
    "            T_global, n_global, sigma_noise, anchor_weights\n",
    "        )\n",
    "\n",
    "        top_indices = np.argpartition(coarse_likelihoods, -top_coarse_points)[-top_coarse_points:]\n",
    "        top_candidates = coarse_grid[top_indices]\n",
    "\n",
    "        fine_candidates, fine_likelihoods = [], []\n",
    "        for center in top_candidates:\n",
    "            fine_grid = generate_grid(center, resolution=fine_res, radius=fine_radius)\n",
    "            likelihoods_fine = compute_likelihood_weighted(\n",
    "                fine_grid, dp_coords_selected, signal_strengths_selected,\n",
    "                T_global, n_global, sigma_noise, anchor_weights\n",
    "            )\n",
    "            fine_candidates.append(fine_grid)\n",
    "            fine_likelihoods.append(likelihoods_fine)\n",
    "\n",
    "        fine_candidates = np.vstack(fine_candidates)\n",
    "        fine_likelihoods = np.hstack(fine_likelihoods)\n",
    "        fine_likelihoods += epsilon\n",
    "        fine_likelihoods /= np.sum(fine_likelihoods)\n",
    "\n",
    "        N = max(1, min(100, int(topN_ratio * len(fine_candidates))))\n",
    "        top_idx = np.argpartition(fine_likelihoods, -N)[-N:]\n",
    "        top_points = fine_candidates[top_idx]\n",
    "        top_weights = fine_likelihoods[top_idx]\n",
    "        top_weights /= np.sum(top_weights)\n",
    "        pred_mle = np.average(top_points, axis=0, weights=top_weights)\n",
    "        conf_mle = np.max(top_weights)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Optimization\n",
    "        # ---------------------------\n",
    "        rssi_values_opt = dict(sorted(filter_rssi(row, beacon_positions, rssi_threshold).items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(rssi_values_opt) < min_beacons_opt:\n",
    "            beacon_coords_opt = anchor_coords\n",
    "            distances_opt = np.ones(anchor_coords.shape[0])\n",
    "        else:\n",
    "            beacon_coords_opt = np.array([list(beacon_positions[b].values()) for b in rssi_values_opt.keys()])\n",
    "            distances_opt = np.array([rssi_to_distance(rssi) for rssi in rssi_values_opt.values()])\n",
    "\n",
    "        close_beacons = beacon_coords_opt[distances_opt <= 1]\n",
    "        if len(close_beacons) > 0:\n",
    "            initial_guess_opt = close_beacons[np.argmin(distances_opt[distances_opt <= 1])]\n",
    "        else:\n",
    "            top_n = min(5, len(distances_opt))\n",
    "            initial_guess_opt = trilateration(beacon_coords_opt[:top_n], distances_opt[:top_n])\n",
    "\n",
    "        best_positions_opt, total_errors_opt, num_beacons_used = [], [], []\n",
    "        for num_beacons in range(min_beacons_opt, min(len(distances_opt), max_beacons_opt)+1):\n",
    "            top_n_coords = beacon_coords_opt[:num_beacons]\n",
    "            top_n_dists = distances_opt[:num_beacons]\n",
    "\n",
    "            expansion_area = generate_expansion_area(initial_guess_opt, radius=expansion_radius, num_points=expansion_points)\n",
    "            quick_errors = np.array([localization_error(p, top_n_coords, top_n_dists) for p in expansion_area])\n",
    "            filtered_expansion_area = expansion_area[np.argsort(quick_errors)[:top_expansion_points]]\n",
    "\n",
    "            best_err, best_pos = float(\"inf\"), None\n",
    "            for point in filtered_expansion_area:\n",
    "                res = minimize(localization_error, point, args=(top_n_coords, top_n_dists), method='L-BFGS-B', options={'maxiter':100})\n",
    "                if res.success:\n",
    "                    est_pos = res.x\n",
    "                    total_err = np.sum(np.linalg.norm(top_n_coords - est_pos, axis=1))\n",
    "                    if total_err < best_err:\n",
    "                        best_err = total_err\n",
    "                        best_pos = est_pos\n",
    "            best_positions_opt.append(best_pos)\n",
    "            total_errors_opt.append(best_err)\n",
    "            num_beacons_used.append(num_beacons)\n",
    "\n",
    "        # Final selection based on total error\n",
    "        best_idx_opt = np.argmin([calculate_total_error_to_all_beacons(p, beacon_coords_opt) for p in best_positions_opt])\n",
    "        pred_opt = best_positions_opt[best_idx_opt]\n",
    "        conf_opt = 1 / (1 + total_errors_opt[best_idx_opt])\n",
    "\n",
    "        # ---------------------------\n",
    "        # Final Refinement (matches original code)\n",
    "        # ---------------------------\n",
    "        pre_refined_pos = pred_opt.copy()\n",
    "        if enable_refinement:\n",
    "            refinement_rssi_threshold = -75\n",
    "            strong_rssi_indices = [i for i, rssi in enumerate(rssi_values_opt.values()) if rssi > refinement_rssi_threshold]\n",
    "\n",
    "            if len(strong_rssi_indices) >= 3:\n",
    "                filtered_coords = beacon_coords_opt[strong_rssi_indices]\n",
    "                filtered_distances = distances_opt[strong_rssi_indices]\n",
    "\n",
    "                result = minimize(\n",
    "                    localization_error,\n",
    "                    pred_opt,\n",
    "                    args=(filtered_coords, filtered_distances),\n",
    "                    method='L-BFGS-B',\n",
    "                    options={'maxiter': 100, 'gtol': 1e-8, 'disp': False}\n",
    "                )\n",
    "                if result.success:\n",
    "                    pred_opt = result.x\n",
    "\n",
    "        refinement_shift = np.linalg.norm(pred_opt - pre_refined_pos)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Fused Results\n",
    "        # ---------------------------\n",
    "        alpha_dynamic = conf_mle / (conf_mle + conf_opt)\n",
    "        pred_fused_fixed = 0.5 * pred_mle + 0.5 * pred_opt\n",
    "        pred_fused_dynamic = alpha_dynamic * pred_mle + (1-alpha_dynamic) * pred_opt\n",
    "\n",
    "        results.append({\n",
    "            'original_index': idx,\n",
    "            'Zone_id': row.get('Zone_id', np.nan),\n",
    "            'Room_name': row.get('Room_name', np.nan),\n",
    "            'Tag_id': row.get('tagId', np.nan),\n",
    "            'timestamp': row.get('timestamp', np.nan),\n",
    "            'Predicted_MLE': pred_mle,\n",
    "            'Predicted_Optimisation': pred_opt,\n",
    "            'Predicted_NLOS': pred_fused_fixed,\n",
    "            'Predicted_NLOS_Dynamic': pred_fused_dynamic,\n",
    "            'MLE_Confidence': conf_mle,\n",
    "            'Opt_Confidence': conf_opt,\n",
    "            'Ground_Truth': np.array([row['x'], row['y']])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca56e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename= \"data_asset_tag_stay_still_combine_3_4\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f43de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df[data_set_df.Room_name==\"Lab Area 3\"][\"x\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_df.groupby(\"Room_name\").tagId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131917d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter() \n",
    "\n",
    "result= fused_localization_ref(data_set_df, anchor_point_df)\n",
    "\n",
    "save_folder = \"Result_Asset\"\n",
    "save_name = f\"{filename.replace('.json', '_NLOS.csv')}\" \n",
    "save_path = os.path.join(save_folder, save_name)\n",
    "\n",
    "result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter() \n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_row = total_time / len(data_set_df)\n",
    "print(avg_time_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape, data_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3333b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_fused_dynamic(result_df, ground_truth_df, map_file_location,\n",
    "                                 fused_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "                                 output_file=\"compare_plot_MLE_Optim_Fused.png\"):\n",
    "    \"\"\"\n",
    "    Plot predicted locations from MLE, Optimisation, and fused methods, showing inside-room accuracy.\n",
    "    Computes overall accuracy and returns per-room statistics including inside-point counts and total points.\n",
    "    Also provides room-type aggregated accuracy.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "    unique_rooms = merged_df['Room_name'].unique()\n",
    "\n",
    "    n_rows = math.ceil(len(unique_rooms) / 2)\n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(14, 3 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    total_inside = {\"MLE\": 0, \"Optimisation\": 0}\n",
    "    total_inside.update({col: 0 for col in fused_cols})\n",
    "    total_points = 0  # only one total points count\n",
    "\n",
    "    for i, room_name in enumerate(unique_rooms):\n",
    "        room_data = merged_df[merged_df['Room_name'] == room_name]\n",
    "        zone = room_data[\"Zone_id\"].iloc[0]\n",
    "        room_type = room_data[\"Room_Type\"].iloc[0]\n",
    "        room_box = room_data.iloc[0]\n",
    "\n",
    "        # Room polygon\n",
    "        x_coords = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        y_coords = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "        coordinates = list(zip(x_coords, y_coords))\n",
    "        polygon = Polygon(coordinates)\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)\n",
    "\n",
    "        # Helper function to parse coordinates\n",
    "        def parse_coords(col_name):\n",
    "            x_list, y_list = [], []\n",
    "            for coord in room_data[col_name]:\n",
    "                try:\n",
    "                    coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                    x_list.append(float(coord[0]))\n",
    "                    y_list.append(float(coord[1]))\n",
    "                except:\n",
    "                    pass\n",
    "            return x_list, y_list\n",
    "\n",
    "        # Predictions\n",
    "        x_mle, y_mle = parse_coords(\"Predicted_MLE\")\n",
    "        x_opt, y_opt = parse_coords(\"Predicted_Optimisation\")\n",
    "        fused_data = {col: parse_coords(col) for col in fused_cols if col in room_data.columns}\n",
    "\n",
    "        # Count points inside polygon\n",
    "        def count_inside(x_list, y_list):\n",
    "            return sum(1 for x, y in zip(x_list, y_list) if Point(x, y).within(polygon))\n",
    "\n",
    "        inside_mle = count_inside(x_mle, y_mle)\n",
    "        inside_opt = count_inside(x_opt, y_opt)\n",
    "        inside_fused = {k: count_inside(*v) for k, v in fused_data.items()}\n",
    "\n",
    "        # Update totals\n",
    "        total_inside[\"MLE\"] += inside_mle\n",
    "        total_inside[\"Optimisation\"] += inside_opt\n",
    "        for k, v in fused_data.items():\n",
    "            total_inside[k] += inside_fused[k]\n",
    "        total_points += len(x_mle)  # same for all methods\n",
    "\n",
    "        # Save per-room results\n",
    "        results.append({\n",
    "            \"Zone_id\": zone,\n",
    "            \"Room_name\": room_name,\n",
    "            \"Room_Type\": room_type,\n",
    "            \"MLE_Accuracy\": inside_mle / max(len(x_mle), 1) * 100,\n",
    "            \"Optimisation_Accuracy\": inside_opt / max(len(x_opt), 1) * 100,\n",
    "            **{f\"{k}_Accuracy\": inside_fused[k] / max(len(fused_data[k][0]), 1) * 100 for k in fused_data},\n",
    "            \"MLE_Inside_Points\": inside_mle,\n",
    "            \"Optimisation_Inside_Points\": inside_opt,\n",
    "            **{f\"{k}_Inside_Points\": inside_fused[k] for k in fused_data},\n",
    "            \"Total_Points\": len(x_mle)\n",
    "        })\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[i]\n",
    "        image = mpimg.imread(map_file_location)\n",
    "        ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "        ax.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'r-', label='Room Boundary')\n",
    "        ax.scatter(x_mle, y_mle, color='blue', s=8, label='MLE')\n",
    "        ax.scatter(x_opt, y_opt, color='green', s=8, label='Optimisation')\n",
    "        colors = ['orange', 'purple', 'red', 'cyan']\n",
    "        for j, (fcol, (x_f, y_f)) in enumerate(fused_data.items()):\n",
    "            ax.scatter(x_f, y_f, color=colors[j % len(colors)], s=8, label=f\"{fcol}\")\n",
    "\n",
    "        ax.set_xlim([0, 65])\n",
    "        ax.set_ylim([0, 28])\n",
    "        \n",
    "        # Build title using percentage accuracy instead of counts\n",
    "        title_str = (\n",
    "            f\"{room_name} - \"\n",
    "            f\"MLE: {inside_mle / max(len(x_mle), 1) * 100:.1f}%, \"\n",
    "            f\"Opt: {inside_opt / max(len(x_opt), 1) * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        for fcol, (x_f, y_f) in fused_data.items():\n",
    "            acc = inside_fused[fcol] / max(len(x_f), 1) * 100\n",
    "            clean_name = fcol.replace(\"Predicted_\", \"\")  # <--- removes the prefix\n",
    "            title_str += f\", {clean_name}: {acc:.1f}%\"\n",
    "\n",
    "\n",
    "            \n",
    "        ax.set_title(title_str)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0, 0), ncol=2)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    accuracy_df = pd.DataFrame(results)\n",
    "\n",
    "    # --- Overall accuracy ---\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    for method in total_inside.keys():\n",
    "        overall = total_inside[method] / max(total_points, 1) * 100\n",
    "        print(f\"{method}: {overall:.2f}%\")\n",
    "\n",
    "    # --- Room-type aggregated accuracy ---\n",
    "    room_type_stats = accuracy_df.groupby('Room_Type').agg({\n",
    "        'MLE_Inside_Points': 'sum',\n",
    "        'Optimisation_Inside_Points': 'sum',\n",
    "        **{f\"{col}_Inside_Points\": 'sum' for col in fused_cols},\n",
    "        'Total_Points': 'sum'\n",
    "    })\n",
    "\n",
    "    for method in ['MLE', 'Optimisation'] + fused_cols:\n",
    "        room_type_stats[f\"{method}_Accuracy\"] = room_type_stats[f\"{method}_Inside_Points\"] / room_type_stats['Total_Points'] * 100\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy_df\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310953eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.groupby(\"Room_name\").Tag_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = plot_predicted_fused_dynamic(\n",
    "    result_df=result,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location=map_file,\n",
    "    fused_cols=['Predicted_NLOS'],\n",
    "    output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df[[\"Zone_id\",\"Room_name\", \"Predicted_NLOS_Accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d88309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_zones_NLOS_only(result_df, ground_truth_df, map_file_location,\n",
    "                             nlos_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "                             output_file=\"combined_all_zones_NLOS.png\"):\n",
    "    \"\"\"\n",
    "    Plot a single combined map of all zones:\n",
    "    - Each zone shown in a unique color\n",
    "    - Only NLOS prediction columns are plotted\n",
    "    - Zone polygon borders included\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge results with ground truth to get coordinates and boundaries\n",
    "    merged_df = pd.merge(result_df, ground_truth_df, on=[\"Zone_id\", \"Room_name\"], how=\"left\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Load background map\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto')\n",
    "\n",
    "    # Unique colors per zone\n",
    "    unique_zones = merged_df['Zone_id'].unique()\n",
    "    zone_colors = {z: plt.cm.tab20(i % 20) for i, z in enumerate(unique_zones)}\n",
    "\n",
    "    # Marker shapes for each NLOS column (different markers, same zone color)\n",
    "    markers = [\"o\", \"s\", \"D\", \"X\", \"P\", \"^\"]\n",
    "    method_markers = {col: markers[i % len(markers)] for i, col in enumerate(nlos_cols)}\n",
    "\n",
    "    # Helper to decode coordinate strings\n",
    "    def parse_coords(col, df):\n",
    "        x_list, y_list = [], []\n",
    "        for coord in df[col]:\n",
    "            try:\n",
    "                coord = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                x_list.append(float(coord[0]))\n",
    "                y_list.append(float(coord[1]))\n",
    "            except:\n",
    "                pass\n",
    "        return x_list, y_list\n",
    "\n",
    "    # ======== PLOT ALL ZONES ========\n",
    "    for zone in unique_zones:\n",
    "        zdata = merged_df[merged_df[\"Zone_id\"] == zone]\n",
    "        color = zone_colors[zone]\n",
    "\n",
    "        # ---- Draw zone boundary (polygon) ----\n",
    "        room_box = zdata.iloc[0]\n",
    "        bx = [room_box.get(f'x{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'x{i+1}', None))]\n",
    "        by = [room_box.get(f'y{i+1}', None) for i in range(8) if pd.notnull(room_box.get(f'y{i+1}', None))]\n",
    "\n",
    "        if len(bx) > 2:\n",
    "            ax.plot(bx + [bx[0]], by + [by[0]], color=color, linewidth=2)\n",
    "\n",
    "        # ---- Plot NLOS predictions only ----\n",
    "        for col in nlos_cols:\n",
    "            if col in zdata.columns:\n",
    "                x_vals, y_vals = parse_coords(col, zdata)\n",
    "                ax.scatter(x_vals, y_vals, s=16, color=color,\n",
    "                           marker=method_markers[col],\n",
    "                           label=f\"Zone {zone}  {col}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlim([0, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "    ax.set_title(\"All Zones  NLOS Predictions Only (With Zone Borders)\")\n",
    "\n",
    "    # Combine legend (remove duplicates)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "#     ax.legend(by_label.values(), by_label.keys(),\n",
    "#               loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e69c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "accuracy_df = plot_all_zones_NLOS_only(\n",
    "    result_df=result_nlos,\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    map_file_location=map_file,\n",
    "    nlos_cols=['Predicted_NLOS'],\n",
    "    output_file=\"compare_fused_results.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df= pd.read_csv(\"data/Asset/Open_area_with_ground_truth/Tag_ground_truth_data_Nov_24_mod.csv\")\n",
    "tag_df.Tag_id= tag_df.Tag_id.astype(str)\n",
    "tag_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1ce60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import ast\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_all_zones_with_predictions(\n",
    "    result_df,\n",
    "    ground_truth_df,\n",
    "    map_file_location,\n",
    "    tag_df=None,\n",
    "    classification_df=None,\n",
    "    nlos_accuracy_df=None,\n",
    "    nlos_cols=['Predicted_NLOS_Dynamic', 'Predicted_NLOS'],\n",
    "    output_file=\"combined_all_zones.png\",\n",
    "    connect_tag_to_pred=True,\n",
    "    connect_tag_to_classification=True,\n",
    "    show_classification_accuracy=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot zones with polygon boundaries, NLOS predictions, tags, and classification predictions.\n",
    "    Only wrong classification predictions are connected for clarity.\n",
    "    Uses dotted lines for classification connections with labels offset from line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge results with ground truth\n",
    "    merged_df = pd.merge(\n",
    "        result_df,\n",
    "        ground_truth_df,\n",
    "        on=[\"Zone_id\", \"Room_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Setup figure and background map\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    image = mpimg.imread(map_file_location)\n",
    "    ax.imshow(image, extent=[0, 65, 0, 28], aspect='auto', alpha=0.8)\n",
    "\n",
    "    # Color assignments for zones\n",
    "    fixed_colors = [\n",
    "        '#1f77b4', '#2ca02c','#ff7f0e',  '#d62728', '#9467bd',\n",
    "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'\n",
    "    ]\n",
    "    unique_zones = sorted(merged_df['Zone_id'].unique())\n",
    "    zone_colors = {z: fixed_colors[i % len(fixed_colors)] for i, z in enumerate(unique_zones)}\n",
    "\n",
    "    markers = [\"o\", \"s\", \"D\", \"X\", \"P\", \"^\"]\n",
    "    method_markers = {col: markers[i % len(markers)] for i, col in enumerate(nlos_cols)}\n",
    "\n",
    "    def parse_coords(col, df):\n",
    "        xs, ys = [], []\n",
    "        for coord in df[col]:\n",
    "            try:\n",
    "                c = ast.literal_eval(coord) if isinstance(coord, str) else coord\n",
    "                xs.append(float(c[0]))\n",
    "                ys.append(float(c[1]))\n",
    "            except Exception:\n",
    "                continue\n",
    "        return xs, ys\n",
    "\n",
    "    # Draw zones & NLOS\n",
    "    for zone in unique_zones:\n",
    "        zdata = merged_df[merged_df[\"Zone_id\"] == zone]\n",
    "        color = zone_colors[zone]\n",
    "\n",
    "        room_row = zdata.iloc[0]\n",
    "        bx = [room_row.get(f'x{i}', None) for i in range(1, 9) if pd.notnull(room_row.get(f'x{i}', None))]\n",
    "        by = [room_row.get(f'y{i}', None) for i in range(1, 9) if pd.notnull(room_row.get(f'y{i}', None))]\n",
    "        if len(bx) > 2 and len(by) > 2:\n",
    "            ax.plot(bx + [bx[0]], by + [by[0]], color=color, linewidth=1, zorder=2)\n",
    "\n",
    "        for col in nlos_cols:\n",
    "            if col in zdata.columns:\n",
    "                xs, ys = parse_coords(col, zdata)\n",
    "                ax.scatter(xs, ys, s=2, color=color, marker=method_markers[col], zorder=3)\n",
    "\n",
    "    # Plot tags and NLOS lines\n",
    "    if tag_df is not None:\n",
    "        if 'Zone_id' not in tag_df.columns:\n",
    "            tag_df = pd.merge(tag_df, ground_truth_df[['Room_name', 'Zone_id']],\n",
    "                              on='Room_name', how='left')\n",
    "\n",
    "        for _, row in tag_df.iterrows():\n",
    "            tx, ty = row[\"x\"], row[\"y\"]\n",
    "            tag_zone = row.get(\"Zone_id\")\n",
    "            tag_color = zone_colors.get(tag_zone, \"red\")\n",
    "\n",
    "            ax.scatter(tx, ty, marker=\"*\", s=180, color=tag_color,\n",
    "                       edgecolors=\"black\", linewidths=1.2, zorder=4)\n",
    "\n",
    "#             # **Add tag label**\n",
    "#             tag_label = row.get(\"Tag_id\", \"\")\n",
    "#             ax.text(tx + 0.2, ty + 0.2, str(tag_label), fontsize=8,\n",
    "#                     color='black', weight='bold', zorder=5)\n",
    "        \n",
    "            if connect_tag_to_pred and tag_zone in unique_zones:\n",
    "                zdata = merged_df[(merged_df['Zone_id'] == tag_zone) &\n",
    "                                  (merged_df['Tag_id'] == row['Tag_id'])]\n",
    "                for col in nlos_cols:\n",
    "                    if col in zdata.columns:\n",
    "                        xs, ys = parse_coords(col, zdata)\n",
    "                        for px, py in zip(xs, ys):\n",
    "                            ax.plot([tx, px], [ty, py], color=tag_color,\n",
    "                                    alpha=0.4, linewidth=1, zorder=3)\n",
    "\n",
    "    # Plot classification predictions (wrong only) with dotted lines\n",
    "    if classification_df is not None and connect_tag_to_classification:\n",
    "        grouped = classification_df.groupby([\"Zone_id\", \"Tag_id\"])\n",
    "        for (zone_id, tag_id), group in grouped:\n",
    "\n",
    "            tag_row = tag_df[(tag_df[\"Tag_id\"] == tag_id) & (tag_df.get(\"Zone_id\") == zone_id)]\n",
    "            if tag_row.empty:\n",
    "                continue\n",
    "            tx, ty = float(tag_row.iloc[0][\"x\"]), float(tag_row.iloc[0][\"y\"])\n",
    "\n",
    "            pred_counts = group[\"Prediction\"].value_counts(normalize=True)\n",
    "\n",
    "            for pred_zone, fraction in pred_counts.items():\n",
    "                # Skip correct prediction for dotted line\n",
    "                if pred_zone == zone_id:\n",
    "                    continue\n",
    "\n",
    "                # Centroid of predicted zone\n",
    "                zone_row = ground_truth_df[ground_truth_df[\"Zone_id\"] == pred_zone]\n",
    "                if zone_row.empty:\n",
    "                    continue\n",
    "                x_cols = [f\"x{i}\" for i in range(1, 9)]\n",
    "                y_cols = [f\"y{i}\" for i in range(1, 9)]\n",
    "                xs = [zone_row.iloc[0][c] for c in x_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "                ys = [zone_row.iloc[0][c] for c in y_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "                if not xs or not ys:\n",
    "                    continue\n",
    "                cx, cy = sum(xs)/len(xs), sum(ys)/len(ys)\n",
    "\n",
    "                # Dotted line to wrong prediction\n",
    "                ax.plot([tx, cx], [ty, cy], color=\"black\", alpha=0.5,\n",
    "                        linewidth=1.2, linestyle='dotted', zorder=4)\n",
    "\n",
    "                # Place percentage + predicted zone label slightly above line\n",
    "                offset = 0.2\n",
    "                mid_x, mid_y = (tx + cx)/2, (ty + cy)/2\n",
    "                dx, dy = cx - tx, cy - ty\n",
    "                angle = math.degrees(math.atan2(dy, dx))\n",
    "                label_x = mid_x - dy/abs(dy+1e-6)*offset if dy != 0 else mid_x\n",
    "                label_y = mid_y + dx/abs(dx+1e-6)*offset if dx != 0 else mid_y + offset\n",
    "                ax.text(label_x, label_y, f\"{fraction*100:.1f}%\",\n",
    "                        fontsize=7, color=\"b\",  zorder=6,\n",
    "                        ha=\"center\", va=\"center\", rotation=angle,weight=\"bold\",\n",
    "                        rotation_mode=\"anchor\",  alpha=0.7) # backgroundcolor=\"white\", weight=\"bold\",\n",
    "\n",
    "                # Triangle at predicted centroid\n",
    "                ax.scatter(cx, cy, marker=\"^\", s=200,\n",
    "                           color=zone_colors.get(pred_zone, \"yellow\"),\n",
    "                           edgecolors=\"black\", linewidths=1.2, zorder=5)\n",
    "\n",
    "    # Optional: per-zone accuracy labels\n",
    "    classification_accuracy = {}\n",
    "    if classification_df is not None:\n",
    "        classification_df['Correct'] = classification_df['Prediction'] == classification_df['Zone_id']\n",
    "        classification_accuracy = classification_df.groupby('Zone_id')['Correct'].mean() * 100\n",
    "        classification_accuracy = classification_accuracy.to_dict()\n",
    "\n",
    "    nlos_accuracy_dict = {}\n",
    "    if nlos_accuracy_df is not None:\n",
    "        nlos_accuracy_dict = dict(zip(nlos_accuracy_df['Zone_id'], nlos_accuracy_df['Predicted_NLOS_Accuracy']))\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        zone_row = ground_truth_df[ground_truth_df['Zone_id'] == zone]\n",
    "        if zone_row.empty:\n",
    "            continue\n",
    "        x_cols = [f'x{i}' for i in range(1, 9)]\n",
    "        y_cols = [f'y{i}' for i in range(1, 9)]\n",
    "        xs = [zone_row.iloc[0][c] for c in x_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "        ys = [zone_row.iloc[0][c] for c in y_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "        if not xs or not ys:\n",
    "            continue\n",
    "        cx, cy = sum(xs)/len(xs), sum(ys)/len(ys)\n",
    "\n",
    "        label_text = \"\"\n",
    "        if show_classification_accuracy:\n",
    "            cls_acc = classification_accuracy.get(zone, np.nan)\n",
    "            label_text += f\"LocAI: {cls_acc:.1f}%\\n\"\n",
    "        nlos_acc = nlos_accuracy_dict.get(zone, np.nan)\n",
    "        label_text += f\"NLOS: {nlos_acc:.1f}%\"\n",
    "\n",
    "        ax.text(cx, cy - 0.5, label_text,\n",
    "                fontsize=10, color='black', ha='center', va='bottom', weight='bold',\n",
    "                zorder=6, bbox=dict(facecolor='white', alpha=0.6, edgecolor='none', pad=1))\n",
    "\n",
    "    # Legend & formatting\n",
    "    ax.set_xlim([30, 65])\n",
    "    ax.set_ylim([0, 28])\n",
    "    ax.set_title(\"NLOS & LocationAI Predictions\", fontsize=14)\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='*', color='w', label='Tag Location',\n",
    "               markerfacecolor='black', markersize=15, markeredgecolor='black'),\n",
    "        Line2D([0], [0], marker='^', color='w', label='LocAI Prediction',\n",
    "               markerfacecolor='gray', markersize=12, markeredgecolor='black'),\n",
    "        Line2D([0], [0], marker='.', color='w', label='NLOS Prediction',\n",
    "               markerfacecolor='gray', markersize=12, markeredgecolor='black')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.22, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39538177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f18412",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone= ['Lab Area 2','Lab Area 3',  'Lab Area 1', 'Lounge Area']\n",
    "# zone= ['Lab Area 1',, '30537']\n",
    "plot_all_zones_with_predictions(\n",
    "    result[result.Room_name.isin(zone)],\n",
    "    ground_truth_df,\n",
    "    map_file,\n",
    "    tag_df=tag_df[tag_df.Room_name.isin(zone)],\n",
    "    classification_df=result_d[result.Room_name.isin(zone)],\n",
    "    nlos_accuracy_df=accuracy_df,\n",
    "    nlos_cols=['Predicted_NLOS'],\n",
    "    output_file=\"combined_all_zones.png\",\n",
    "    connect_tag_to_pred=True,\n",
    "    connect_tag_to_classification=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d002f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df= result[[\"Zone_id\", \"Room_name\", \"Tag_id\", \"Predicted_NLOS\"]]\n",
    "\n",
    "result_df['Tag_id'] = result_df['Tag_id'].astype(str)\n",
    "tag_df['Tag_id'] = tag_df['Tag_id'].astype(str)\n",
    "\n",
    "# Merge to get tag x, y\n",
    "merged_los = pd.merge(\n",
    "    result_df,\n",
    "    tag_df[['Tag_id', 'x', 'y', 'Room_name']],\n",
    "    on=['Tag_id', 'Room_name'],\n",
    "    how='left',\n",
    "    suffixes=('', '_tag')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pt1, pt2):\n",
    "    try:\n",
    "        return np.sqrt((pt1[0]-pt2[0])**2 + (pt1[1]-pt2[1])**2)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Compute distance between Predicted_MLE or Predicted_NLOS and tag location\n",
    "merged_los['LOS_Error'] = merged_los.apply(\n",
    "    lambda row: euclidean_distance(row['Predicted_NLOS'], [row['x'], row['y']]),\n",
    "    axis=1\n",
    ")\n",
    "# merged_los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Tag_id is string in both dataframes\n",
    "result_d['Tag_id'] = result_d['Tag_id'].astype(str)\n",
    "tag_df['Tag_id'] = tag_df['Tag_id'].astype(str)\n",
    "\n",
    "# Merge to get tag x, y coordinates\n",
    "merged_locai = pd.merge(\n",
    "    result_d,\n",
    "    tag_df[['Tag_id', 'x', 'y', 'Room_name']],\n",
    "    on=['Tag_id', 'Room_name'],\n",
    "    how='left'\n",
    ")\n",
    "# merged_locai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_centroid(zone_id, gt_df):\n",
    "    zone_row = gt_df[gt_df['Zone_id'] == zone_id]\n",
    "    if zone_row.empty:\n",
    "        return np.nan, np.nan\n",
    "    x_cols = [f'x{i}' for i in range(1, 9)]\n",
    "    y_cols = [f'y{i}' for i in range(1, 9)]\n",
    "    xs = [zone_row.iloc[0][c] for c in x_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "    ys = [zone_row.iloc[0][c] for c in y_cols if pd.notnull(zone_row.iloc[0][c])]\n",
    "    if not xs or not ys:\n",
    "        return np.nan, np.nan\n",
    "    return np.mean(xs), np.mean(ys)\n",
    "\n",
    "def euclidean_distance(pt1, pt2):\n",
    "    try:\n",
    "        return np.sqrt((pt1[0]-pt2[0])**2 + (pt1[1]-pt2[1])**2)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Compute centroid of predicted zone and distance to tag\n",
    "def compute_location_ai_error(row, gt_df):\n",
    "    pred_cx, pred_cy = get_zone_centroid(row['Prediction'], gt_df)\n",
    "    if np.isnan(pred_cx) or np.isnan(pred_cy):\n",
    "        return np.nan\n",
    "    tag_x, tag_y = row['x'], row['y']\n",
    "    return euclidean_distance([pred_cx, pred_cy], [tag_x, tag_y])\n",
    "\n",
    "merged_locai['LocationAI_Error'] = merged_locai.apply(lambda row: compute_location_ai_error(row, ground_truth_df), axis=1)\n",
    "merged_locai.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f37798",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_errors = merged_los['LOS_Error'].dropna().values\n",
    "locai_errors = merged_locai['LocationAI_Error'].dropna().values\n",
    "\n",
    "# Sort and compute CDF\n",
    "los_sorted = np.sort(los_errors)\n",
    "los_cdf = np.arange(1, len(los_sorted)+1) / len(los_sorted)\n",
    "\n",
    "locai_sorted = np.sort(locai_errors)\n",
    "locai_cdf = np.arange(1, len(locai_sorted)+1) / len(locai_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_at_cdf(x, cdf, percentile):\n",
    "    # percentile in [0,1]\n",
    "    return np.interp(percentile, cdf, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2492cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS\n",
    "los_50_val = get_value_at_cdf(los_sorted, los_cdf, 0.5)\n",
    "los_90_val = get_value_at_cdf(los_sorted, los_cdf, 0.9)\n",
    "\n",
    "# LocationAI\n",
    "locai_50_val = get_value_at_cdf(locai_sorted, locai_cdf, 0.5)\n",
    "locai_90_val = get_value_at_cdf(locai_sorted, locai_cdf, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.3  # meters to shift text left/right\n",
    "\n",
    "# Convert CDF to percent\n",
    "los_cdf_percent = los_cdf * 100\n",
    "locai_cdf_percent = locai_cdf * 100\n",
    "\n",
    "# Plot CDFs\n",
    "plt.plot(los_sorted, los_cdf_percent, label='NLOS Error', marker='o', linestyle='-', alpha=0.1)\n",
    "plt.plot(locai_sorted, locai_cdf_percent, label='LocationAI Error', marker='s', linestyle='--', alpha=0.1)\n",
    "\n",
    "# Horizontal lines at 50% and 90%\n",
    "for y in [50, 90]:\n",
    "    plt.axhline(y=y, color='gray', linestyle=':', alpha=0.7)\n",
    "\n",
    "# Annotate x-values for 50% CDF\n",
    "plt.text(los_50_val - offset, 50+2, f'{los_50_val:.1f}m', color='blue', ha='right', va='bottom')\n",
    "plt.text(locai_50_val + offset, 50+2, f'{locai_50_val:.1f}m', color='red', ha='right', va='bottom')\n",
    "\n",
    "# Annotate x-values for 90% CDF\n",
    "plt.text(los_90_val - offset, 90+2, f'{los_90_val:.1f}m', color='blue', ha='right', va='bottom')\n",
    "plt.text(locai_90_val + offset, 90+2, f'{locai_90_val:.1f}m', color='red', ha='right', va='bottom')\n",
    "\n",
    "plt.xlabel('Distance Error, meters')\n",
    "plt.ylabel('%')\n",
    "plt.title('CDF of Distance Errors of 4 Open Areas with 50% and 90% Confidence')\n",
    "plt.grid(False)\n",
    "plt.legend()\n",
    "plt.ylim(0, 105)  # add some space above 100%\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cd306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1WPcT_OjSaguNAQ9AnkpmmUV_GDiijIq5",
     "timestamp": 1717696268126
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa6ad618d5445e7a2343db292d10e8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a8df1c1c1f1460695b2fdd40ab5d774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3887145dd10742d2aaec48b48fbf209f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841ae5e6f05644f4a06df98a4b3d3e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "868e78ed1be844ff89c007db86f1c77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8810a66e33784737a04f2d4de120a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a78544536d5d4e87b691714c15e379a5",
       "IPY_MODEL_ece131d2b91b40e28153ed88cb397481",
       "IPY_MODEL_919d61916e484f10a410b26b643574bf"
      ],
      "layout": "IPY_MODEL_3887145dd10742d2aaec48b48fbf209f"
     }
    },
    "89cc3e4005b640cab78311861d917f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919d61916e484f10a410b26b643574bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841ae5e6f05644f4a06df98a4b3d3e56",
      "placeholder": "",
      "style": "IPY_MODEL_0aa6ad618d5445e7a2343db292d10e8b",
      "value": "0/100[00:00&lt;?,?it/s]"
     }
    },
    "a78544536d5d4e87b691714c15e379a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89cc3e4005b640cab78311861d917f9c",
      "placeholder": "",
      "style": "IPY_MODEL_e5079de1d27140eb9f6c4fabe6f199f5",
      "value": "0%"
     }
    },
    "e5079de1d27140eb9f6c4fabe6f199f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece131d2b91b40e28153ed88cb397481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a8df1c1c1f1460695b2fdd40ab5d774",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_868e78ed1be844ff89c007db86f1c77e",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
